[
  {
    "objectID": "index.html#опис-навчальної-дисципліни",
    "href": "index.html#опис-навчальної-дисципліни",
    "title": "Основи роботи з даними в R",
    "section": "Опис навчальної дисципліни",
    "text": "Опис навчальної дисципліни\nНавчальна дисципліна спрямована на вивчення основ практичного застосування популярної мови R для проведення статистичних досліджень в економіці.\nУ процесі вивчення курсу розглядаються теми, що стосуються теоретичних основ та практичної реалізації алгоритмів, завантаження, підготовки та обробки економічних даних.\nМісце навчальної дисципліни у підготовці здобувачів: програмні результати дисципліни використовуються під час вивчення таких навчальних дисциплін: “Алгоритми та структури даних”, “Аналіз даних в R”, “Прикладне математичне моделювання в R”, “Підготовка аналітичних звітів”. Закріплення на практиці здобутих програмних результатів відбувається під час проходження навчальної практики з курсу “Економіко-математичне моделювання”."
  },
  {
    "objectID": "index.html#мета-дисципліни",
    "href": "index.html#мета-дисципліни",
    "title": "Основи роботи з даними в R",
    "section": "Мета дисципліни",
    "text": "Мета дисципліни\nМета навчальної дисципліни – формування у студентів теоретичних знань та практичних навичок використання мови програмування R для роботи з даними та базовими структурами мови (типи даних, розгалуження, цикли, функції)."
  },
  {
    "objectID": "index.html#підтримка-проєкту",
    "href": "index.html#підтримка-проєкту",
    "title": "Основи роботи з даними в R",
    "section": "Підтримка проєкту",
    "text": "Підтримка проєкту\nМатеріали навчального посібника створено у межах проєкту “Підготовка, обробка та ефективне використання даних для наукових досліджень (на основі R)”, що підтримується Європейським союзою за програмою House of Europe."
  },
  {
    "objectID": "index.html#дотримання-принципів-доброчесності",
    "href": "index.html#дотримання-принципів-доброчесності",
    "title": "Основи роботи з даними в R",
    "section": "Дотримання принципів доброчесності",
    "text": "Дотримання принципів доброчесності\nВикладач та слухач цього курсу, як очікується, повинні дотримуватися Кодексу академічної доброчесності університету:\n\nбудь-яка робота, подана здобувачем протягом курсу, має бути його власною роботою здобувача; не вдаватися до кроків, що можуть нечесно покращити Ваші результати чи погіршити/покращити результати інших здобувачів;\nякщо буде виявлено ознаки плагіату або іншої недобросовісної академічної поведінки, то студент буде позбавлений можливості отримати передбачені бали за завдання;\nне публікувати у відкритому доступі відповіді на запитання, що використовуються в рамках курсу для оцінювання знань здобувачів;\nпід час фінальних видів контролю необхідно працювати самостійно; не дозволяється говорити або обговорювати, а також не можна копіювати документи, використовувати електронні засоби отримання інформації.\n\nПорушення академічної доброчесності під час виконання контрольних завдань призведе до втрати балів або вживання заходів, які передбачені Кодексу академічної доброчесності НаУОА.\n\n\n\n\n\n\n\nМатеріали курсу створені з використанням ряду технологій та середовищ розробки:\n\nМова R - безкоштована мова програмування для виконання досліджень у сфері статистики, машинного навчання та візуалізацї результатів.\nQuarto Book - система для публікації наукових та технічних текстів з відкритим кодом (R/Python/Julia/Observable).\nJupyterLab - середовище розробки на основі Jupyter Notebook. JupyterLab є розширеним веб-інтерфейсом для роботи з ноутбуками.\nGit/Github - система контролю версій та, відповідно, сервіс для організації зберігання коду, а також публікації статичних сторінок.\nRStudio Desktop - інтегроване середовище розробки (IDE) для мови R з відкритим кодом, що містить в собі редактор коду, консоль, планер, засоби візуалізації та можливості.\nVisual Studio Code - інтегроване середовище розробки (IDE) з відкритим кодом практично для усіх відомих технологій та мов програмування.\n\n\n\n\n\nБібілографічний опис bibtex:\n@book{yk-r-intro,\n  author       = {Юрій Клебан},\n  title        = {Вступ до програмування в R},\n  publisher    = {Zenodo},\n  year         = 2022,\n  doi          = {10.5281/zenodo.7251419},\n  url          = {https://doi.org/10.5281/zenodo.7251419}\n}"
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Вступ",
    "section": "",
    "text": "Фахівці спеціальності економічна кібернетика, а також фінанси та кредит у майбутньому працюватимуть з великими масивами даних, що накопичуються у даний момент і збиралися у попередні дисятиліття. Підготовка, обробка і трансформація даних у зручний формат прийняття рішень забирає все більше часу, а звичні рашіне інструменти аналізу даних, як наприклад, Microsoft Excel не мають достатньо вбудованих можливостей для виконнання задач бізнесу.\nНа даний час існує велика кількість мов програмування, що інтегруються у суспільні сфери діяльності людини та роботи технічних систем: біоінформатика, а також економіка та бізнес.\nОднією з мов програмування, що отримали широке поширення серед економістів-науковців, аналітиків та практиків математичного моделювання (machine learning) є мова програмування R(R Core Team 2020). Свою популярність ця мова програмування здобула завдяки простоті у використанні, доступності (безкоштовні як базові компоненти для написання коду, так і середовища розробки), розширюваності (кожен розробник має можливість створювати власні пакети та публікувати їх у відкритому доступі).\nОсновними задачами курсу “Вступ до прикладного програмування в R” є ознайомлення студентів з базовми конструкціями мови програмування R, вивчення способів роботи з найпоширенішими типами даних, читання інформації з різноманітних джерел. Також студенти отримують знання про можливості використання R для виконання задач аналізу даних та візуалізації.\n\n\n\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Список використаних джерел",
    "section": "",
    "text": "R Core Team. 2020. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/."
  },
  {
    "objectID": "10-intro.html",
    "href": "10-intro.html",
    "title": "\n1  Вступ\n",
    "section": "",
    "text": "Розглянути осноновні типи джерел даних, їх структуру та способи завантаження/вивантаження у R.\n\nЦе вбудований документ <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> на платформі <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "21-data-read.html",
    "href": "21-data-read.html",
    "title": "\n2  Презентація до лекції\n",
    "section": "",
    "text": "Розглянути осноновні типи джерел даних, їх структуру та способи завантаження/вивантаження у R.\n\nЦе вбудований документ <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> на платформі <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "22-r-csv.html",
    "href": "22-r-csv.html",
    "title": "3  CSV",
    "section": "",
    "text": "You need this packages for code execution:"
  },
  {
    "objectID": "22-r-csv.html#csv-comma-separated-values",
    "href": "22-r-csv.html#csv-comma-separated-values",
    "title": "3  Data collection and saving",
    "section": "3.1 CSV (Comma Separated Values)",
    "text": "3.1 CSV (Comma Separated Values)\nCSV - comma separated values.\n\n# lets check current working directory to write correct files path\ngetwd()\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en'\n\n\nYou can use / or \\\\ for writing correct path in R. For example:\n\npath = \"d:/projects/file.csv\"\npath = \"d:\\\\projects\\\\file.csv\"\n\nTo combine path use paste() or paste0() functions\n\nwork_dir = getwd()\nwork_dir \n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en'\n\n\n\nfile_name = \"temp_file.csv\"\nfile_path = paste0(work_dir, \"/\", file_name)\nfile_path\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en/temp_file.csv'\n\n\n\nfile_path = paste(work_dir, file_name, sep = \"/\")\nfile_path\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en/temp_file.csv'\n\n\n\n3.1.1 Sample dataset description\nInformation about dataset from kaggle.com. Original file located at url: https://www.kaggle.com/radmirzosimov/telecom-users-dataset.\nAny business wants to maximize the number of customers. To achieve this goal, it is important not only to try to attract new ones, but also to retain existing ones. Retaining a client will cost the company less than attracting a new one. In addition, a new client may be weakly interested in business services and it will be difficult to work with him, while old clients already have the necessary data on interaction with the service.\nAccordingly, predicting the churn, we can react in time and try to keep the client who wants to leave. Based on the data about the services that the client uses, we can make him a special offer, trying to change his decision to leave the operator. This will make the task of retention easier to implement than the task of attracting new users, about which we do not know anything yet.\nYou are provided with a dataset from a telecommunications company. The data contains information about almost six thousand users, their demographic characteristics, the services they use, the duration of using the operator’s services, the method of payment, and the amount of payment.\nThe task is to analyze the data and predict the churn of users (to identify people who will and will not renew their contract). The work should include the following mandatory items:\n\nDescription of the data (with the calculation of basic statistics);\nResearch of dependencies and formulation of hypotheses;\nBuilding models for predicting the outflow (with justification for the choice of a particular model) 4. based on tested hypotheses and identified relationships;\nComparison of the quality of the obtained models.\n\nFields description:\n\ncustomerID - customer id\ngender - client gender (male / female)\nSeniorCitizen - is the client retired (1, 0)\nPartner - is the client married (Yes, No)\ntenure - how many months a person has been a client of the company\nPhoneService - is the telephone service connected (Yes, No)\nMultipleLines - are multiple phone lines connected (Yes, No, No phone service)\nInternetService - client’s Internet service provider (DSL, Fiber optic, No)\nOnlineSecurity - is the online security service connected (Yes, No, No internet service)\nOnlineBackup - is the online backup service activated (Yes, No, No internet service)\nDeviceProtection - does the client have equipment insurance (Yes, No, No internet service)\nTechSupport - is the technical support service connected (Yes, No, No internet service)\nStreamingTV - is the streaming TV service connected (Yes, No, No internet service)\nStreamingMovies - is the streaming cinema service activated (Yes, No, No internet service)\nContract - type of customer contract (Month-to-month, One year, Two year)\nPaperlessBilling - whether the client uses paperless billing (Yes, No)\nPaymentMethod - payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\nMonthlyCharges - current monthly payment\nTotalCharges - the total amount that the client paid for the services for the entire time\nChurn - whether there was a churn (Yes or No)\n\nThare are few methods for reading/writing csv in base package:\n\nread.csv(), write.csv - default data separator is ,, decimal is separator ..\nread.csv2(), write.csv2 - default data separator is ;, decimal is separator ,.\n\nBefore using any new function check it usage information with help(function_name) or ?function_name, example: ?read.csv.\nYou can read (current data set has NA values as example, there are no NA in original datase):\n\ndata <- read.csv(\"../../data/telecom_users.csv\") # default reading\nstr(data)\n\n'data.frame':   5986 obs. of  22 variables:\n $ X               : int  1869 4528 6344 6739 432 2215 5260 6001 1480 5137 ...\n $ customerID      : chr  \"7010-BRBUU\" \"9688-YGXVR\" \"9286-DOJGF\" \"6994-KERXL\" ...\n $ gender          : chr  \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ SeniorCitizen   : int  0 0 1 0 0 0 0 0 0 1 ...\n $ Partner         : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Dependents      : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n $ tenure          : int  72 44 38 4 2 70 33 1 39 55 ...\n $ PhoneService    : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ MultipleLines   : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ InternetService : chr  \"No\" \"Fiber optic\" \"Fiber optic\" \"DSL\" ...\n $ OnlineSecurity  : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ OnlineBackup    : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ DeviceProtection: chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ TechSupport     : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ StreamingTV     : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ StreamingMovies : chr  \"No internet service\" \"No\" \"No\" \"Yes\" ...\n $ Contract        : chr  \"Two year\" \"Month-to-month\" \"Month-to-month\" \"Month-to-month\" ...\n $ PaperlessBilling: chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ PaymentMethod   : chr  \"Credit card (automatic)\" \"Credit card (automatic)\" \"Bank transfer (automatic)\" \"Electronic check\" ...\n $ MonthlyCharges  : chr  \"24.1\" \"88.15\" \"74.95\" \"55.9\" ...\n $ TotalCharges    : num  1735 3973 2870 238 120 ...\n $ Churn           : chr  \"No\" \"No\" \"Yes\" \"No\" ...\n\n\n\ndata <- read.csv(\"../../data/telecom_users.csv\",\n                  sep = \",\", # comma not only possibel separator\n                  dec = \".\", # decimal separator can be different\n                  na.strings = c(\"\", \"NA\", \"NULL\")) # you can define NA values\n\n\nstr(data) # chack data structure / types/ values\n\n'data.frame':   5986 obs. of  22 variables:\n $ X               : int  1869 4528 6344 6739 432 2215 5260 6001 1480 5137 ...\n $ customerID      : chr  \"7010-BRBUU\" \"9688-YGXVR\" \"9286-DOJGF\" \"6994-KERXL\" ...\n $ gender          : chr  \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ SeniorCitizen   : int  0 0 1 0 0 0 0 0 0 1 ...\n $ Partner         : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Dependents      : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n $ tenure          : int  72 44 38 4 2 70 33 1 39 55 ...\n $ PhoneService    : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ MultipleLines   : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ InternetService : chr  \"No\" \"Fiber optic\" \"Fiber optic\" \"DSL\" ...\n $ OnlineSecurity  : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ OnlineBackup    : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ DeviceProtection: chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ TechSupport     : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ StreamingTV     : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ StreamingMovies : chr  \"No internet service\" \"No\" \"No\" \"Yes\" ...\n $ Contract        : chr  \"Two year\" \"Month-to-month\" \"Month-to-month\" \"Month-to-month\" ...\n $ PaperlessBilling: chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ PaymentMethod   : chr  \"Credit card (automatic)\" \"Credit card (automatic)\" \"Bank transfer (automatic)\" \"Electronic check\" ...\n $ MonthlyCharges  : num  24.1 88.2 75 55.9 53.5 ...\n $ TotalCharges    : num  1735 3973 2870 238 120 ...\n $ Churn           : chr  \"No\" \"No\" \"Yes\" \"No\" ...\n\n\n\nhead(data, 2) # top 6 rows, use n = X, for viewing top X lines\n\n\n\nA data.frame: 2 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><dbl><dbl><chr>\n\n\n    118697010-BRBUUMale  0YesYes72YesYesNo         ...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)24.101734.65No\n    245289688-YGXVRFemale0No No 44YesNo Fiber optic...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)88.153973.20No\n\n\n\n\n\nis.data.frame(data) # if data is data.frame\n\nTRUE\n\n\n\nanyNA(data) # if dataframe contains any NA values\n\nTRUE\n\n\n\nlapply(data, anyNA)\n#lapply(, any) #check NA by 2nd dimension - columns\n\n\n    $X\n        FALSE\n    $customerID\n        FALSE\n    $gender\n        FALSE\n    $SeniorCitizen\n        FALSE\n    $Partner\n        FALSE\n    $Dependents\n        FALSE\n    $tenure\n        FALSE\n    $PhoneService\n        FALSE\n    $MultipleLines\n        FALSE\n    $InternetService\n        FALSE\n    $OnlineSecurity\n        FALSE\n    $OnlineBackup\n        FALSE\n    $DeviceProtection\n        FALSE\n    $TechSupport\n        FALSE\n    $StreamingTV\n        FALSE\n    $StreamingMovies\n        FALSE\n    $Contract\n        FALSE\n    $PaperlessBilling\n        FALSE\n    $PaymentMethod\n        FALSE\n    $MonthlyCharges\n        TRUE\n    $TotalCharges\n        TRUE\n    $Churn\n        FALSE\n\n\n\nCheck MonthlyCharges: TRUE and TotalCharges: TRUE. These columns has NA-values.\nLet’s replace them with mean:\n\ndata[is.na(data$TotalCharges), \"TotalCharges\"] <- mean(data$TotalCharges, na.rm = T)\ndata[is.na(data$MonthlyCharges), \"MonthlyCharges\"] <- mean(data$MonthlyCharges, na.rm = T)\n\n\nany(is.na(data)) # check for NA\n\nFALSE\n\n\nYou can write data with write.csv(), write.csv2() from base package.\n\nwrite.csv(data, file = \"../../data/cleaned_data.csv\", row.names = F)\n# by default row.names = TRUE and file will contain first column with row numbers 1,2, ..., N\n\nOne more useful package is readr. Examples of using:\n\n# library(readr)\n# data <- read_csv(file = \"../../data/telecom_users.csv\")\n# data <- read_csv2(file = \"../../data/telecom_users.csv\")`"
  },
  {
    "objectID": "22-r-csv.html#ms-excel-files-xlsx",
    "href": "22-r-csv.html#ms-excel-files-xlsx",
    "title": "3  Data collection and saving",
    "section": "3.2 MS Excel files (xlsx)",
    "text": "3.2 MS Excel files (xlsx)\nThere are many packages to read/write MS Excel files. xlsx one of the most useful.\n\n# install.packages(\"xlsx\") #install before use it\n\n\nlibrary(xlsx)\n\n\nany(grepl(\"xlsx\", installed.packages())) # check if package installed\n\nTRUE\n\n\n?read.xlsx - review package functions and params\nLet’s read the data telecom_users.xlsx:\n\ndata <- read.xlsx(\"../../data/telecom_users.xlsx\", sheetIndex = 1)\n# sheetIndex = 1 - select sheet to read, or use sheetName = \"sheet1\" to read by Name\nhead(data)\n\n\n\nA data.frame: 6 × 21\n\n    customerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetServiceOnlineSecurity...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <chr><chr><dbl><chr><chr><dbl><chr><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><dbl><dbl><chr>\n\n\n    17010-BRBUUMale  0YesYes72YesYes             No         No internet service...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.101734.65No \n    29688-YGXVRFemale0No No 44YesNo              Fiber opticNo                 ...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    39286-DOJGFFemale1YesNo 38YesYes             Fiber opticNo                 ...No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    46994-KERXLMale  0No No  4YesNo              DSL        No                 ...No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.90 238.50No \n    52181-UAESMMale  0No No  2YesNo              DSL        Yes                ...Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    64312-GVYNHFemale0YesNo 70No No phone serviceDSL        Yes                ...Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\n\n# You can also use startRow, endRow and other params to define how much data read\ndata <- read.xlsx(\"../../data/telecom_users.xlsx\", sheetIndex = 1, endRow = 100)\nhead(data)\n\n\n\nA data.frame: 6 × 21\n\n    customerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetServiceOnlineSecurity...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <chr><chr><dbl><chr><chr><dbl><chr><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><dbl><dbl><chr>\n\n\n    17010-BRBUUMale  0YesYes72YesYes             No         No internet service...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.101734.65No \n    29688-YGXVRFemale0No No 44YesNo              Fiber opticNo                 ...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    39286-DOJGFFemale1YesNo 38YesYes             Fiber opticNo                 ...No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    46994-KERXLMale  0No No  4YesNo              DSL        No                 ...No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.90 238.50No \n    52181-UAESMMale  0No No  2YesNo              DSL        Yes                ...Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    64312-GVYNHFemale0YesNo 70No No phone serviceDSL        Yes                ...Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\nLet’s replace Churn values Yes/No by 1/0:\n\nhead(data$Churn)\n\n\n'No''No''Yes''No''No''No'\n\n\n\ndata$Churn <- ifelse(data$Churn == \"Yes\", 1, 0)\n\n\nhead(data$Churn)\n\n\n001000\n\n\nWrite final data to excel:\n\nwrite.xlsx(data, file = \"../../data/final_telecom_data.xlsx\")\n\n\n\n3.2.1 Task 1\nDownload from kaggle.com and read dataset Default_Fin.csv: https://www.kaggle.com/kmldas/loan-default-prediction\nDescription:\nThis is a synthetic dataset created using actual data from a financial institution. The data has been modified to remove identifiable features and the numbers transformed to ensure they do not link to original source (financial institution).\nThis is intended to be used for academic purposes for beginners who want to practice financial analytics from a simple financial dataset\n\nIndex - This is the serial number or unique identifier of the loan taker\nEmployed - This is a Boolean 1= employed 0= unemployed\nBank.Balance - Bank Balance of the loan taker\nAnnual.Salary - Annual salary of the loan taker\n\nDefaulted - This is a Boolean 1= defaulted 0= not defaulted\n\n\nCheck what columns has missing values\nCount default and non-default clients / and parts of total clients in %\nCount Employed clients\nCount Employed Default clients\nAverage salary by Employed clients\nRename columns to “id”, “empl”, “balance”, “salary”, “default”\n\n\nSolution for Task 1\n\ndata <- read.csv(\"../../data/Default_Fin.csv\")\nhead(data)\n\n\n\nA data.frame: 6 × 5\n\n    IndexEmployedBank.BalanceAnnual.SalaryDefaulted.\n    <int><int><dbl><dbl><int>\n\n\n    111 8754.36532339.560\n    220 9806.16145273.560\n    33112882.60381205.680\n    441 6351.00428453.880\n    551 9427.92461562.000\n    66011035.08 89898.720\n\n\n\n\n\n\nCheck what columns has missing values\n\n\n\nanyNA(data)\n\nFALSE\n\n\n\n\nCount default and non-default clients / and parts of total clients in %\n\n\n\ndef_count <- nrow(data[data$Defaulted. == 1, ])\nno_def_count <- nrow(data[data$Defaulted. == 0, ])\ndef_count\nno_def_count \n\n333\n\n\n9667\n\n\n\ndef_count / nrow(data) * 100 # part defaults\nno_def_count / nrow(data) * 100 # part non-defaults\n\n3.33\n\n\n96.67\n\n\n\n\nCount Employed clients\n\n\n\nempl <- data[data$Employed == 1, ]\nnrow(empl)\n\n7056\n\n\n\n\nCount Employed Default clients\n\n\n\nempl <- data[data$Employed == 1 & data$Defaulted. == 1, ]\nnrow(empl)\n\n206\n\n\n\n\nAverage salary by Employed clients\n\n\n\nempl <- data[data$Employed == 1, ]\nmean(empl$Annual.Salary)\n\n480143.43414966\n\n\n\n\nRename columns to “id”, “empl”, “balance”, “salary”, “default”:\n\n\n\ncolnames(data) <- c(\"id\", \"empl\", \"balance\", \"salary\", \"default\")\nhead(data)\n\n\n\nA data.frame: 6 × 5\n\n    idemplbalancesalarydefault\n    <int><int><dbl><dbl><int>\n\n\n    111 8754.36532339.560\n    220 9806.16145273.560\n    33112882.60381205.680\n    441 6351.00428453.880\n    551 9427.92461562.000\n    66011035.08 89898.720"
  },
  {
    "objectID": "22-r-csv.html#xml-extensible-markup-language",
    "href": "22-r-csv.html#xml-extensible-markup-language",
    "title": "3  Data collection and saving",
    "section": "3.3 XML (eXtensible Markup Language)",
    "text": "3.3 XML (eXtensible Markup Language)\nFor our example we will use data from data/employes.xml. File contains records with info:\n<RECORDS>\n   <EMPLOYEE>\n      <ID>1</ID>\n      <NAME>Rick</NAME>\n      <SALARY>623.3</SALARY>\n      <STARTDATE>1/1/2012</STARTDATE>\n      <DEPT>IT</DEPT>\n   </EMPLOYEE>\n   ...\n</RECORDS>\n\n#install.packages(\"XML\")\nlibrary(\"XML\")\n#install.packages(\"methods\")\nlibrary(\"methods\")\n\n\nresult <- xmlParse(file = \"../../data/employes.xml\")\nprint(result)\n\n<?xml version=\"1.0\"?>\n<RECORDS>\n  <EMPLOYEE>\n    <ID>1</ID>\n    <NAME>Rick</NAME>\n    <SALARY>623.3</SALARY>\n    <STARTDATE>1/1/2012</STARTDATE>\n    <DEPT>IT</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>2</ID>\n    <NAME>Dan</NAME>\n    <SALARY>515.2</SALARY>\n    <STARTDATE>9/23/2013</STARTDATE>\n    <DEPT>Operations</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>3</ID>\n    <NAME>Michelle</NAME>\n    <SALARY>611</SALARY>\n    <STARTDATE>11/15/2014</STARTDATE>\n    <DEPT>IT</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>4</ID>\n    <NAME>Ryan</NAME>\n    <SALARY>729</SALARY>\n    <STARTDATE>5/11/2014</STARTDATE>\n    <DEPT>HR</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>5</ID>\n    <NAME>Gary</NAME>\n    <SALARY>843.25</SALARY>\n    <STARTDATE>3/27/2015</STARTDATE>\n    <DEPT>Finance</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>6</ID>\n    <NAME>Nina</NAME>\n    <SALARY>578</SALARY>\n    <STARTDATE>5/21/2013</STARTDATE>\n    <DEPT>IT</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>7</ID>\n    <NAME>Simon</NAME>\n    <SALARY>632.8</SALARY>\n    <STARTDATE>7/30/2013</STARTDATE>\n    <DEPT>Operations</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>8</ID>\n    <NAME>Guru</NAME>\n    <SALARY>722.5</SALARY>\n    <STARTDATE>6/17/2014</STARTDATE>\n    <DEPT>Finance</DEPT>\n  </EMPLOYEE>\n</RECORDS>\n \n\n\n\nrootnode <- xmlRoot(result) # reading rootnode of xml document\nrootnode[[1]] # reading first record\n\n<EMPLOYEE>\n  <ID>1</ID>\n  <NAME>Rick</NAME>\n  <SALARY>623.3</SALARY>\n  <STARTDATE>1/1/2012</STARTDATE>\n  <DEPT>IT</DEPT>\n</EMPLOYEE> \n\n\n\nrootnode[[1]][[2]] # reading first record in root node and second tag, its <NAME>\n\n<NAME>Rick</NAME> \n\n\nFor us the best way is to get dataframe:\n\nxmldataframe <- xmlToDataFrame(\"../../data/employes.xml\")\nxmldataframe\n\n\n\nA data.frame: 8 × 5\n\n    IDNAMESALARYSTARTDATEDEPT\n    <chr><chr><chr><chr><chr>\n\n\n    1Rick    623.3 1/1/2012  IT        \n    2Dan     515.2 9/23/2013 Operations\n    3Michelle611   11/15/2014IT        \n    4Ryan    729   5/11/2014 HR        \n    5Gary    843.253/27/2015 Finance   \n    6Nina    578   5/21/2013 IT        \n    7Simon   632.8 7/30/2013 Operations\n    8Guru    722.5 6/17/2014 Finance"
  },
  {
    "objectID": "22-r-csv.html#api-and-json",
    "href": "22-r-csv.html#api-and-json",
    "title": "3  Data collection and saving",
    "section": "3.4 API and JSON",
    "text": "3.4 API and JSON\nJSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. It is based on a subset of the JavaScript Programming Language Standard.\nAPI is the acronym for Application Programming Interface, which is a software intermediary that allows two applications to talk to each other.\nOne of the most popular packages for json is jsonlite.\n\n#install.packages(\"jsonlite\")\nlibrary(jsonlite)\n\nLet’s use readinginformation about BTC and USDT crypro currencies from Binance\n\nmarket = 'BTCUSDT'\ninterval = '1h'\nlimit = 100\n\nurl <- paste0(url = \"https://api.binance.com/api/v3/klines?symbol=\", market ,\"&interval=\", interval,\"&limit=\", limit)\nprint(url) # complete request URL\n\n[1] \"https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1h&limit=100\"\n\n\nOn the next stage you need use fromJSON() function to get data.\nMore details about requests to Binanace at https://github.com/binance/binance-spot-api-docs/blob/master/rest-api.md#klinecandlestick-data\nIf you enter ‘url’ value at browser response is going to be like this:\n[\n  [\n    1499040000000,      // Open time\n    \"0.01634790\",       // Open\n    \"0.80000000\",       // High\n    \"0.01575800\",       // Low\n    \"0.01577100\",       // Close\n    \"148976.11427815\",  // Volume\n    1499644799999,      // Close time\n    \"2434.19055334\",    // Quote asset volume\n    308,                // Number of trades\n    \"1756.87402397\",    // Taker buy base asset volume\n    \"28.46694368\",      // Taker buy quote asset volume\n    \"17928899.62484339\" // Ignore.\n  ]\n]\n\ndata <- fromJSON(url) # get json and transform it to list()\ndata <- data[, 1:7] # let's left only 1:7 columns (from Open time to Close time)\nhead(data)\n\n\n\nA matrix: 6 × 7 of type chr\n\n    165051360000041693.5800000041750.0000000041525.0000000041610.010000001138.643370001650517199999\n    165051720000041610.0100000041699.0000000041434.4400000041462.760000001229.259360001650520799999\n    165052080000041462.7500000041600.0000000041419.2000000041522.380000001049.712440001650524399999\n    165052440000041522.3800000041940.0000000041451.0000000041855.690000001928.480910001650527999999\n    1.650528e+12 41855.6900000042050.3000000041741.1000000041922.970000002518.040900001650531599999\n    165053160000041922.9600000041971.9000000041743.9600000041803.700000001655.769930001650535199999\n\n\n\n\n\ntypeof(data) # check data type\ndata <- as.data.frame(data) # convert to dataframe\nhead(data)\n\n'character'\n\n\n\n\nA data.frame: 6 × 7\n\n    V1V2V3V4V5V6V7\n    <chr><chr><chr><chr><chr><chr><chr>\n\n\n    1165051360000041693.5800000041750.0000000041525.0000000041610.010000001138.643370001650517199999\n    2165051720000041610.0100000041699.0000000041434.4400000041462.760000001229.259360001650520799999\n    3165052080000041462.7500000041600.0000000041419.2000000041522.380000001049.712440001650524399999\n    4165052440000041522.3800000041940.0000000041451.0000000041855.690000001928.480910001650527999999\n    51.650528e+12 41855.6900000042050.3000000041741.1000000041922.970000002518.040900001650531599999\n    6165053160000041922.9600000041971.9000000041743.9600000041803.700000001655.769930001650535199999\n\n\n\n\n\n# fix columns names\ncolnames(data) <- c(\"Open_time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Close_time\")\nhead(data) # looks better, but columns are characters still\n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <chr><chr><chr><chr><chr><chr><chr>\n\n\n    1165051360000041693.5800000041750.0000000041525.0000000041610.010000001138.643370001650517199999\n    2165051720000041610.0100000041699.0000000041434.4400000041462.760000001229.259360001650520799999\n    3165052080000041462.7500000041600.0000000041419.2000000041522.380000001049.712440001650524399999\n    4165052440000041522.3800000041940.0000000041451.0000000041855.690000001928.480910001650527999999\n    51.650528e+12 41855.6900000042050.3000000041741.1000000041922.970000002518.040900001650531599999\n    6165053160000041922.9600000041971.9000000041743.9600000041803.700000001655.769930001650535199999\n\n\n\n\n\nis.numeric(data[,1]) # check 1st column type is numeric\nis.numeric(data[,2]) # check 2nd column type is numeric\n\nFALSE\n\n\nFALSE\n\n\n\ndata <- as.data.frame(sapply(data, as.numeric)) # convert all columns to numeric\nhead(data) # good, its double now\n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    11.650514e+1241693.5841750.041525.0041610.011138.6431.650517e+12\n    21.650517e+1241610.0141699.041434.4441462.761229.2591.650521e+12\n    31.650521e+1241462.7541600.041419.2041522.381049.7121.650524e+12\n    41.650524e+1241522.3841940.041451.0041855.691928.4811.650528e+12\n    51.650528e+1241855.6942050.341741.1041922.972518.0411.650532e+12\n    61.650532e+1241922.9641971.941743.9641803.701655.7701.650535e+12\n\n\n\n\nFinal stage is to convert Open_time and Close_time to dates.\n\ndata$Open_time <- as.POSIXct(data$Open_time/1e3, origin = '1970-01-01')\ndata$Close_time <- as.POSIXct(data$Close_time/1e3, origin = '1970-01-01')\n\nhead(data) \n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <dttm><dbl><dbl><dbl><dbl><dbl><dttm>\n\n\n    12022-04-21 07:00:0041693.5841750.041525.0041610.011138.6432022-04-21 07:59:59\n    22022-04-21 08:00:0041610.0141699.041434.4441462.761229.2592022-04-21 08:59:59\n    32022-04-21 09:00:0041462.7541600.041419.2041522.381049.7122022-04-21 09:59:59\n    42022-04-21 10:00:0041522.3841940.041451.0041855.691928.4812022-04-21 10:59:59\n    52022-04-21 11:00:0041855.6942050.341741.1041922.972518.0412022-04-21 11:59:59\n    62022-04-21 12:00:0041922.9641971.941743.9641803.701655.7702022-04-21 12:59:59\n\n\n\n\n\ntail(data) # check last records\n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <dttm><dbl><dbl><dbl><dbl><dbl><dttm>\n\n\n    952022-04-25 05:00:0039095.8139153.9438961.6439091.171205.51582022-04-25 05:59:59\n    962022-04-25 06:00:0039091.1739294.7639086.3739253.711443.33182022-04-25 06:59:59\n    972022-04-25 07:00:0039253.7039256.2839055.7139139.74 896.85542022-04-25 07:59:59\n    982022-04-25 08:00:0039139.7439230.5038947.4238975.221057.49002022-04-25 08:59:59\n    992022-04-25 09:00:0038975.2139057.9738590.0038636.352814.97162022-04-25 09:59:59\n    1002022-04-25 10:00:0038636.3538675.6838200.0038534.993528.23552022-04-25 10:59:59"
  },
  {
    "objectID": "22-r-csv.html#google-services",
    "href": "22-r-csv.html#google-services",
    "title": "3  Data collection and saving",
    "section": "3.5 Google Services",
    "text": "3.5 Google Services\n\n3.5.1 Google Spreadsheets\n\nTHIS CHAPTER IS UNDER CONSTRUCTION / Working with Google Spreadsheets need account authorization.\n\ngooglesheets4 is a package to work with Google Sheets from R.\n\n#install.packages(\"googlesheets4\")\nlibrary(googlesheets4)\n\nYou can read google documents after authentification on google service. There is sample code:\nread_sheet(\"https://docs.google.com/spreadsheets/d/1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit#gid=780868077\")\ngs4_deauth()\nLet’s read sample dataset gapminder. It detailed described in next paragraph.\n\n# gs4_example(\"gapminder\")\n\n\n\n\n3.5.2 Google Search Trends\nGoogle Trends is a service for analyzing search requests by many filters like region (continent, country, locality), period (year, month), information category (business, education, hobby, healthcare), information type (news, shopping, video, images) https://trends.google.com/trends/\n\n# install.packages('gtrendsR')\n# install.packages('ggplot2')\nlibrary(gtrendsR) # loading package for Google Trends queries\nlibrary(ggplot2)\n\nLet’s configure out google trends query params\n\nkeywords = c(\"Bitcoin\", \"FC Barcelona\") # search keywords\ncountry = c('AT') # search region from https://support.google.com/business/answer/6270107?hl=en\ntime = (\"2021-01-01 2021-06-01\") # period\nchannel = 'web' # search channel: google search ('news' - google news, 'images' - google images)\n\n\n# query\ntrends = gtrends(keywords, gprop = channel, geo = country, time = time, tz = \"UTC\")\n\n\ntime_trend = trends$interest_over_time\nhead(time_trend)\n\n\n\nA data.frame: 6 × 7\n\n    datehitskeywordgeotimegpropcategory\n    <dttm><chr><chr><chr><chr><chr><int>\n\n\n    12021-01-0136BitcoinAT2021-01-01 2021-06-01web0\n    22021-01-0267BitcoinAT2021-01-01 2021-06-01web0\n    32021-01-0374BitcoinAT2021-01-01 2021-06-01web0\n    42021-01-0457BitcoinAT2021-01-01 2021-06-01web0\n    52021-01-0553BitcoinAT2021-01-01 2021-06-01web0\n    62021-01-0666BitcoinAT2021-01-01 2021-06-01web0\n\n\n\n\n\nplot <- ggplot(data=time_trend, aes(x=date, y=hits, group=keyword, col=keyword)) +\n  geom_line() +\n  xlab('Time') + \n  ylab('Relative Interest') + \n  theme(legend.title = element_blank(), legend.position=\"bottom\", legend.text=element_text(size=15)) + \n  ggtitle(\"Google Search Volume\")  \n\nplot"
  },
  {
    "objectID": "22-r-csv.html#sql-with-sqlite-sample",
    "href": "22-r-csv.html#sql-with-sqlite-sample",
    "title": "3  Data collection and saving",
    "section": "3.6 SQL (with SQLite sample)",
    "text": "3.6 SQL (with SQLite sample)\nWe are going to review working with database on SQLite, becouse it allows us not to install DB-server and start working with simple file.\nFor now we will use RSQLite package.\n\n# install.packages(\"RSQLite\")\nlibrary(RSQLite)\n\n\n# let's use mtcars dataset\n\ndata(\"mtcars\") # loads the data\nhead(mtcars) # preview the data\n\n\n\nA data.frame: 6 × 11\n\n    mpgcyldisphpdratwtqsecvsamgearcarb\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    Mazda RX421.061601103.902.62016.460144\n    Mazda RX4 Wag21.061601103.902.87517.020144\n    Datsun 71022.84108 933.852.32018.611141\n    Hornet 4 Drive21.462581103.083.21519.441031\n    Hornet Sportabout18.783601753.153.44017.020032\n    Valiant18.162251052.763.46020.221031\n\n\n\n\n\nI need this code for book successful building (remove database file if exists):\n\n\n#Define the file name that will be deleted\nfn <- paste0(\"../../data/cars.sqlite\")\n#Check its existence\nif (file.exists(fn)) {\n  #Delete file if it exists\n  file.remove(fn)\n}\n\nTRUE\n\n\nNow, let’s create new:\n\n# create new db file\ndb_path = paste0(\"../../data/cars.sqlite\")\n# create connection\nconn <- dbConnect(RSQLite::SQLite(), \n                    db_path,\n                    overwrite = TRUE, append = FALSE) # for lecture content only\n\n\n# Write the mtcars dataset into a table names mtcars_data\ndbWriteTable(conn, \"cars_table\", mtcars)\n# List all the tables available in the database\ndbListTables(conn)\n\n\n'cars_table'\n\n\n\ntable_data <- dbGetQuery(conn, \"SELECT * FROM cars_table\")\nhead(table_data)\n\n\n\nA data.frame: 6 × 11\n\n    mpgcyldisphpdratwtqsecvsamgearcarb\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    121.061601103.902.62016.460144\n    221.061601103.902.87517.020144\n    322.84108 933.852.32018.611141\n    421.462581103.083.21519.441031\n    518.783601753.153.44017.020032\n    618.162251052.763.46020.221031\n\n\n\n\n\n# close connection\ndbDisconnect(conn)\n\nYou can write complex queries for many tables if you knowledge of SQL allows."
  },
  {
    "objectID": "22-r-csv.html#web-pages-html",
    "href": "22-r-csv.html#web-pages-html",
    "title": "3  Data collection and saving",
    "section": "3.7 Web-pages (HTML)",
    "text": "3.7 Web-pages (HTML)\nSometimes decision making needs scrap data from web sources and pages.\nLet’s try to parse data from Wikipedia as table.\n\n#install.packages(\"rvest\")\nlibrary(rvest) # Parsing of HTML/XML files\n\nGo to web page https://en.wikipedia.org/wiki/List_of_largest_banks and check it.\n\n# fix URL\nurl <- \"https://en.wikipedia.org/wiki/List_of_largest_banks\"\n#url <- \"data/List of largest banks - Wikipedia_.html\"\n\n\n# read html content of the page\npage <- read_html(url)\npage\n\n{html_document}\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...\n\n\n\n# read all yables on page\ntables <- html_nodes(page, \"table\")\ntables\n\n{xml_nodeset (4)}\n[1] <table class=\"box-Missing_information plainlinks metadata ambox ambox-con ...\n[2] <table class=\"wikitable sortable mw-collapsible\"><tbody>\\n<tr>\\n<th data- ...\n[3] <table class=\"wikitable sortable mw-collapsible\">\\n<caption>Number of ban ...\n[4] <table class=\"wikitable sortable mw-collapsible\"><tbody>\\n<tr>\\n<th data- ...\n\n\nFor now, let’s read a table of Total Assets in US Billion\n\n# with pipe operator\n#tables[2] %>% \n #   html_table(fill = TRUE) %>% \n #   as.data.frame()\n#without pipe operator\nassets_table <- as.data.frame(html_table(tables[2], fill = TRUE))   \nhead(assets_table)\n\n\n\nA data.frame: 6 × 3\n\n    RankBank.nameTotal.assets.2020..US..billion.\n    <int><chr><chr>\n\n\n    11Industrial and Commercial Bank of China5,518.00\n    22China Construction Bank                4,400.00\n    33Agricultural Bank of China             4,300.00\n    44Bank of China                          4,200.00\n    55JPMorgan Chase                         3,831.65\n    66Mitsubishi UFJ Financial Group         3,175.21\n\n\n\n\nNext is reading data of market capitalization table (4th):\n\ncapital_table <- as.data.frame(html_table(tables[4], fill = TRUE))   \nhead(capital_table)\n\n\n\nA data.frame: 6 × 3\n\n    RankBank.nameMarket.cap.US..billion.\n    <int><chr><dbl>\n\n\n    11JPMorgan Chase                         368.78\n    22Industrial and Commercial Bank of China295.65\n    33Bank of America                        279.73\n    44Wells Fargo                            214.34\n    55China Construction Bank                207.98\n    66Agricultural Bank of China             181.49\n\n\n\n\nAnd now let’s merge() this two datasets:\n\nmerged_data <- merge(assets_table, capital_table, by = \"Bank.name\")\nhead(merged_data)\n\n\n\nA data.frame: 6 × 5\n\n    Bank.nameRank.xTotal.assets.2020..US..billion.Rank.yMarket.cap.US..billion.\n    <chr><int><chr><int><dbl>\n\n\n    1Agricultural Bank of China              34,300.00 6181.49\n    2Australia and New Zealand Banking Group48661.72  26 54.88\n    3Banco Bilbao Vizcaya Argentaria        42782.16  37 37.42\n    4Banco Bradesco                         79345.21  18 74.67\n    5Banco Santander                        161,702.6117 75.47\n    6Bank of America                         82,434.08 3279.73\n\n\n\n\n\n3.7.1 Task 3\nFrom a page https://en.wikipedia.org/wiki/List_of_largest_banks read and merge by country named tables:\n\nNumber of banks in the top 100 by total assets\nTotal market capital (US$ billion) across the top 70 banks by country\n\nSolution\n\nlibrary(rvest)\nurl <- \"https://en.wikipedia.org/wiki/List_of_largest_banks\" # got to url in other tab\n#url <- \"data/List of largest banks - Wikipedia_.html\"\npage_data <- read_html(url) # read html content\n\ntables <- html_nodes(page_data, \"table\")\nhtml_table(tables[1]) #its not needed table\n\n\n\n    \nA tibble: 1  2\n\n    X1X2\n    <lgl><chr>\n\n\n    NAThis article is missing information about Revenue and Employment. Please expand the article to include this information. Further details may exist on the talk page.  (September 2020)\n\n\n\n\n\n\n\nhtml_table(tables[3]) # thats solution for \"Number of banks in the top 100 by total assets\"\n#check the end of table. There are NA record\n# lets remove it\n\n\n\n    \nA tibble: 26  3\n\n    RankCountryNumber\n    <int><chr><int>\n\n\n    1China         19\n    2United States 11\n    3Japan          8\n    4United Kingdom 6\n    4France         6\n    4South Korea    6\n    5Canada         5\n    5Germany        5\n    6Australia      4\n    6Brazil         4\n    6Spain          4\n    7Netherlands    3\n    7Singapore      3\n    7Sweden         3\n    7Switzerland    3\n    8Italy          2\n    9India          1\n    9Austria        1\n    9Belgium        1\n    9Denmark        1\n    9Finland        1\n    9Norway         1\n    9Russia         1\n    9Qatar          1\n    9NA            NA\n    9NA            NA\n\n\n\n\n\n\n\ntable1 <- as.data.frame(html_table(tables[3]))\ntable1 <- table1[!is.na(table1$Country), ]\ntable1 # now it OK!\n\n\n\nA data.frame: 24 × 3\n\n    RankCountryNumber\n    <int><chr><int>\n\n\n    11China         19\n    22United States 11\n    33Japan          8\n    44United Kingdom 6\n    54France         6\n    64South Korea    6\n    75Canada         5\n    85Germany        5\n    96Australia      4\n    106Brazil         4\n    116Spain          4\n    127Netherlands    3\n    137Singapore      3\n    147Sweden         3\n    157Switzerland    3\n    168Italy          2\n    179India          1\n    189Austria        1\n    199Belgium        1\n    209Denmark        1\n    219Finland        1\n    229Norway         1\n    239Russia         1\n    249Qatar          1\n\n\n\n\n\n# SOlution for \"Total market capital (US$ billion) across the top 70 banks by country\"\n# compare this with table on a given page\ntable2 <- as.data.frame(html_table(tables[4]))\ntable2 # now it OK!\n\n\n\nA data.frame: 50 × 3\n\n    RankBank.nameMarket.cap.US..billion.\n    <int><chr><dbl>\n\n\n     1JPMorgan Chase                         368.78\n     2Industrial and Commercial Bank of China295.65\n     3Bank of America                        279.73\n     4Wells Fargo                            214.34\n     5China Construction Bank                207.98\n     6Agricultural Bank of China             181.49\n     7HSBC Holdings PLC                      169.47\n     8Citigroup Inc.                         163.58\n     9Bank of China                          151.15\n    10China Merchants Bank                   133.37\n    11Royal Bank of Canada                   113.80\n    12Toronto-Dominion Bank                  106.61\n    13Commonwealth Bank                       99.77\n    14HDFC Bank                              105.90\n    15U.S. Bancorp                            84.40\n    16Goldman Sachs                           78.70\n    17Banco Santander                         75.47\n    18Banco Bradesco                          74.67\n    19Morgan Stanley                          73.93\n    20Westpac                                 67.84\n    21Mitsubishi UFJ Financial Group          66.20\n    22Scotiabank                              65.48\n    23PNC Financial Services                  63.11\n    24Bank of Communications                  61.85\n    25BNP Paribas                             59.36\n    26Australia and New Zealand Banking Group 54.88\n    27National Australia Bank                 51.68\n    28Lloyds Banking Group                    51.19\n    29Sumitomo Mitsui Financial Group         49.85\n    30Bank of Montreal                        48.12\n    31UBS                                     45.92\n    32ING Group                               44.97\n    33Capital One                             43.22\n    34The Bank of New York Mellon             42.58\n    35China Minsheng Bank                     39.13\n    36China CITIC Bank                        38.55\n    37Banco Bilbao Vizcaya Argentaria         37.42\n    38Mizuho Financial Group                  36.95\n    39Intesa Sanpaolo                         36.90\n    40Credit Agricole                         34.89\n    41Canadian Imperial Bank of Commerce      34.87\n    42Royal Bank of Scotland                  33.95\n    43Barclays                                33.26\n    44Credit Suisse                           30.75\n    45Nordea                                  29.59\n    46Standard Chartered                      29.37\n    47KBC Bank                                27.40\n    48UniCredit                               26.88\n    49Societe Generale                        21.27\n    50Deutsche Bank                           15.77"
  },
  {
    "objectID": "22-r-csv.html#набори-даних",
    "href": "22-r-csv.html#набори-даних",
    "title": "3  CSV",
    "section": "3.5 Набори даних",
    "text": "3.5 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml"
  },
  {
    "objectID": "22-r-csv.html#references",
    "href": "22-r-csv.html#references",
    "title": "3  CSV",
    "section": "3.6 References",
    "text": "3.6 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl"
  },
  {
    "objectID": "23-r-xml.html",
    "href": "23-r-xml.html",
    "title": "4  Data collection and saving",
    "section": "",
    "text": "This chapter contains information about data reading and writing from/to different formats: csv, xml, json, google services, sql, html.\nYou need this packages for code execution:\nThere are many data source types for data storing, reading. Let’s review and try some of them."
  },
  {
    "objectID": "23-r-xml.html#csv-comma-separated-values",
    "href": "23-r-xml.html#csv-comma-separated-values",
    "title": "4  Data collection and saving",
    "section": "4.1 CSV (Comma Separated Values)",
    "text": "4.1 CSV (Comma Separated Values)\nCSV - comma separated values.\n\n# lets check current working directory to write correct files path\ngetwd()\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en'\n\n\nYou can use / or \\\\ for writing correct path in R. For example:\n\npath = \"d:/projects/file.csv\"\npath = \"d:\\\\projects\\\\file.csv\"\n\nTo combine path use paste() or paste0() functions\n\nwork_dir = getwd()\nwork_dir \n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en'\n\n\n\nfile_name = \"temp_file.csv\"\nfile_path = paste0(work_dir, \"/\", file_name)\nfile_path\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en/temp_file.csv'\n\n\n\nfile_path = paste(work_dir, file_name, sep = \"/\")\nfile_path\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en/temp_file.csv'\n\n\n\n4.1.1 Sample dataset description\nInformation about dataset from kaggle.com. Original file located at url: https://www.kaggle.com/radmirzosimov/telecom-users-dataset.\nAny business wants to maximize the number of customers. To achieve this goal, it is important not only to try to attract new ones, but also to retain existing ones. Retaining a client will cost the company less than attracting a new one. In addition, a new client may be weakly interested in business services and it will be difficult to work with him, while old clients already have the necessary data on interaction with the service.\nAccordingly, predicting the churn, we can react in time and try to keep the client who wants to leave. Based on the data about the services that the client uses, we can make him a special offer, trying to change his decision to leave the operator. This will make the task of retention easier to implement than the task of attracting new users, about which we do not know anything yet.\nYou are provided with a dataset from a telecommunications company. The data contains information about almost six thousand users, their demographic characteristics, the services they use, the duration of using the operator’s services, the method of payment, and the amount of payment.\nThe task is to analyze the data and predict the churn of users (to identify people who will and will not renew their contract). The work should include the following mandatory items:\n\nDescription of the data (with the calculation of basic statistics);\nResearch of dependencies and formulation of hypotheses;\nBuilding models for predicting the outflow (with justification for the choice of a particular model) 4. based on tested hypotheses and identified relationships;\nComparison of the quality of the obtained models.\n\nFields description:\n\ncustomerID - customer id\ngender - client gender (male / female)\nSeniorCitizen - is the client retired (1, 0)\nPartner - is the client married (Yes, No)\ntenure - how many months a person has been a client of the company\nPhoneService - is the telephone service connected (Yes, No)\nMultipleLines - are multiple phone lines connected (Yes, No, No phone service)\nInternetService - client’s Internet service provider (DSL, Fiber optic, No)\nOnlineSecurity - is the online security service connected (Yes, No, No internet service)\nOnlineBackup - is the online backup service activated (Yes, No, No internet service)\nDeviceProtection - does the client have equipment insurance (Yes, No, No internet service)\nTechSupport - is the technical support service connected (Yes, No, No internet service)\nStreamingTV - is the streaming TV service connected (Yes, No, No internet service)\nStreamingMovies - is the streaming cinema service activated (Yes, No, No internet service)\nContract - type of customer contract (Month-to-month, One year, Two year)\nPaperlessBilling - whether the client uses paperless billing (Yes, No)\nPaymentMethod - payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\nMonthlyCharges - current monthly payment\nTotalCharges - the total amount that the client paid for the services for the entire time\nChurn - whether there was a churn (Yes or No)\n\nThare are few methods for reading/writing csv in base package:\n\nread.csv(), write.csv - default data separator is ,, decimal is separator ..\nread.csv2(), write.csv2 - default data separator is ;, decimal is separator ,.\n\nBefore using any new function check it usage information with help(function_name) or ?function_name, example: ?read.csv.\nYou can read (current data set has NA values as example, there are no NA in original datase):\n\ndata <- read.csv(\"../../data/telecom_users.csv\") # default reading\nstr(data)\n\n'data.frame':   5986 obs. of  22 variables:\n $ X               : int  1869 4528 6344 6739 432 2215 5260 6001 1480 5137 ...\n $ customerID      : chr  \"7010-BRBUU\" \"9688-YGXVR\" \"9286-DOJGF\" \"6994-KERXL\" ...\n $ gender          : chr  \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ SeniorCitizen   : int  0 0 1 0 0 0 0 0 0 1 ...\n $ Partner         : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Dependents      : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n $ tenure          : int  72 44 38 4 2 70 33 1 39 55 ...\n $ PhoneService    : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ MultipleLines   : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ InternetService : chr  \"No\" \"Fiber optic\" \"Fiber optic\" \"DSL\" ...\n $ OnlineSecurity  : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ OnlineBackup    : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ DeviceProtection: chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ TechSupport     : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ StreamingTV     : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ StreamingMovies : chr  \"No internet service\" \"No\" \"No\" \"Yes\" ...\n $ Contract        : chr  \"Two year\" \"Month-to-month\" \"Month-to-month\" \"Month-to-month\" ...\n $ PaperlessBilling: chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ PaymentMethod   : chr  \"Credit card (automatic)\" \"Credit card (automatic)\" \"Bank transfer (automatic)\" \"Electronic check\" ...\n $ MonthlyCharges  : chr  \"24.1\" \"88.15\" \"74.95\" \"55.9\" ...\n $ TotalCharges    : num  1735 3973 2870 238 120 ...\n $ Churn           : chr  \"No\" \"No\" \"Yes\" \"No\" ...\n\n\n\ndata <- read.csv(\"../../data/telecom_users.csv\",\n                  sep = \",\", # comma not only possibel separator\n                  dec = \".\", # decimal separator can be different\n                  na.strings = c(\"\", \"NA\", \"NULL\")) # you can define NA values\n\n\nstr(data) # chack data structure / types/ values\n\n'data.frame':   5986 obs. of  22 variables:\n $ X               : int  1869 4528 6344 6739 432 2215 5260 6001 1480 5137 ...\n $ customerID      : chr  \"7010-BRBUU\" \"9688-YGXVR\" \"9286-DOJGF\" \"6994-KERXL\" ...\n $ gender          : chr  \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ SeniorCitizen   : int  0 0 1 0 0 0 0 0 0 1 ...\n $ Partner         : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Dependents      : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n $ tenure          : int  72 44 38 4 2 70 33 1 39 55 ...\n $ PhoneService    : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ MultipleLines   : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ InternetService : chr  \"No\" \"Fiber optic\" \"Fiber optic\" \"DSL\" ...\n $ OnlineSecurity  : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ OnlineBackup    : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ DeviceProtection: chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ TechSupport     : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ StreamingTV     : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ StreamingMovies : chr  \"No internet service\" \"No\" \"No\" \"Yes\" ...\n $ Contract        : chr  \"Two year\" \"Month-to-month\" \"Month-to-month\" \"Month-to-month\" ...\n $ PaperlessBilling: chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ PaymentMethod   : chr  \"Credit card (automatic)\" \"Credit card (automatic)\" \"Bank transfer (automatic)\" \"Electronic check\" ...\n $ MonthlyCharges  : num  24.1 88.2 75 55.9 53.5 ...\n $ TotalCharges    : num  1735 3973 2870 238 120 ...\n $ Churn           : chr  \"No\" \"No\" \"Yes\" \"No\" ...\n\n\n\nhead(data, 2) # top 6 rows, use n = X, for viewing top X lines\n\n\n\nA data.frame: 2 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><dbl><dbl><chr>\n\n\n    118697010-BRBUUMale  0YesYes72YesYesNo         ...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)24.101734.65No\n    245289688-YGXVRFemale0No No 44YesNo Fiber optic...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)88.153973.20No\n\n\n\n\n\nis.data.frame(data) # if data is data.frame\n\nTRUE\n\n\n\nanyNA(data) # if dataframe contains any NA values\n\nTRUE\n\n\n\nlapply(data, anyNA)\n#lapply(, any) #check NA by 2nd dimension - columns\n\n\n    $X\n        FALSE\n    $customerID\n        FALSE\n    $gender\n        FALSE\n    $SeniorCitizen\n        FALSE\n    $Partner\n        FALSE\n    $Dependents\n        FALSE\n    $tenure\n        FALSE\n    $PhoneService\n        FALSE\n    $MultipleLines\n        FALSE\n    $InternetService\n        FALSE\n    $OnlineSecurity\n        FALSE\n    $OnlineBackup\n        FALSE\n    $DeviceProtection\n        FALSE\n    $TechSupport\n        FALSE\n    $StreamingTV\n        FALSE\n    $StreamingMovies\n        FALSE\n    $Contract\n        FALSE\n    $PaperlessBilling\n        FALSE\n    $PaymentMethod\n        FALSE\n    $MonthlyCharges\n        TRUE\n    $TotalCharges\n        TRUE\n    $Churn\n        FALSE\n\n\n\nCheck MonthlyCharges: TRUE and TotalCharges: TRUE. These columns has NA-values.\nLet’s replace them with mean:\n\ndata[is.na(data$TotalCharges), \"TotalCharges\"] <- mean(data$TotalCharges, na.rm = T)\ndata[is.na(data$MonthlyCharges), \"MonthlyCharges\"] <- mean(data$MonthlyCharges, na.rm = T)\n\n\nany(is.na(data)) # check for NA\n\nFALSE\n\n\nYou can write data with write.csv(), write.csv2() from base package.\n\nwrite.csv(data, file = \"../../data/cleaned_data.csv\", row.names = F)\n# by default row.names = TRUE and file will contain first column with row numbers 1,2, ..., N\n\nOne more useful package is readr. Examples of using:\n\n# library(readr)\n# data <- read_csv(file = \"../../data/telecom_users.csv\")\n# data <- read_csv2(file = \"../../data/telecom_users.csv\")`"
  },
  {
    "objectID": "23-r-xml.html#ms-excel-files-xlsx",
    "href": "23-r-xml.html#ms-excel-files-xlsx",
    "title": "4  Data collection and saving",
    "section": "4.2 MS Excel files (xlsx)",
    "text": "4.2 MS Excel files (xlsx)\nThere are many packages to read/write MS Excel files. xlsx one of the most useful.\n\n# install.packages(\"xlsx\") #install before use it\n\n\nlibrary(xlsx)\n\n\nany(grepl(\"xlsx\", installed.packages())) # check if package installed\n\nTRUE\n\n\n?read.xlsx - review package functions and params\nLet’s read the data telecom_users.xlsx:\n\ndata <- read.xlsx(\"../../data/telecom_users.xlsx\", sheetIndex = 1)\n# sheetIndex = 1 - select sheet to read, or use sheetName = \"sheet1\" to read by Name\nhead(data)\n\n\n\nA data.frame: 6 × 21\n\n    customerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetServiceOnlineSecurity...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <chr><chr><dbl><chr><chr><dbl><chr><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><dbl><dbl><chr>\n\n\n    17010-BRBUUMale  0YesYes72YesYes             No         No internet service...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.101734.65No \n    29688-YGXVRFemale0No No 44YesNo              Fiber opticNo                 ...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    39286-DOJGFFemale1YesNo 38YesYes             Fiber opticNo                 ...No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    46994-KERXLMale  0No No  4YesNo              DSL        No                 ...No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.90 238.50No \n    52181-UAESMMale  0No No  2YesNo              DSL        Yes                ...Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    64312-GVYNHFemale0YesNo 70No No phone serviceDSL        Yes                ...Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\n\n# You can also use startRow, endRow and other params to define how much data read\ndata <- read.xlsx(\"../../data/telecom_users.xlsx\", sheetIndex = 1, endRow = 100)\nhead(data)\n\n\n\nA data.frame: 6 × 21\n\n    customerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetServiceOnlineSecurity...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <chr><chr><dbl><chr><chr><dbl><chr><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><dbl><dbl><chr>\n\n\n    17010-BRBUUMale  0YesYes72YesYes             No         No internet service...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.101734.65No \n    29688-YGXVRFemale0No No 44YesNo              Fiber opticNo                 ...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    39286-DOJGFFemale1YesNo 38YesYes             Fiber opticNo                 ...No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    46994-KERXLMale  0No No  4YesNo              DSL        No                 ...No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.90 238.50No \n    52181-UAESMMale  0No No  2YesNo              DSL        Yes                ...Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    64312-GVYNHFemale0YesNo 70No No phone serviceDSL        Yes                ...Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\nLet’s replace Churn values Yes/No by 1/0:\n\nhead(data$Churn)\n\n\n'No''No''Yes''No''No''No'\n\n\n\ndata$Churn <- ifelse(data$Churn == \"Yes\", 1, 0)\n\n\nhead(data$Churn)\n\n\n001000\n\n\nWrite final data to excel:\n\nwrite.xlsx(data, file = \"../../data/final_telecom_data.xlsx\")\n\n\n\n4.2.1 Task 1\nDownload from kaggle.com and read dataset Default_Fin.csv: https://www.kaggle.com/kmldas/loan-default-prediction\nDescription:\nThis is a synthetic dataset created using actual data from a financial institution. The data has been modified to remove identifiable features and the numbers transformed to ensure they do not link to original source (financial institution).\nThis is intended to be used for academic purposes for beginners who want to practice financial analytics from a simple financial dataset\n\nIndex - This is the serial number or unique identifier of the loan taker\nEmployed - This is a Boolean 1= employed 0= unemployed\nBank.Balance - Bank Balance of the loan taker\nAnnual.Salary - Annual salary of the loan taker\n\nDefaulted - This is a Boolean 1= defaulted 0= not defaulted\n\n\nCheck what columns has missing values\nCount default and non-default clients / and parts of total clients in %\nCount Employed clients\nCount Employed Default clients\nAverage salary by Employed clients\nRename columns to “id”, “empl”, “balance”, “salary”, “default”\n\n\nSolution for Task 1\n\ndata <- read.csv(\"../../data/Default_Fin.csv\")\nhead(data)\n\n\n\nA data.frame: 6 × 5\n\n    IndexEmployedBank.BalanceAnnual.SalaryDefaulted.\n    <int><int><dbl><dbl><int>\n\n\n    111 8754.36532339.560\n    220 9806.16145273.560\n    33112882.60381205.680\n    441 6351.00428453.880\n    551 9427.92461562.000\n    66011035.08 89898.720\n\n\n\n\n\n\nCheck what columns has missing values\n\n\n\nanyNA(data)\n\nFALSE\n\n\n\n\nCount default and non-default clients / and parts of total clients in %\n\n\n\ndef_count <- nrow(data[data$Defaulted. == 1, ])\nno_def_count <- nrow(data[data$Defaulted. == 0, ])\ndef_count\nno_def_count \n\n333\n\n\n9667\n\n\n\ndef_count / nrow(data) * 100 # part defaults\nno_def_count / nrow(data) * 100 # part non-defaults\n\n3.33\n\n\n96.67\n\n\n\n\nCount Employed clients\n\n\n\nempl <- data[data$Employed == 1, ]\nnrow(empl)\n\n7056\n\n\n\n\nCount Employed Default clients\n\n\n\nempl <- data[data$Employed == 1 & data$Defaulted. == 1, ]\nnrow(empl)\n\n206\n\n\n\n\nAverage salary by Employed clients\n\n\n\nempl <- data[data$Employed == 1, ]\nmean(empl$Annual.Salary)\n\n480143.43414966\n\n\n\n\nRename columns to “id”, “empl”, “balance”, “salary”, “default”:\n\n\n\ncolnames(data) <- c(\"id\", \"empl\", \"balance\", \"salary\", \"default\")\nhead(data)\n\n\n\nA data.frame: 6 × 5\n\n    idemplbalancesalarydefault\n    <int><int><dbl><dbl><int>\n\n\n    111 8754.36532339.560\n    220 9806.16145273.560\n    33112882.60381205.680\n    441 6351.00428453.880\n    551 9427.92461562.000\n    66011035.08 89898.720"
  },
  {
    "objectID": "23-r-xml.html#xml-extensible-markup-language",
    "href": "23-r-xml.html#xml-extensible-markup-language",
    "title": "4  Data collection and saving",
    "section": "4.3 XML (eXtensible Markup Language)",
    "text": "4.3 XML (eXtensible Markup Language)\nFor our example we will use data from data/employes.xml. File contains records with info:\n<RECORDS>\n   <EMPLOYEE>\n      <ID>1</ID>\n      <NAME>Rick</NAME>\n      <SALARY>623.3</SALARY>\n      <STARTDATE>1/1/2012</STARTDATE>\n      <DEPT>IT</DEPT>\n   </EMPLOYEE>\n   ...\n</RECORDS>\n\n#install.packages(\"XML\")\nlibrary(\"XML\")\n#install.packages(\"methods\")\nlibrary(\"methods\")\n\n\nresult <- xmlParse(file = \"../../data/employes.xml\")\nprint(result)\n\n<?xml version=\"1.0\"?>\n<RECORDS>\n  <EMPLOYEE>\n    <ID>1</ID>\n    <NAME>Rick</NAME>\n    <SALARY>623.3</SALARY>\n    <STARTDATE>1/1/2012</STARTDATE>\n    <DEPT>IT</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>2</ID>\n    <NAME>Dan</NAME>\n    <SALARY>515.2</SALARY>\n    <STARTDATE>9/23/2013</STARTDATE>\n    <DEPT>Operations</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>3</ID>\n    <NAME>Michelle</NAME>\n    <SALARY>611</SALARY>\n    <STARTDATE>11/15/2014</STARTDATE>\n    <DEPT>IT</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>4</ID>\n    <NAME>Ryan</NAME>\n    <SALARY>729</SALARY>\n    <STARTDATE>5/11/2014</STARTDATE>\n    <DEPT>HR</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>5</ID>\n    <NAME>Gary</NAME>\n    <SALARY>843.25</SALARY>\n    <STARTDATE>3/27/2015</STARTDATE>\n    <DEPT>Finance</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>6</ID>\n    <NAME>Nina</NAME>\n    <SALARY>578</SALARY>\n    <STARTDATE>5/21/2013</STARTDATE>\n    <DEPT>IT</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>7</ID>\n    <NAME>Simon</NAME>\n    <SALARY>632.8</SALARY>\n    <STARTDATE>7/30/2013</STARTDATE>\n    <DEPT>Operations</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>8</ID>\n    <NAME>Guru</NAME>\n    <SALARY>722.5</SALARY>\n    <STARTDATE>6/17/2014</STARTDATE>\n    <DEPT>Finance</DEPT>\n  </EMPLOYEE>\n</RECORDS>\n \n\n\n\nrootnode <- xmlRoot(result) # reading rootnode of xml document\nrootnode[[1]] # reading first record\n\n<EMPLOYEE>\n  <ID>1</ID>\n  <NAME>Rick</NAME>\n  <SALARY>623.3</SALARY>\n  <STARTDATE>1/1/2012</STARTDATE>\n  <DEPT>IT</DEPT>\n</EMPLOYEE> \n\n\n\nrootnode[[1]][[2]] # reading first record in root node and second tag, its <NAME>\n\n<NAME>Rick</NAME> \n\n\nFor us the best way is to get dataframe:\n\nxmldataframe <- xmlToDataFrame(\"../../data/employes.xml\")\nxmldataframe\n\n\n\nA data.frame: 8 × 5\n\n    IDNAMESALARYSTARTDATEDEPT\n    <chr><chr><chr><chr><chr>\n\n\n    1Rick    623.3 1/1/2012  IT        \n    2Dan     515.2 9/23/2013 Operations\n    3Michelle611   11/15/2014IT        \n    4Ryan    729   5/11/2014 HR        \n    5Gary    843.253/27/2015 Finance   \n    6Nina    578   5/21/2013 IT        \n    7Simon   632.8 7/30/2013 Operations\n    8Guru    722.5 6/17/2014 Finance"
  },
  {
    "objectID": "23-r-xml.html#api-and-json",
    "href": "23-r-xml.html#api-and-json",
    "title": "4  Data collection and saving",
    "section": "4.4 API and JSON",
    "text": "4.4 API and JSON\nJSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. It is based on a subset of the JavaScript Programming Language Standard.\nAPI is the acronym for Application Programming Interface, which is a software intermediary that allows two applications to talk to each other.\nOne of the most popular packages for json is jsonlite.\n\n#install.packages(\"jsonlite\")\nlibrary(jsonlite)\n\nLet’s use readinginformation about BTC and USDT crypro currencies from Binance\n\nmarket = 'BTCUSDT'\ninterval = '1h'\nlimit = 100\n\nurl <- paste0(url = \"https://api.binance.com/api/v3/klines?symbol=\", market ,\"&interval=\", interval,\"&limit=\", limit)\nprint(url) # complete request URL\n\n[1] \"https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1h&limit=100\"\n\n\nOn the next stage you need use fromJSON() function to get data.\nMore details about requests to Binanace at https://github.com/binance/binance-spot-api-docs/blob/master/rest-api.md#klinecandlestick-data\nIf you enter ‘url’ value at browser response is going to be like this:\n[\n  [\n    1499040000000,      // Open time\n    \"0.01634790\",       // Open\n    \"0.80000000\",       // High\n    \"0.01575800\",       // Low\n    \"0.01577100\",       // Close\n    \"148976.11427815\",  // Volume\n    1499644799999,      // Close time\n    \"2434.19055334\",    // Quote asset volume\n    308,                // Number of trades\n    \"1756.87402397\",    // Taker buy base asset volume\n    \"28.46694368\",      // Taker buy quote asset volume\n    \"17928899.62484339\" // Ignore.\n  ]\n]\n\ndata <- fromJSON(url) # get json and transform it to list()\ndata <- data[, 1:7] # let's left only 1:7 columns (from Open time to Close time)\nhead(data)\n\n\n\nA matrix: 6 × 7 of type chr\n\n    165051360000041693.5800000041750.0000000041525.0000000041610.010000001138.643370001650517199999\n    165051720000041610.0100000041699.0000000041434.4400000041462.760000001229.259360001650520799999\n    165052080000041462.7500000041600.0000000041419.2000000041522.380000001049.712440001650524399999\n    165052440000041522.3800000041940.0000000041451.0000000041855.690000001928.480910001650527999999\n    1.650528e+12 41855.6900000042050.3000000041741.1000000041922.970000002518.040900001650531599999\n    165053160000041922.9600000041971.9000000041743.9600000041803.700000001655.769930001650535199999\n\n\n\n\n\ntypeof(data) # check data type\ndata <- as.data.frame(data) # convert to dataframe\nhead(data)\n\n'character'\n\n\n\n\nA data.frame: 6 × 7\n\n    V1V2V3V4V5V6V7\n    <chr><chr><chr><chr><chr><chr><chr>\n\n\n    1165051360000041693.5800000041750.0000000041525.0000000041610.010000001138.643370001650517199999\n    2165051720000041610.0100000041699.0000000041434.4400000041462.760000001229.259360001650520799999\n    3165052080000041462.7500000041600.0000000041419.2000000041522.380000001049.712440001650524399999\n    4165052440000041522.3800000041940.0000000041451.0000000041855.690000001928.480910001650527999999\n    51.650528e+12 41855.6900000042050.3000000041741.1000000041922.970000002518.040900001650531599999\n    6165053160000041922.9600000041971.9000000041743.9600000041803.700000001655.769930001650535199999\n\n\n\n\n\n# fix columns names\ncolnames(data) <- c(\"Open_time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Close_time\")\nhead(data) # looks better, but columns are characters still\n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <chr><chr><chr><chr><chr><chr><chr>\n\n\n    1165051360000041693.5800000041750.0000000041525.0000000041610.010000001138.643370001650517199999\n    2165051720000041610.0100000041699.0000000041434.4400000041462.760000001229.259360001650520799999\n    3165052080000041462.7500000041600.0000000041419.2000000041522.380000001049.712440001650524399999\n    4165052440000041522.3800000041940.0000000041451.0000000041855.690000001928.480910001650527999999\n    51.650528e+12 41855.6900000042050.3000000041741.1000000041922.970000002518.040900001650531599999\n    6165053160000041922.9600000041971.9000000041743.9600000041803.700000001655.769930001650535199999\n\n\n\n\n\nis.numeric(data[,1]) # check 1st column type is numeric\nis.numeric(data[,2]) # check 2nd column type is numeric\n\nFALSE\n\n\nFALSE\n\n\n\ndata <- as.data.frame(sapply(data, as.numeric)) # convert all columns to numeric\nhead(data) # good, its double now\n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    11.650514e+1241693.5841750.041525.0041610.011138.6431.650517e+12\n    21.650517e+1241610.0141699.041434.4441462.761229.2591.650521e+12\n    31.650521e+1241462.7541600.041419.2041522.381049.7121.650524e+12\n    41.650524e+1241522.3841940.041451.0041855.691928.4811.650528e+12\n    51.650528e+1241855.6942050.341741.1041922.972518.0411.650532e+12\n    61.650532e+1241922.9641971.941743.9641803.701655.7701.650535e+12\n\n\n\n\nFinal stage is to convert Open_time and Close_time to dates.\n\ndata$Open_time <- as.POSIXct(data$Open_time/1e3, origin = '1970-01-01')\ndata$Close_time <- as.POSIXct(data$Close_time/1e3, origin = '1970-01-01')\n\nhead(data) \n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <dttm><dbl><dbl><dbl><dbl><dbl><dttm>\n\n\n    12022-04-21 07:00:0041693.5841750.041525.0041610.011138.6432022-04-21 07:59:59\n    22022-04-21 08:00:0041610.0141699.041434.4441462.761229.2592022-04-21 08:59:59\n    32022-04-21 09:00:0041462.7541600.041419.2041522.381049.7122022-04-21 09:59:59\n    42022-04-21 10:00:0041522.3841940.041451.0041855.691928.4812022-04-21 10:59:59\n    52022-04-21 11:00:0041855.6942050.341741.1041922.972518.0412022-04-21 11:59:59\n    62022-04-21 12:00:0041922.9641971.941743.9641803.701655.7702022-04-21 12:59:59\n\n\n\n\n\ntail(data) # check last records\n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <dttm><dbl><dbl><dbl><dbl><dbl><dttm>\n\n\n    952022-04-25 05:00:0039095.8139153.9438961.6439091.171205.51582022-04-25 05:59:59\n    962022-04-25 06:00:0039091.1739294.7639086.3739253.711443.33182022-04-25 06:59:59\n    972022-04-25 07:00:0039253.7039256.2839055.7139139.74 896.85542022-04-25 07:59:59\n    982022-04-25 08:00:0039139.7439230.5038947.4238975.221057.49002022-04-25 08:59:59\n    992022-04-25 09:00:0038975.2139057.9738590.0038636.352814.97162022-04-25 09:59:59\n    1002022-04-25 10:00:0038636.3538675.6838200.0038534.993528.23552022-04-25 10:59:59"
  },
  {
    "objectID": "23-r-xml.html#google-services",
    "href": "23-r-xml.html#google-services",
    "title": "4  Data collection and saving",
    "section": "4.5 Google Services",
    "text": "4.5 Google Services\n\n4.5.1 Google Spreadsheets\n\nTHIS CHAPTER IS UNDER CONSTRUCTION / Working with Google Spreadsheets need account authorization.\n\ngooglesheets4 is a package to work with Google Sheets from R.\n\n#install.packages(\"googlesheets4\")\nlibrary(googlesheets4)\n\nYou can read google documents after authentification on google service. There is sample code:\nread_sheet(\"https://docs.google.com/spreadsheets/d/1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit#gid=780868077\")\ngs4_deauth()\nLet’s read sample dataset gapminder. It detailed described in next paragraph.\n\n# gs4_example(\"gapminder\")\n\n\n\n\n4.5.2 Google Search Trends\nGoogle Trends is a service for analyzing search requests by many filters like region (continent, country, locality), period (year, month), information category (business, education, hobby, healthcare), information type (news, shopping, video, images) https://trends.google.com/trends/\n\n# install.packages('gtrendsR')\n# install.packages('ggplot2')\nlibrary(gtrendsR) # loading package for Google Trends queries\nlibrary(ggplot2)\n\nLet’s configure out google trends query params\n\nkeywords = c(\"Bitcoin\", \"FC Barcelona\") # search keywords\ncountry = c('AT') # search region from https://support.google.com/business/answer/6270107?hl=en\ntime = (\"2021-01-01 2021-06-01\") # period\nchannel = 'web' # search channel: google search ('news' - google news, 'images' - google images)\n\n\n# query\ntrends = gtrends(keywords, gprop = channel, geo = country, time = time, tz = \"UTC\")\n\n\ntime_trend = trends$interest_over_time\nhead(time_trend)\n\n\n\nA data.frame: 6 × 7\n\n    datehitskeywordgeotimegpropcategory\n    <dttm><chr><chr><chr><chr><chr><int>\n\n\n    12021-01-0136BitcoinAT2021-01-01 2021-06-01web0\n    22021-01-0267BitcoinAT2021-01-01 2021-06-01web0\n    32021-01-0374BitcoinAT2021-01-01 2021-06-01web0\n    42021-01-0457BitcoinAT2021-01-01 2021-06-01web0\n    52021-01-0553BitcoinAT2021-01-01 2021-06-01web0\n    62021-01-0666BitcoinAT2021-01-01 2021-06-01web0\n\n\n\n\n\nplot <- ggplot(data=time_trend, aes(x=date, y=hits, group=keyword, col=keyword)) +\n  geom_line() +\n  xlab('Time') + \n  ylab('Relative Interest') + \n  theme(legend.title = element_blank(), legend.position=\"bottom\", legend.text=element_text(size=15)) + \n  ggtitle(\"Google Search Volume\")  \n\nplot"
  },
  {
    "objectID": "23-r-xml.html#sql-with-sqlite-sample",
    "href": "23-r-xml.html#sql-with-sqlite-sample",
    "title": "4  Data collection and saving",
    "section": "4.6 SQL (with SQLite sample)",
    "text": "4.6 SQL (with SQLite sample)\nWe are going to review working with database on SQLite, becouse it allows us not to install DB-server and start working with simple file.\nFor now we will use RSQLite package.\n\n# install.packages(\"RSQLite\")\nlibrary(RSQLite)\n\n\n# let's use mtcars dataset\n\ndata(\"mtcars\") # loads the data\nhead(mtcars) # preview the data\n\n\n\nA data.frame: 6 × 11\n\n    mpgcyldisphpdratwtqsecvsamgearcarb\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    Mazda RX421.061601103.902.62016.460144\n    Mazda RX4 Wag21.061601103.902.87517.020144\n    Datsun 71022.84108 933.852.32018.611141\n    Hornet 4 Drive21.462581103.083.21519.441031\n    Hornet Sportabout18.783601753.153.44017.020032\n    Valiant18.162251052.763.46020.221031\n\n\n\n\n\nI need this code for book successful building (remove database file if exists):\n\n\n#Define the file name that will be deleted\nfn <- paste0(\"../../data/cars.sqlite\")\n#Check its existence\nif (file.exists(fn)) {\n  #Delete file if it exists\n  file.remove(fn)\n}\n\nTRUE\n\n\nNow, let’s create new:\n\n# create new db file\ndb_path = paste0(\"../../data/cars.sqlite\")\n# create connection\nconn <- dbConnect(RSQLite::SQLite(), \n                    db_path,\n                    overwrite = TRUE, append = FALSE) # for lecture content only\n\n\n# Write the mtcars dataset into a table names mtcars_data\ndbWriteTable(conn, \"cars_table\", mtcars)\n# List all the tables available in the database\ndbListTables(conn)\n\n\n'cars_table'\n\n\n\ntable_data <- dbGetQuery(conn, \"SELECT * FROM cars_table\")\nhead(table_data)\n\n\n\nA data.frame: 6 × 11\n\n    mpgcyldisphpdratwtqsecvsamgearcarb\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    121.061601103.902.62016.460144\n    221.061601103.902.87517.020144\n    322.84108 933.852.32018.611141\n    421.462581103.083.21519.441031\n    518.783601753.153.44017.020032\n    618.162251052.763.46020.221031\n\n\n\n\n\n# close connection\ndbDisconnect(conn)\n\nYou can write complex queries for many tables if you knowledge of SQL allows."
  },
  {
    "objectID": "23-r-xml.html#web-pages-html",
    "href": "23-r-xml.html#web-pages-html",
    "title": "4  Data collection and saving",
    "section": "4.7 Web-pages (HTML)",
    "text": "4.7 Web-pages (HTML)\nSometimes decision making needs scrap data from web sources and pages.\nLet’s try to parse data from Wikipedia as table.\n\n#install.packages(\"rvest\")\nlibrary(rvest) # Parsing of HTML/XML files\n\nGo to web page https://en.wikipedia.org/wiki/List_of_largest_banks and check it.\n\n# fix URL\nurl <- \"https://en.wikipedia.org/wiki/List_of_largest_banks\"\n#url <- \"data/List of largest banks - Wikipedia_.html\"\n\n\n# read html content of the page\npage <- read_html(url)\npage\n\n{html_document}\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...\n\n\n\n# read all yables on page\ntables <- html_nodes(page, \"table\")\ntables\n\n{xml_nodeset (4)}\n[1] <table class=\"box-Missing_information plainlinks metadata ambox ambox-con ...\n[2] <table class=\"wikitable sortable mw-collapsible\"><tbody>\\n<tr>\\n<th data- ...\n[3] <table class=\"wikitable sortable mw-collapsible\">\\n<caption>Number of ban ...\n[4] <table class=\"wikitable sortable mw-collapsible\"><tbody>\\n<tr>\\n<th data- ...\n\n\nFor now, let’s read a table of Total Assets in US Billion\n\n# with pipe operator\n#tables[2] %>% \n #   html_table(fill = TRUE) %>% \n #   as.data.frame()\n#without pipe operator\nassets_table <- as.data.frame(html_table(tables[2], fill = TRUE))   \nhead(assets_table)\n\n\n\nA data.frame: 6 × 3\n\n    RankBank.nameTotal.assets.2020..US..billion.\n    <int><chr><chr>\n\n\n    11Industrial and Commercial Bank of China5,518.00\n    22China Construction Bank                4,400.00\n    33Agricultural Bank of China             4,300.00\n    44Bank of China                          4,200.00\n    55JPMorgan Chase                         3,831.65\n    66Mitsubishi UFJ Financial Group         3,175.21\n\n\n\n\nNext is reading data of market capitalization table (4th):\n\ncapital_table <- as.data.frame(html_table(tables[4], fill = TRUE))   \nhead(capital_table)\n\n\n\nA data.frame: 6 × 3\n\n    RankBank.nameMarket.cap.US..billion.\n    <int><chr><dbl>\n\n\n    11JPMorgan Chase                         368.78\n    22Industrial and Commercial Bank of China295.65\n    33Bank of America                        279.73\n    44Wells Fargo                            214.34\n    55China Construction Bank                207.98\n    66Agricultural Bank of China             181.49\n\n\n\n\nAnd now let’s merge() this two datasets:\n\nmerged_data <- merge(assets_table, capital_table, by = \"Bank.name\")\nhead(merged_data)\n\n\n\nA data.frame: 6 × 5\n\n    Bank.nameRank.xTotal.assets.2020..US..billion.Rank.yMarket.cap.US..billion.\n    <chr><int><chr><int><dbl>\n\n\n    1Agricultural Bank of China              34,300.00 6181.49\n    2Australia and New Zealand Banking Group48661.72  26 54.88\n    3Banco Bilbao Vizcaya Argentaria        42782.16  37 37.42\n    4Banco Bradesco                         79345.21  18 74.67\n    5Banco Santander                        161,702.6117 75.47\n    6Bank of America                         82,434.08 3279.73\n\n\n\n\n\n4.7.1 Task 3\nFrom a page https://en.wikipedia.org/wiki/List_of_largest_banks read and merge by country named tables:\n\nNumber of banks in the top 100 by total assets\nTotal market capital (US$ billion) across the top 70 banks by country\n\nSolution\n\nlibrary(rvest)\nurl <- \"https://en.wikipedia.org/wiki/List_of_largest_banks\" # got to url in other tab\n#url <- \"data/List of largest banks - Wikipedia_.html\"\npage_data <- read_html(url) # read html content\n\ntables <- html_nodes(page_data, \"table\")\nhtml_table(tables[1]) #its not needed table\n\n\n\n    \nA tibble: 1  2\n\n    X1X2\n    <lgl><chr>\n\n\n    NAThis article is missing information about Revenue and Employment. Please expand the article to include this information. Further details may exist on the talk page.  (September 2020)\n\n\n\n\n\n\n\nhtml_table(tables[3]) # thats solution for \"Number of banks in the top 100 by total assets\"\n#check the end of table. There are NA record\n# lets remove it\n\n\n\n    \nA tibble: 26  3\n\n    RankCountryNumber\n    <int><chr><int>\n\n\n    1China         19\n    2United States 11\n    3Japan          8\n    4United Kingdom 6\n    4France         6\n    4South Korea    6\n    5Canada         5\n    5Germany        5\n    6Australia      4\n    6Brazil         4\n    6Spain          4\n    7Netherlands    3\n    7Singapore      3\n    7Sweden         3\n    7Switzerland    3\n    8Italy          2\n    9India          1\n    9Austria        1\n    9Belgium        1\n    9Denmark        1\n    9Finland        1\n    9Norway         1\n    9Russia         1\n    9Qatar          1\n    9NA            NA\n    9NA            NA\n\n\n\n\n\n\n\ntable1 <- as.data.frame(html_table(tables[3]))\ntable1 <- table1[!is.na(table1$Country), ]\ntable1 # now it OK!\n\n\n\nA data.frame: 24 × 3\n\n    RankCountryNumber\n    <int><chr><int>\n\n\n    11China         19\n    22United States 11\n    33Japan          8\n    44United Kingdom 6\n    54France         6\n    64South Korea    6\n    75Canada         5\n    85Germany        5\n    96Australia      4\n    106Brazil         4\n    116Spain          4\n    127Netherlands    3\n    137Singapore      3\n    147Sweden         3\n    157Switzerland    3\n    168Italy          2\n    179India          1\n    189Austria        1\n    199Belgium        1\n    209Denmark        1\n    219Finland        1\n    229Norway         1\n    239Russia         1\n    249Qatar          1\n\n\n\n\n\n# SOlution for \"Total market capital (US$ billion) across the top 70 banks by country\"\n# compare this with table on a given page\ntable2 <- as.data.frame(html_table(tables[4]))\ntable2 # now it OK!\n\n\n\nA data.frame: 50 × 3\n\n    RankBank.nameMarket.cap.US..billion.\n    <int><chr><dbl>\n\n\n     1JPMorgan Chase                         368.78\n     2Industrial and Commercial Bank of China295.65\n     3Bank of America                        279.73\n     4Wells Fargo                            214.34\n     5China Construction Bank                207.98\n     6Agricultural Bank of China             181.49\n     7HSBC Holdings PLC                      169.47\n     8Citigroup Inc.                         163.58\n     9Bank of China                          151.15\n    10China Merchants Bank                   133.37\n    11Royal Bank of Canada                   113.80\n    12Toronto-Dominion Bank                  106.61\n    13Commonwealth Bank                       99.77\n    14HDFC Bank                              105.90\n    15U.S. Bancorp                            84.40\n    16Goldman Sachs                           78.70\n    17Banco Santander                         75.47\n    18Banco Bradesco                          74.67\n    19Morgan Stanley                          73.93\n    20Westpac                                 67.84\n    21Mitsubishi UFJ Financial Group          66.20\n    22Scotiabank                              65.48\n    23PNC Financial Services                  63.11\n    24Bank of Communications                  61.85\n    25BNP Paribas                             59.36\n    26Australia and New Zealand Banking Group 54.88\n    27National Australia Bank                 51.68\n    28Lloyds Banking Group                    51.19\n    29Sumitomo Mitsui Financial Group         49.85\n    30Bank of Montreal                        48.12\n    31UBS                                     45.92\n    32ING Group                               44.97\n    33Capital One                             43.22\n    34The Bank of New York Mellon             42.58\n    35China Minsheng Bank                     39.13\n    36China CITIC Bank                        38.55\n    37Banco Bilbao Vizcaya Argentaria         37.42\n    38Mizuho Financial Group                  36.95\n    39Intesa Sanpaolo                         36.90\n    40Credit Agricole                         34.89\n    41Canadian Imperial Bank of Commerce      34.87\n    42Royal Bank of Scotland                  33.95\n    43Barclays                                33.26\n    44Credit Suisse                           30.75\n    45Nordea                                  29.59\n    46Standard Chartered                      29.37\n    47KBC Bank                                27.40\n    48UniCredit                               26.88\n    49Societe Generale                        21.27\n    50Deutsche Bank                           15.77"
  },
  {
    "objectID": "23-r-xml.html#набори-даних",
    "href": "23-r-xml.html#набори-даних",
    "title": "4  Data collection and saving",
    "section": "4.8 Набори даних",
    "text": "4.8 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml"
  },
  {
    "objectID": "23-r-xml.html#references",
    "href": "23-r-xml.html#references",
    "title": "4  Data collection and saving",
    "section": "4.9 References",
    "text": "4.9 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl"
  },
  {
    "objectID": "22-r-csv.html#what-is-csv-comma-separated-values",
    "href": "22-r-csv.html#what-is-csv-comma-separated-values",
    "title": "3  CSV",
    "section": "3.1 What is CSV (Comma Separated Values)?",
    "text": "3.1 What is CSV (Comma Separated Values)?\nCSV - comma separated values.\n\n# lets check current working directory to write correct files path\ngetwd()\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en'\n\n\nYou can use / or \\\\ for writing correct path in R. For example:\n\npath = \"d:/projects/file.csv\"\npath = \"d:\\\\projects\\\\file.csv\"\n\nTo combine path use paste() or paste0() functions\n\nwork_dir = getwd()\nwork_dir \n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en'\n\n\n\nfile_name = \"temp_file.csv\"\nfile_path = paste0(work_dir, \"/\", file_name)\nfile_path\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en/temp_file.csv'\n\n\n\nfile_path = paste(work_dir, file_name, sep = \"/\")\nfile_path\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en/temp_file.csv'"
  },
  {
    "objectID": "22-r-csv.html#sample-dataset-description",
    "href": "22-r-csv.html#sample-dataset-description",
    "title": "3  CSV",
    "section": "3.2 Sample dataset description",
    "text": "3.2 Sample dataset description\nInformation about dataset from kaggle.com. Original file located at url: https://www.kaggle.com/radmirzosimov/telecom-users-dataset.\nAny business wants to maximize the number of customers. To achieve this goal, it is important not only to try to attract new ones, but also to retain existing ones. Retaining a client will cost the company less than attracting a new one. In addition, a new client may be weakly interested in business services and it will be difficult to work with him, while old clients already have the necessary data on interaction with the service.\nAccordingly, predicting the churn, we can react in time and try to keep the client who wants to leave. Based on the data about the services that the client uses, we can make him a special offer, trying to change his decision to leave the operator. This will make the task of retention easier to implement than the task of attracting new users, about which we do not know anything yet.\nYou are provided with a dataset from a telecommunications company. The data contains information about almost six thousand users, their demographic characteristics, the services they use, the duration of using the operator’s services, the method of payment, and the amount of payment.\nThe task is to analyze the data and predict the churn of users (to identify people who will and will not renew their contract). The work should include the following mandatory items:\n\nDescription of the data (with the calculation of basic statistics);\nResearch of dependencies and formulation of hypotheses;\nBuilding models for predicting the outflow (with justification for the choice of a particular model) 4. based on tested hypotheses and identified relationships;\nComparison of the quality of the obtained models.\n\nFields description:\n\ncustomerID - customer id\ngender - client gender (male / female)\nSeniorCitizen - is the client retired (1, 0)\nPartner - is the client married (Yes, No)\ntenure - how many months a person has been a client of the company\nPhoneService - is the telephone service connected (Yes, No)\nMultipleLines - are multiple phone lines connected (Yes, No, No phone service)\nInternetService - client’s Internet service provider (DSL, Fiber optic, No)\nOnlineSecurity - is the online security service connected (Yes, No, No internet service)\nOnlineBackup - is the online backup service activated (Yes, No, No internet service)\nDeviceProtection - does the client have equipment insurance (Yes, No, No internet service)\nTechSupport - is the technical support service connected (Yes, No, No internet service)\nStreamingTV - is the streaming TV service connected (Yes, No, No internet service)\nStreamingMovies - is the streaming cinema service activated (Yes, No, No internet service)\nContract - type of customer contract (Month-to-month, One year, Two year)\nPaperlessBilling - whether the client uses paperless billing (Yes, No)\nPaymentMethod - payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\nMonthlyCharges - current monthly payment\nTotalCharges - the total amount that the client paid for the services for the entire time\nChurn - whether there was a churn (Yes or No)"
  },
  {
    "objectID": "22-r-csv.html#reading",
    "href": "22-r-csv.html#reading",
    "title": "3  CSV",
    "section": "3.3 Reading",
    "text": "3.3 Reading\nThare are few methods for reading/writing csv in base package:\n\nread.csv(), write.csv - default data separator is ,, decimal is separator ..\nread.csv2(), write.csv2 - default data separator is ;, decimal is separator ,.\n\nBefore using any new function check it usage information with help(function_name) or ?function_name, example: ?read.csv.\nYou can read (current data set has NA values as example, there are no NA in original datase):\n\ndata <- read.csv(\"../../data/telecom_users.csv\") # default reading\nstr(data)\n\n'data.frame':   5986 obs. of  22 variables:\n $ X               : int  1869 4528 6344 6739 432 2215 5260 6001 1480 5137 ...\n $ customerID      : chr  \"7010-BRBUU\" \"9688-YGXVR\" \"9286-DOJGF\" \"6994-KERXL\" ...\n $ gender          : chr  \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ SeniorCitizen   : int  0 0 1 0 0 0 0 0 0 1 ...\n $ Partner         : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Dependents      : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n $ tenure          : int  72 44 38 4 2 70 33 1 39 55 ...\n $ PhoneService    : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ MultipleLines   : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ InternetService : chr  \"No\" \"Fiber optic\" \"Fiber optic\" \"DSL\" ...\n $ OnlineSecurity  : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ OnlineBackup    : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ DeviceProtection: chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ TechSupport     : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ StreamingTV     : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ StreamingMovies : chr  \"No internet service\" \"No\" \"No\" \"Yes\" ...\n $ Contract        : chr  \"Two year\" \"Month-to-month\" \"Month-to-month\" \"Month-to-month\" ...\n $ PaperlessBilling: chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ PaymentMethod   : chr  \"Credit card (automatic)\" \"Credit card (automatic)\" \"Bank transfer (automatic)\" \"Electronic check\" ...\n $ MonthlyCharges  : chr  \"24.1\" \"88.15\" \"74.95\" \"55.9\" ...\n $ TotalCharges    : num  1735 3973 2870 238 120 ...\n $ Churn           : chr  \"No\" \"No\" \"Yes\" \"No\" ...\n\n\n\ndata <- read.csv(\"../../data/telecom_users.csv\",\n                  sep = \",\", # comma not only possibel separator\n                  dec = \".\", # decimal separator can be different\n                  na.strings = c(\"\", \"NA\", \"NULL\")) # you can define NA values\n\n\nstr(data) # chack data structure / types/ values\n\n'data.frame':   5986 obs. of  22 variables:\n $ X               : int  1869 4528 6344 6739 432 2215 5260 6001 1480 5137 ...\n $ customerID      : chr  \"7010-BRBUU\" \"9688-YGXVR\" \"9286-DOJGF\" \"6994-KERXL\" ...\n $ gender          : chr  \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ SeniorCitizen   : int  0 0 1 0 0 0 0 0 0 1 ...\n $ Partner         : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Dependents      : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n $ tenure          : int  72 44 38 4 2 70 33 1 39 55 ...\n $ PhoneService    : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ MultipleLines   : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ InternetService : chr  \"No\" \"Fiber optic\" \"Fiber optic\" \"DSL\" ...\n $ OnlineSecurity  : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ OnlineBackup    : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ DeviceProtection: chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ TechSupport     : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ StreamingTV     : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ StreamingMovies : chr  \"No internet service\" \"No\" \"No\" \"Yes\" ...\n $ Contract        : chr  \"Two year\" \"Month-to-month\" \"Month-to-month\" \"Month-to-month\" ...\n $ PaperlessBilling: chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ PaymentMethod   : chr  \"Credit card (automatic)\" \"Credit card (automatic)\" \"Bank transfer (automatic)\" \"Electronic check\" ...\n $ MonthlyCharges  : num  24.1 88.2 75 55.9 53.5 ...\n $ TotalCharges    : num  1735 3973 2870 238 120 ...\n $ Churn           : chr  \"No\" \"No\" \"Yes\" \"No\" ...\n\n\n\nhead(data, 2) # top 6 rows, use n = X, for viewing top X lines\n\n\n\nA data.frame: 2 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><dbl><dbl><chr>\n\n\n    118697010-BRBUUMale  0YesYes72YesYesNo         ...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)24.101734.65No\n    245289688-YGXVRFemale0No No 44YesNo Fiber optic...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)88.153973.20No\n\n\n\n\n\nis.data.frame(data) # if data is data.frame\n\nTRUE\n\n\n\nanyNA(data) # if dataframe contains any NA values\n\nTRUE\n\n\n\nlapply(data, anyNA)\n#lapply(, any) #check NA by 2nd dimension - columns\n\n\n    $X\n        FALSE\n    $customerID\n        FALSE\n    $gender\n        FALSE\n    $SeniorCitizen\n        FALSE\n    $Partner\n        FALSE\n    $Dependents\n        FALSE\n    $tenure\n        FALSE\n    $PhoneService\n        FALSE\n    $MultipleLines\n        FALSE\n    $InternetService\n        FALSE\n    $OnlineSecurity\n        FALSE\n    $OnlineBackup\n        FALSE\n    $DeviceProtection\n        FALSE\n    $TechSupport\n        FALSE\n    $StreamingTV\n        FALSE\n    $StreamingMovies\n        FALSE\n    $Contract\n        FALSE\n    $PaperlessBilling\n        FALSE\n    $PaymentMethod\n        FALSE\n    $MonthlyCharges\n        TRUE\n    $TotalCharges\n        TRUE\n    $Churn\n        FALSE\n\n\n\nCheck MonthlyCharges: TRUE and TotalCharges: TRUE. These columns has NA-values.\nLet’s replace them with mean:\n\ndata[is.na(data$TotalCharges), \"TotalCharges\"] <- mean(data$TotalCharges, na.rm = T)\ndata[is.na(data$MonthlyCharges), \"MonthlyCharges\"] <- mean(data$MonthlyCharges, na.rm = T)\n\n\nany(is.na(data)) # check for NA\n\nFALSE\n\n\nYou can write data with write.csv(), write.csv2() from base package.\n\nwrite.csv(data, file = \"../../data/cleaned_data.csv\", row.names = F)\n# by default row.names = TRUE and file will contain first column with row numbers 1,2, ..., N\n\nERROR: Error in as.data.frame.default(x[[i]], optional = TRUE): cannot coerce class '\"function\"' to a data.frame"
  },
  {
    "objectID": "22-r-csv.html#readr-package",
    "href": "22-r-csv.html#readr-package",
    "title": "3  CSV",
    "section": "3.4 readr package",
    "text": "3.4 readr package\nOne more useful package is readr. Examples of using:\n\n# library(readr)\n# data <- read_csv(file = \"../../data/telecom_users.csv\")\n# data <- read_csv2(file = \"../../data/telecom_users.csv\")`"
  },
  {
    "objectID": "23-r-xlsx.html",
    "href": "23-r-xlsx.html",
    "title": "4  MS Excel (xlsx)",
    "section": "",
    "text": "You need this packages for code execution:"
  },
  {
    "objectID": "23-r-xlsx.html#xlsx-format",
    "href": "23-r-xlsx.html#xlsx-format",
    "title": "4  MS Excel (xlsx)",
    "section": "4.1 XLSX-format",
    "text": "4.1 XLSX-format\nThere are many packages to read/write MS Excel files. xlsx one of the most useful.\n\n# install.packages(\"xlsx\") #install before use it\n\n\nlibrary(xlsx)\n\n\nany(grepl(\"xlsx\", installed.packages())) # check if package installed\n\nTRUE\n\n\n?read.xlsx - review package functions and params\nLet’s read the data telecom_users.xlsx:\n\ndata <- read.xlsx(\"../../data/telecom_users.xlsx\", sheetIndex = 1)\n# sheetIndex = 1 - select sheet to read, or use sheetName = \"sheet1\" to read by Name\nhead(data)\n\n\n\nA data.frame: 6 × 21\n\n    customerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetServiceOnlineSecurity...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <chr><chr><dbl><chr><chr><dbl><chr><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><dbl><dbl><chr>\n\n\n    17010-BRBUUMale  0YesYes72YesYes             No         No internet service...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.101734.65No \n    29688-YGXVRFemale0No No 44YesNo              Fiber opticNo                 ...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    39286-DOJGFFemale1YesNo 38YesYes             Fiber opticNo                 ...No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    46994-KERXLMale  0No No  4YesNo              DSL        No                 ...No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.90 238.50No \n    52181-UAESMMale  0No No  2YesNo              DSL        Yes                ...Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    64312-GVYNHFemale0YesNo 70No No phone serviceDSL        Yes                ...Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\n\n# You can also use startRow, endRow and other params to define how much data read\ndata <- read.xlsx(\"../../data/telecom_users.xlsx\", sheetIndex = 1, endRow = 100)\nhead(data)\n\n\n\nA data.frame: 6 × 21\n\n    customerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetServiceOnlineSecurity...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <chr><chr><dbl><chr><chr><dbl><chr><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><dbl><dbl><chr>\n\n\n    17010-BRBUUMale  0YesYes72YesYes             No         No internet service...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.101734.65No \n    29688-YGXVRFemale0No No 44YesNo              Fiber opticNo                 ...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    39286-DOJGFFemale1YesNo 38YesYes             Fiber opticNo                 ...No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    46994-KERXLMale  0No No  4YesNo              DSL        No                 ...No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.90 238.50No \n    52181-UAESMMale  0No No  2YesNo              DSL        Yes                ...Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    64312-GVYNHFemale0YesNo 70No No phone serviceDSL        Yes                ...Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\nLet’s replace Churn values Yes/No by 1/0:\n\nhead(data$Churn)\n\n\n'No''No''Yes''No''No''No'\n\n\n\ndata$Churn <- ifelse(data$Churn == \"Yes\", 1, 0)\n\n\nhead(data$Churn)\n\n\n001000\n\n\nWrite final data to excel:\n\nwrite.xlsx(data, file = \"../../data/final_telecom_data.xlsx\")"
  },
  {
    "objectID": "23-r-xlsx.html#task-1",
    "href": "23-r-xlsx.html#task-1",
    "title": "4  MS Excel (xlsx)",
    "section": "4.2 Task 1",
    "text": "4.2 Task 1\nDownload from kaggle.com and read dataset Default_Fin.csv: https://www.kaggle.com/kmldas/loan-default-prediction\nDescription:\nThis is a synthetic dataset created using actual data from a financial institution. The data has been modified to remove identifiable features and the numbers transformed to ensure they do not link to original source (financial institution).\nThis is intended to be used for academic purposes for beginners who want to practice financial analytics from a simple financial dataset\n\nIndex - This is the serial number or unique identifier of the loan taker\nEmployed - This is a Boolean 1= employed 0= unemployed\nBank.Balance - Bank Balance of the loan taker\nAnnual.Salary - Annual salary of the loan taker\n\nDefaulted - This is a Boolean 1= defaulted 0= not defaulted\n\n\nCheck what columns has missing values\nCount default and non-default clients / and parts of total clients in %\nCount Employed clients\nCount Employed Default clients\nAverage salary by Employed clients\nRename columns to “id”, “empl”, “balance”, “salary”, “default”\n\n\nSolution for Task 1\n\ndata <- read.csv(\"../../data/Default_Fin.csv\")\nhead(data)\n\n\n\nA data.frame: 6 × 5\n\n    IndexEmployedBank.BalanceAnnual.SalaryDefaulted.\n    <int><int><dbl><dbl><int>\n\n\n    111 8754.36532339.560\n    220 9806.16145273.560\n    33112882.60381205.680\n    441 6351.00428453.880\n    551 9427.92461562.000\n    66011035.08 89898.720\n\n\n\n\n\n\nCheck what columns has missing values\n\n\n\nanyNA(data)\n\nFALSE\n\n\n\n\nCount default and non-default clients / and parts of total clients in %\n\n\n\ndef_count <- nrow(data[data$Defaulted. == 1, ])\nno_def_count <- nrow(data[data$Defaulted. == 0, ])\ndef_count\nno_def_count \n\n333\n\n\n9667\n\n\n\ndef_count / nrow(data) * 100 # part defaults\nno_def_count / nrow(data) * 100 # part non-defaults\n\n3.33\n\n\n96.67\n\n\n\n\nCount Employed clients\n\n\n\nempl <- data[data$Employed == 1, ]\nnrow(empl)\n\n7056\n\n\n\n\nCount Employed Default clients\n\n\n\nempl <- data[data$Employed == 1 & data$Defaulted. == 1, ]\nnrow(empl)\n\n206\n\n\n\n\nAverage salary by Employed clients\n\n\n\nempl <- data[data$Employed == 1, ]\nmean(empl$Annual.Salary)\n\n480143.43414966\n\n\n\n\nRename columns to “id”, “empl”, “balance”, “salary”, “default”:\n\n\n\ncolnames(data) <- c(\"id\", \"empl\", \"balance\", \"salary\", \"default\")\nhead(data)\n\n\n\nA data.frame: 6 × 5\n\n    idemplbalancesalarydefault\n    <int><int><dbl><dbl><int>\n\n\n    111 8754.36532339.560\n    220 9806.16145273.560\n    33112882.60381205.680\n    441 6351.00428453.880\n    551 9427.92461562.000\n    66011035.08 89898.720"
  },
  {
    "objectID": "23-r-xlsx.html#набори-даних",
    "href": "23-r-xlsx.html#набори-даних",
    "title": "4  MS Excel (xlsx)",
    "section": "4.3 Набори даних",
    "text": "4.3 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml"
  },
  {
    "objectID": "23-r-xlsx.html#references",
    "href": "23-r-xlsx.html#references",
    "title": "4  MS Excel (xlsx)",
    "section": "4.4 References",
    "text": "4.4 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl"
  },
  {
    "objectID": "24-r-xml.html",
    "href": "24-r-xml.html",
    "title": "5  XML",
    "section": "",
    "text": "You need this packages for code execution:"
  },
  {
    "objectID": "24-r-xml.html#xml-extensible-markup-language",
    "href": "24-r-xml.html#xml-extensible-markup-language",
    "title": "5  XML",
    "section": "5.1 XML (eXtensible Markup Language)",
    "text": "5.1 XML (eXtensible Markup Language)\nFor our example we will use data from data/employes.xml. File contains records with info:\n<RECORDS>\n   <EMPLOYEE>\n      <ID>1</ID>\n      <NAME>Rick</NAME>\n      <SALARY>623.3</SALARY>\n      <STARTDATE>1/1/2012</STARTDATE>\n      <DEPT>IT</DEPT>\n   </EMPLOYEE>\n   ...\n</RECORDS>\n\n#install.packages(\"XML\")\nlibrary(\"XML\")\n#install.packages(\"methods\")\nlibrary(\"methods\")\n\n\nresult <- xmlParse(file = \"../../data/employes.xml\")\nprint(result)\n\n<?xml version=\"1.0\"?>\n<RECORDS>\n  <EMPLOYEE>\n    <ID>1</ID>\n    <NAME>Rick</NAME>\n    <SALARY>623.3</SALARY>\n    <STARTDATE>1/1/2012</STARTDATE>\n    <DEPT>IT</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>2</ID>\n    <NAME>Dan</NAME>\n    <SALARY>515.2</SALARY>\n    <STARTDATE>9/23/2013</STARTDATE>\n    <DEPT>Operations</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>3</ID>\n    <NAME>Michelle</NAME>\n    <SALARY>611</SALARY>\n    <STARTDATE>11/15/2014</STARTDATE>\n    <DEPT>IT</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>4</ID>\n    <NAME>Ryan</NAME>\n    <SALARY>729</SALARY>\n    <STARTDATE>5/11/2014</STARTDATE>\n    <DEPT>HR</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>5</ID>\n    <NAME>Gary</NAME>\n    <SALARY>843.25</SALARY>\n    <STARTDATE>3/27/2015</STARTDATE>\n    <DEPT>Finance</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>6</ID>\n    <NAME>Nina</NAME>\n    <SALARY>578</SALARY>\n    <STARTDATE>5/21/2013</STARTDATE>\n    <DEPT>IT</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>7</ID>\n    <NAME>Simon</NAME>\n    <SALARY>632.8</SALARY>\n    <STARTDATE>7/30/2013</STARTDATE>\n    <DEPT>Operations</DEPT>\n  </EMPLOYEE>\n  <EMPLOYEE>\n    <ID>8</ID>\n    <NAME>Guru</NAME>\n    <SALARY>722.5</SALARY>\n    <STARTDATE>6/17/2014</STARTDATE>\n    <DEPT>Finance</DEPT>\n  </EMPLOYEE>\n</RECORDS>\n \n\n\n\nrootnode <- xmlRoot(result) # reading rootnode of xml document\nrootnode[[1]] # reading first record\n\n<EMPLOYEE>\n  <ID>1</ID>\n  <NAME>Rick</NAME>\n  <SALARY>623.3</SALARY>\n  <STARTDATE>1/1/2012</STARTDATE>\n  <DEPT>IT</DEPT>\n</EMPLOYEE> \n\n\n\nrootnode[[1]][[2]] # reading first record in root node and second tag, its <NAME>\n\n<NAME>Rick</NAME> \n\n\nFor us the best way is to get dataframe:\n\nxmldataframe <- xmlToDataFrame(\"../../data/employes.xml\")\nxmldataframe\n\n\n\nA data.frame: 8 × 5\n\n    IDNAMESALARYSTARTDATEDEPT\n    <chr><chr><chr><chr><chr>\n\n\n    1Rick    623.3 1/1/2012  IT        \n    2Dan     515.2 9/23/2013 Operations\n    3Michelle611   11/15/2014IT        \n    4Ryan    729   5/11/2014 HR        \n    5Gary    843.253/27/2015 Finance   \n    6Nina    578   5/21/2013 IT        \n    7Simon   632.8 7/30/2013 Operations\n    8Guru    722.5 6/17/2014 Finance"
  },
  {
    "objectID": "24-r-xml.html#набори-даних",
    "href": "24-r-xml.html#набори-даних",
    "title": "5  XML",
    "section": "5.2 Набори даних",
    "text": "5.2 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml"
  },
  {
    "objectID": "24-r-xml.html#references",
    "href": "24-r-xml.html#references",
    "title": "5  XML",
    "section": "5.3 References",
    "text": "5.3 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl"
  },
  {
    "objectID": "10-r-data-read-intro.html",
    "href": "10-r-data-read-intro.html",
    "title": "\n1  Загальна інформація + презентація\n",
    "section": "",
    "text": "Розглянути осноновні типи джерел даних, їх структуру, сервіси, бібліотеки та способи завантаження/вивантаження у R."
  },
  {
    "objectID": "10-r-data-read-intro.html#короткий-опис",
    "href": "10-r-data-read-intro.html#короткий-опис",
    "title": "\n1  Загальна інформація + презентація\n",
    "section": "\n1.2 Короткий опис",
    "text": "1.2 Короткий опис\nМатеріали розділу містять інформацію про структуру файлів у форматах CSV, XML, XLSX, JSON, а також способи читання інформації з API, chatGPT 3.5. Окрім того розглянуто також можливості читання запису SQL (на прикладі SQLite) та веб-сторінок (HTML).\n\n\nЦе вбудований документ <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> на платформі <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "20-r-data-read-intro.html",
    "href": "20-r-data-read-intro.html",
    "title": "\n2  Загальна інформація + презентація\n",
    "section": "",
    "text": "Розглянути осноновні типи джерел даних, їх структуру, сервіси, бібліотеки та способи завантаження/вивантаження у R."
  },
  {
    "objectID": "20-r-data-read-intro.html#короткий-опис",
    "href": "20-r-data-read-intro.html#короткий-опис",
    "title": "\n2  Загальна інформація + презентація\n",
    "section": "\n2.2 Короткий опис",
    "text": "2.2 Короткий опис\nМатеріали розділу містять інформацію про структуру файлів у форматах CSV, XML, XLSX, JSON, а також способи читання інформації з API, chatGPT 3.5. Окрім того розглянуто також можливості читання запису SQL (на прикладі SQLite) та веб-сторінок (HTML)."
  },
  {
    "objectID": "20-r-data-read-intro.html#презентація",
    "href": "20-r-data-read-intro.html#презентація",
    "title": "\n2  Загальна інформація + презентація\n",
    "section": "\n2.3 Презентація",
    "text": "2.3 Презентація\n\nЦе вбудований документ <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> на платформі <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "25-r-api-json.html",
    "href": "25-r-api-json.html",
    "title": "6  JSON and API",
    "section": "",
    "text": "JSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. It is based on a subset of the JavaScript Programming Language Standard.\nAPI is the acronym for Application Programming Interface, which is a software intermediary that allows two applications to talk to each other.\nOne of the most popular packages for json is jsonlite.\n\n#install.packages(\"jsonlite\")\nlibrary(jsonlite)\n\nLet’s use readinginformation about BTC and USDT crypro currencies from Binance\n\nmarket = 'BTCUSDT'\ninterval = '1h'\nlimit = 100\n\nurl <- paste0(url = \"https://api.binance.com/api/v3/klines?symbol=\", market ,\"&interval=\", interval,\"&limit=\", limit)\nprint(url) # complete request URL\n\n[1] \"https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1h&limit=100\"\n\n\nOn the next stage you need use fromJSON() function to get data.\nMore details about requests to Binanace at https://github.com/binance/binance-spot-api-docs/blob/master/rest-api.md#klinecandlestick-data\nIf you enter ‘url’ value at browser response is going to be like this:\n[\n  [\n    1499040000000,      // Open time\n    \"0.01634790\",       // Open\n    \"0.80000000\",       // High\n    \"0.01575800\",       // Low\n    \"0.01577100\",       // Close\n    \"148976.11427815\",  // Volume\n    1499644799999,      // Close time\n    \"2434.19055334\",    // Quote asset volume\n    308,                // Number of trades\n    \"1756.87402397\",    // Taker buy base asset volume\n    \"28.46694368\",      // Taker buy quote asset volume\n    \"17928899.62484339\" // Ignore.\n  ]\n]\n\ndata <- fromJSON(url) # get json and transform it to list()\ndata <- data[, 1:7] # let's left only 1:7 columns (from Open time to Close time)\nhead(data)\n\n\n\nA matrix: 6 × 7 of type chr\n\n    165051360000041693.5800000041750.0000000041525.0000000041610.010000001138.643370001650517199999\n    165051720000041610.0100000041699.0000000041434.4400000041462.760000001229.259360001650520799999\n    165052080000041462.7500000041600.0000000041419.2000000041522.380000001049.712440001650524399999\n    165052440000041522.3800000041940.0000000041451.0000000041855.690000001928.480910001650527999999\n    1.650528e+12 41855.6900000042050.3000000041741.1000000041922.970000002518.040900001650531599999\n    165053160000041922.9600000041971.9000000041743.9600000041803.700000001655.769930001650535199999\n\n\n\n\n\ntypeof(data) # check data type\ndata <- as.data.frame(data) # convert to dataframe\nhead(data)\n\n'character'\n\n\n\n\nA data.frame: 6 × 7\n\n    V1V2V3V4V5V6V7\n    <chr><chr><chr><chr><chr><chr><chr>\n\n\n    1165051360000041693.5800000041750.0000000041525.0000000041610.010000001138.643370001650517199999\n    2165051720000041610.0100000041699.0000000041434.4400000041462.760000001229.259360001650520799999\n    3165052080000041462.7500000041600.0000000041419.2000000041522.380000001049.712440001650524399999\n    4165052440000041522.3800000041940.0000000041451.0000000041855.690000001928.480910001650527999999\n    51.650528e+12 41855.6900000042050.3000000041741.1000000041922.970000002518.040900001650531599999\n    6165053160000041922.9600000041971.9000000041743.9600000041803.700000001655.769930001650535199999\n\n\n\n\n\n# fix columns names\ncolnames(data) <- c(\"Open_time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Close_time\")\nhead(data) # looks better, but columns are characters still\n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <chr><chr><chr><chr><chr><chr><chr>\n\n\n    1165051360000041693.5800000041750.0000000041525.0000000041610.010000001138.643370001650517199999\n    2165051720000041610.0100000041699.0000000041434.4400000041462.760000001229.259360001650520799999\n    3165052080000041462.7500000041600.0000000041419.2000000041522.380000001049.712440001650524399999\n    4165052440000041522.3800000041940.0000000041451.0000000041855.690000001928.480910001650527999999\n    51.650528e+12 41855.6900000042050.3000000041741.1000000041922.970000002518.040900001650531599999\n    6165053160000041922.9600000041971.9000000041743.9600000041803.700000001655.769930001650535199999\n\n\n\n\n\nis.numeric(data[,1]) # check 1st column type is numeric\nis.numeric(data[,2]) # check 2nd column type is numeric\n\nFALSE\n\n\nFALSE\n\n\n\ndata <- as.data.frame(sapply(data, as.numeric)) # convert all columns to numeric\nhead(data) # good, its double now\n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    11.650514e+1241693.5841750.041525.0041610.011138.6431.650517e+12\n    21.650517e+1241610.0141699.041434.4441462.761229.2591.650521e+12\n    31.650521e+1241462.7541600.041419.2041522.381049.7121.650524e+12\n    41.650524e+1241522.3841940.041451.0041855.691928.4811.650528e+12\n    51.650528e+1241855.6942050.341741.1041922.972518.0411.650532e+12\n    61.650532e+1241922.9641971.941743.9641803.701655.7701.650535e+12\n\n\n\n\nFinal stage is to convert Open_time and Close_time to dates.\n\ndata$Open_time <- as.POSIXct(data$Open_time/1e3, origin = '1970-01-01')\ndata$Close_time <- as.POSIXct(data$Close_time/1e3, origin = '1970-01-01')\n\nhead(data) \n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <dttm><dbl><dbl><dbl><dbl><dbl><dttm>\n\n\n    12022-04-21 07:00:0041693.5841750.041525.0041610.011138.6432022-04-21 07:59:59\n    22022-04-21 08:00:0041610.0141699.041434.4441462.761229.2592022-04-21 08:59:59\n    32022-04-21 09:00:0041462.7541600.041419.2041522.381049.7122022-04-21 09:59:59\n    42022-04-21 10:00:0041522.3841940.041451.0041855.691928.4812022-04-21 10:59:59\n    52022-04-21 11:00:0041855.6942050.341741.1041922.972518.0412022-04-21 11:59:59\n    62022-04-21 12:00:0041922.9641971.941743.9641803.701655.7702022-04-21 12:59:59\n\n\n\n\n\ntail(data) # check last records\n\n\n\nA data.frame: 6 × 7\n\n    Open_timeOpenHighLowCloseVolumeClose_time\n    <dttm><dbl><dbl><dbl><dbl><dbl><dttm>\n\n\n    952022-04-25 05:00:0039095.8139153.9438961.6439091.171205.51582022-04-25 05:59:59\n    962022-04-25 06:00:0039091.1739294.7639086.3739253.711443.33182022-04-25 06:59:59\n    972022-04-25 07:00:0039253.7039256.2839055.7139139.74 896.85542022-04-25 07:59:59\n    982022-04-25 08:00:0039139.7439230.5038947.4238975.221057.49002022-04-25 08:59:59\n    992022-04-25 09:00:0038975.2139057.9738590.0038636.352814.97162022-04-25 09:59:59\n    1002022-04-25 10:00:0038636.3538675.6838200.0038534.993528.23552022-04-25 10:59:59"
  },
  {
    "objectID": "25-r-api-json.html#набори-даних",
    "href": "25-r-api-json.html#набори-даних",
    "title": "6  JSON and API",
    "section": "6.2 Набори даних",
    "text": "6.2 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml"
  },
  {
    "objectID": "25-r-api-json.html#references",
    "href": "25-r-api-json.html#references",
    "title": "6  JSON and API",
    "section": "6.3 References",
    "text": "6.3 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl"
  },
  {
    "objectID": "26-r-google.html",
    "href": "26-r-google.html",
    "title": "7  Google Services",
    "section": "",
    "text": "THIS CHAPTER IS UNDER CONSTRUCTION / Working with Google Spreadsheets need account authorization.\n\ngooglesheets4 is a package to work with Google Sheets from R.\n\n#install.packages(\"googlesheets4\")\nlibrary(googlesheets4)\n\nYou can read google documents after authentification on google service. There is sample code:\nread_sheet(\"https://docs.google.com/spreadsheets/d/1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit#gid=780868077\")\ngs4_deauth()\nLet’s read sample dataset gapminder. It detailed described in next paragraph.\n\n# gs4_example(\"gapminder\")"
  },
  {
    "objectID": "26-r-google.html#google-search-trends",
    "href": "26-r-google.html#google-search-trends",
    "title": "7  Google Services",
    "section": "7.2 Google Search Trends",
    "text": "7.2 Google Search Trends\nGoogle Trends is a service for analyzing search requests by many filters like region (continent, country, locality), period (year, month), information category (business, education, hobby, healthcare), information type (news, shopping, video, images) https://trends.google.com/trends/\n\n# install.packages('gtrendsR')\n# install.packages('ggplot2')\nlibrary(gtrendsR) # loading package for Google Trends queries\nlibrary(ggplot2)\n\nLet’s configure out google trends query params\n\nkeywords = c(\"Bitcoin\", \"FC Barcelona\") # search keywords\ncountry = c('AT') # search region from https://support.google.com/business/answer/6270107?hl=en\ntime = (\"2021-01-01 2021-06-01\") # period\nchannel = 'web' # search channel: google search ('news' - google news, 'images' - google images)\n\n\n# query\ntrends = gtrends(keywords, gprop = channel, geo = country, time = time, tz = \"UTC\")\n\n\ntime_trend = trends$interest_over_time\nhead(time_trend)\n\n\n\nA data.frame: 6 × 7\n\n    datehitskeywordgeotimegpropcategory\n    <dttm><chr><chr><chr><chr><chr><int>\n\n\n    12021-01-0136BitcoinAT2021-01-01 2021-06-01web0\n    22021-01-0267BitcoinAT2021-01-01 2021-06-01web0\n    32021-01-0374BitcoinAT2021-01-01 2021-06-01web0\n    42021-01-0457BitcoinAT2021-01-01 2021-06-01web0\n    52021-01-0553BitcoinAT2021-01-01 2021-06-01web0\n    62021-01-0666BitcoinAT2021-01-01 2021-06-01web0\n\n\n\n\n\nplot <- ggplot(data=time_trend, aes(x=date, y=hits, group=keyword, col=keyword)) +\n  geom_line() +\n  xlab('Time') + \n  ylab('Relative Interest') + \n  theme(legend.title = element_blank(), legend.position=\"bottom\", legend.text=element_text(size=15)) + \n  ggtitle(\"Google Search Volume\")  \n\nplot"
  },
  {
    "objectID": "26-r-google.html#набори-даних",
    "href": "26-r-google.html#набори-даних",
    "title": "7  Google Services",
    "section": "7.3 Набори даних",
    "text": "7.3 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml"
  },
  {
    "objectID": "26-r-google.html#references",
    "href": "26-r-google.html#references",
    "title": "7  Google Services",
    "section": "7.4 References",
    "text": "7.4 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl"
  },
  {
    "objectID": "27-r-sql.html",
    "href": "27-r-sql.html",
    "title": "8  SQL (with SQLite sample)",
    "section": "",
    "text": "We are going to review working with database on SQLite, becouse it allows us not to install DB-server and start working with simple file.\nFor now we will use RSQLite package.\nNow, let’s create new:\nYou can write complex queries for many tables if you knowledge of SQL allows."
  },
  {
    "objectID": "27-r-sql.html#набори-даних",
    "href": "27-r-sql.html#набори-даних",
    "title": "8  SQL (with SQLite sample)",
    "section": "8.1 Набори даних",
    "text": "8.1 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml"
  },
  {
    "objectID": "27-r-sql.html#references",
    "href": "27-r-sql.html#references",
    "title": "8  SQL (with SQLite sample)",
    "section": "8.2 References",
    "text": "8.2 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl"
  },
  {
    "objectID": "28-r-html.html",
    "href": "28-r-html.html",
    "title": "9  Web-pages (HTML)",
    "section": "",
    "text": "Sometimes decision making needs scrap data from web sources and pages.\nLet’s try to parse data from Wikipedia as table.\nGo to web page https://en.wikipedia.org/wiki/List_of_largest_banks and check it.\nFor now, let’s read a table of Total Assets in US Billion\nNext is reading data of market capitalization table (4th):\nAnd now let’s merge() this two datasets:"
  },
  {
    "objectID": "28-r-html.html#task-3",
    "href": "28-r-html.html#task-3",
    "title": "9  Web-pages (HTML)",
    "section": "9.1 Task 3",
    "text": "9.1 Task 3\nFrom a page https://en.wikipedia.org/wiki/List_of_largest_banks read and merge by country named tables:\n\nNumber of banks in the top 100 by total assets\nTotal market capital (US$ billion) across the top 70 banks by country\n\nSolution\n\nlibrary(rvest)\nurl <- \"https://en.wikipedia.org/wiki/List_of_largest_banks\" # got to url in other tab\n#url <- \"data/List of largest banks - Wikipedia_.html\"\npage_data <- read_html(url) # read html content\n\ntables <- html_nodes(page_data, \"table\")\nhtml_table(tables[1]) #its not needed table\n\n\n\n    \nA tibble: 1  2\n\n    X1X2\n    <lgl><chr>\n\n\n    NAThis article is missing information about Revenue and Employment. Please expand the article to include this information. Further details may exist on the talk page.  (September 2020)\n\n\n\n\n\n\n\nhtml_table(tables[3]) # thats solution for \"Number of banks in the top 100 by total assets\"\n#check the end of table. There are NA record\n# lets remove it\n\n\n\n    \nA tibble: 26  3\n\n    RankCountryNumber\n    <int><chr><int>\n\n\n    1China         19\n    2United States 11\n    3Japan          8\n    4United Kingdom 6\n    4France         6\n    4South Korea    6\n    5Canada         5\n    5Germany        5\n    6Australia      4\n    6Brazil         4\n    6Spain          4\n    7Netherlands    3\n    7Singapore      3\n    7Sweden         3\n    7Switzerland    3\n    8Italy          2\n    9India          1\n    9Austria        1\n    9Belgium        1\n    9Denmark        1\n    9Finland        1\n    9Norway         1\n    9Russia         1\n    9Qatar          1\n    9NA            NA\n    9NA            NA\n\n\n\n\n\n\n\ntable1 <- as.data.frame(html_table(tables[3]))\ntable1 <- table1[!is.na(table1$Country), ]\ntable1 # now it OK!\n\n\n\nA data.frame: 24 × 3\n\n    RankCountryNumber\n    <int><chr><int>\n\n\n    11China         19\n    22United States 11\n    33Japan          8\n    44United Kingdom 6\n    54France         6\n    64South Korea    6\n    75Canada         5\n    85Germany        5\n    96Australia      4\n    106Brazil         4\n    116Spain          4\n    127Netherlands    3\n    137Singapore      3\n    147Sweden         3\n    157Switzerland    3\n    168Italy          2\n    179India          1\n    189Austria        1\n    199Belgium        1\n    209Denmark        1\n    219Finland        1\n    229Norway         1\n    239Russia         1\n    249Qatar          1\n\n\n\n\n\n# SOlution for \"Total market capital (US$ billion) across the top 70 banks by country\"\n# compare this with table on a given page\ntable2 <- as.data.frame(html_table(tables[4]))\ntable2 # now it OK!\n\n\n\nA data.frame: 50 × 3\n\n    RankBank.nameMarket.cap.US..billion.\n    <int><chr><dbl>\n\n\n     1JPMorgan Chase                         368.78\n     2Industrial and Commercial Bank of China295.65\n     3Bank of America                        279.73\n     4Wells Fargo                            214.34\n     5China Construction Bank                207.98\n     6Agricultural Bank of China             181.49\n     7HSBC Holdings PLC                      169.47\n     8Citigroup Inc.                         163.58\n     9Bank of China                          151.15\n    10China Merchants Bank                   133.37\n    11Royal Bank of Canada                   113.80\n    12Toronto-Dominion Bank                  106.61\n    13Commonwealth Bank                       99.77\n    14HDFC Bank                              105.90\n    15U.S. Bancorp                            84.40\n    16Goldman Sachs                           78.70\n    17Banco Santander                         75.47\n    18Banco Bradesco                          74.67\n    19Morgan Stanley                          73.93\n    20Westpac                                 67.84\n    21Mitsubishi UFJ Financial Group          66.20\n    22Scotiabank                              65.48\n    23PNC Financial Services                  63.11\n    24Bank of Communications                  61.85\n    25BNP Paribas                             59.36\n    26Australia and New Zealand Banking Group 54.88\n    27National Australia Bank                 51.68\n    28Lloyds Banking Group                    51.19\n    29Sumitomo Mitsui Financial Group         49.85\n    30Bank of Montreal                        48.12\n    31UBS                                     45.92\n    32ING Group                               44.97\n    33Capital One                             43.22\n    34The Bank of New York Mellon             42.58\n    35China Minsheng Bank                     39.13\n    36China CITIC Bank                        38.55\n    37Banco Bilbao Vizcaya Argentaria         37.42\n    38Mizuho Financial Group                  36.95\n    39Intesa Sanpaolo                         36.90\n    40Credit Agricole                         34.89\n    41Canadian Imperial Bank of Commerce      34.87\n    42Royal Bank of Scotland                  33.95\n    43Barclays                                33.26\n    44Credit Suisse                           30.75\n    45Nordea                                  29.59\n    46Standard Chartered                      29.37\n    47KBC Bank                                27.40\n    48UniCredit                               26.88\n    49Societe Generale                        21.27\n    50Deutsche Bank                           15.77"
  },
  {
    "objectID": "28-r-html.html#набори-даних",
    "href": "28-r-html.html#набори-даних",
    "title": "9  Web-pages (HTML)",
    "section": "9.2 Набори даних",
    "text": "9.2 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml"
  },
  {
    "objectID": "28-r-html.html#references",
    "href": "28-r-html.html#references",
    "title": "9  Web-pages (HTML)",
    "section": "9.3 References",
    "text": "9.3 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl"
  },
  {
    "objectID": "etl-dplyr.html",
    "href": "etl-dplyr.html",
    "title": "10  Manipulate data with dplyr",
    "section": "",
    "text": "You need this packages for code execution:"
  },
  {
    "objectID": "etl-dplyr.html#whats-dplyr-package",
    "href": "etl-dplyr.html#whats-dplyr-package",
    "title": "10  Manipulate data with dplyr",
    "section": "10.1 What’s dplyr package",
    "text": "10.1 What’s dplyr package\nThe dplyr package is one of the most powerful and popular package in R for data manipulation.\nWorking with data:\n\nFigure out what you want to do.\nDescribe those tasks in the form of a computer program.\nExecute the program.\n\nThe dplyr package makes these steps fast and easy:\n\nBy constraining your options, it helps you think about your data manipulation challenges.\nIt provides simple verbs, functions that correspond to the most common data manipulation tasks, to help you translate your thoughts into code.\nIt uses efficient backends, so you spend less time waiting for the computer.\n\nBefore use you should install package:\n\n# install.packages(\"dplyr\")\n\nNext step is loading package:\n\nlibrary(dplyr)\n\ndplyr functions work with pipes and expect tidy data. In tidy data:\n\nAlternative way is to load tidyverse package with other attached:\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)"
  },
  {
    "objectID": "etl-dplyr.html#exploring-data-with-dplyr",
    "href": "etl-dplyr.html#exploring-data-with-dplyr",
    "title": "10  Manipulate data with dplyr",
    "section": "10.2 Exploring data with dplyr",
    "text": "10.2 Exploring data with dplyr\n\n10.2.1 Basic funtions and dataset explore\nThere are most popular functions in dplyr is listed in table.\n\n\n\ndplyr Function\nDescription\nEquivalent SQL\n\n\n\n\nselect()\nSelecting columns (variables)\nSELECT\n\n\nfilter()\nFilter (subset) rows.\nWHERE\n\n\ngroup_by()\nGroup the data\nGROUP BY\n\n\nsummarise()\nSummarise (or aggregate) data\n-\n\n\narrange()\nSort the data\nORDER BY\n\n\njoin()\nJoining data frames (tables)\nJOIN\n\n\nmutate()\nCreating New Variables\nCOLUMN ALIAS\n\n\n\nFor the next sample we are going to use gapminder dataset. Go to gapminder dataset description\nThe gapminder data frame include six variables:\n\n\n\nvariable\nmeaning\n\n\n\n\ncountry\n-\n\n\ncontinent\n-\n\n\nyear\n-\n\n\nlifeExp\nlife expectancy at birth\n\n\npop\ntotal population\n\n\ngdpPercap\nper-capita GDP\n\n\n\nPer-capita GDP (Gross domestic product) is given in units of international dollars, a hypothetical unit of currency that has the same purchasing power parity that the U.S. dollar had in the United States at a given point in time – 2005, in this case.\nThe gapminder data frame is a special kind of data frame: a tibble.\n\n# install.packages(\"gapminder\")\nlibrary(gapminder)  # load package and dataset\nclass(gapminder)\n\n\n'tbl_df''tbl''data.frame'\n\n\nLet’s preview it with functions str(), glimpse(), head(), tail(), summary().\n\nstr(gapminder)\n\ntibble [1,704 x 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\n\n\n\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   <fct> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", ~\n$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, ~\n$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, ~\n$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8~\n$ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12~\n$ gdpPercap <dbl> 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, ~\n\n\n\nhead(gapminder) #shows first n-rows, 6 by default\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453\n    AfghanistanAsia195730.332 9240934820.8530\n    AfghanistanAsia196231.99710267083853.1007\n    AfghanistanAsia196734.02011537966836.1971\n    AfghanistanAsia197236.08813079460739.9811\n    AfghanistanAsia197738.43814880372786.1134\n\n\n\n\n\ntail(gapminder) #shows last n-rows, 6 by default\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    ZimbabweAfrica198260.363 7636524788.8550\n    ZimbabweAfrica198762.351 9216418706.1573\n    ZimbabweAfrica199260.37710704340693.4208\n    ZimbabweAfrica199746.80911404948792.4500\n    ZimbabweAfrica200239.98911926563672.0386\n    ZimbabweAfrica200743.48712311143469.7093\n\n\n\n\n\nsummary(gapminder)\n\n        country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1  \n                                       \n\n\n\n\n10.2.2 filter() function\n\naustria <- filter(gapminder, country == \"Austria\")\naustria\n\n\n\nA tibble: 12 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AustriaEurope195266.8006927772 6137.076\n    AustriaEurope195767.4806965860 8842.598\n    AustriaEurope196269.540712986410750.721\n    AustriaEurope196770.140737699812834.602\n    AustriaEurope197270.630754420116661.626\n    AustriaEurope197772.170756843019749.422\n    AustriaEurope198273.180757461321597.084\n    AustriaEurope198774.940757890323687.826\n    AustriaEurope199276.040791496927042.019\n    AustriaEurope199777.510806987629095.921\n    AustriaEurope200278.980814831232417.608\n    AustriaEurope200779.829819978336126.493\n\n\n\n\nfilter() takes logical expressions and returns the rows for which all are TRUE.\n\n# task: select rows with lifeExp less than 31\nfilter(gapminder, lifeExp < 31)\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    Afghanistan Asia  195228.8018425333 779.4453\n    Afghanistan Asia  195730.3329240934 820.8530\n    Angola      Africa195230.01542320953520.6103\n    Gambia      Africa195230.000 284320 485.2307\n    Rwanda      Africa199223.5997290203 737.0686\n    Sierra LeoneAfrica195230.3312143249 879.7877\n\n\n\n\n\n# task: select Austria only and year after 1980\nfilter(gapminder, country == \"Austria\", year > 1980)\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AustriaEurope198273.180757461321597.08\n    AustriaEurope198774.940757890323687.83\n    AustriaEurope199276.040791496927042.02\n    AustriaEurope199777.510806987629095.92\n    AustriaEurope200278.980814831232417.61\n    AustriaEurope200779.829819978336126.49\n\n\n\n\n\n# task: select Austria and Belgium\nfilter(gapminder, country %in% c(\"Austria\", \"Belgium\"))\n\n\n\nA tibble: 24 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AustriaEurope195266.800 6927772 6137.076\n    AustriaEurope195767.480 6965860 8842.598\n    AustriaEurope196269.540 712986410750.721\n    AustriaEurope196770.140 737699812834.602\n    AustriaEurope197270.630 754420116661.626\n    AustriaEurope197772.170 756843019749.422\n    AustriaEurope198273.180 757461321597.084\n    AustriaEurope198774.940 757890323687.826\n    AustriaEurope199276.040 791496927042.019\n    AustriaEurope199777.510 806987629095.921\n    AustriaEurope200278.980 814831232417.608\n    AustriaEurope200779.829 819978336126.493\n    BelgiumEurope195268.000 8730405 8343.105\n    BelgiumEurope195769.240 8989111 9714.961\n    BelgiumEurope196270.250 921840010991.207\n    BelgiumEurope196770.940 955650013149.041\n    BelgiumEurope197271.440 970910016672.144\n    BelgiumEurope197772.800 982180019117.974\n    BelgiumEurope198273.930 985630320979.846\n    BelgiumEurope198775.350 987020022525.563\n    BelgiumEurope199276.4601004562225575.571\n    BelgiumEurope199777.5301019978727561.197\n    BelgiumEurope200278.3201031197030485.884\n    BelgiumEurope200779.4411039222633692.605\n\n\n\n\nLets rewrite initial code and record it to the variable/data.frame:\n\n\n10.2.3 Pipe (%>%) operator\n%>% is pipe operator. The pipe operator takes the thing on the left-hand-side and pipes it into the function call on the right-hand-side – literally, drops it in as the first argument.\nhead() function without pipe and top 4 items:\n\nIn R version before 4.1.0 pipe %>% operator is not a language build-in and you should install magrittr package:\n\n\nPipe opertor in R 4.1+ |>, using this is preferable\n\n\n#install.packages(\"magrittr\") # for pipe %>% operator\nlibrary(magrittr)\n\n\nhead(gapminder, n = 4)\n\n\n\nA tibble: 4 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453\n    AfghanistanAsia195730.332 9240934820.8530\n    AfghanistanAsia196231.99710267083853.1007\n    AfghanistanAsia196734.02011537966836.1971\n\n\n\n\nhead() function with pipe and top 4 items:\n\ngapminder %>% head(4)\n\n\n\nA tibble: 4 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453\n    AfghanistanAsia195730.332 9240934820.8530\n    AfghanistanAsia196231.99710267083853.1007\n    AfghanistanAsia196734.02011537966836.1971\n\n\n\n\nOutput is the same. So, let’s rewrire filtering for Austria with pipe:\n\naustria <- gapminder %>% filter(country == \"Austria\")\naustria\n\n\n\nA tibble: 12 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AustriaEurope195266.8006927772 6137.076\n    AustriaEurope195767.4806965860 8842.598\n    AustriaEurope196269.540712986410750.721\n    AustriaEurope196770.140737699812834.602\n    AustriaEurope197270.630754420116661.626\n    AustriaEurope197772.170756843019749.422\n    AustriaEurope198273.180757461321597.084\n    AustriaEurope198774.940757890323687.826\n    AustriaEurope199276.040791496927042.019\n    AustriaEurope199777.510806987629095.921\n    AustriaEurope200278.980814831232417.608\n    AustriaEurope200779.829819978336126.493\n\n\n\n\n\n# add more conditions in filter\naustria <- gapminder %>% filter(country == \"Austria\", year > 2000)\naustria\n\n\n\nA tibble: 2 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AustriaEurope200278.980814831232417.61\n    AustriaEurope200779.829819978336126.49\n\n\n\n\n\n\n\n10.2.4 select() function\nUse select() to subset the data on variables/columns by names or index. You also can define order of columns with select().\n\ngapminder %>% \nselect(year, country, pop) %>%\nslice(1: 10)\n\n\n\nA tibble: 10 × 3\n\n    yearcountrypop\n    <int><fct><int>\n\n\n    1952Afghanistan 8425333\n    1957Afghanistan 9240934\n    1962Afghanistan10267083\n    1967Afghanistan11537966\n    1972Afghanistan13079460\n    1977Afghanistan14880372\n    1982Afghanistan12881816\n    1987Afghanistan13867957\n    1992Afghanistan16317921\n    1997Afghanistan22227415\n\n\n\n\nLets combine few functions with pipe (%>%):\nFinally, lest extend our filtering:\n\n# compare dplyr syntax with base R call\ngapminder[gapminder$country == \"Austria\", c(\"year\", \"pop\", \"lifeExp\")]\n\ngapminder %>% \nfilter(country == \"Austria\") %>%\nselect(year, pop, lifeExp)\n\n\n\nA tibble: 12 × 3\n\n    yearpoplifeExp\n    <int><int><dbl>\n\n\n    1952692777266.800\n    1957696586067.480\n    1962712986469.540\n    1967737699870.140\n    1972754420170.630\n    1977756843072.170\n    1982757461373.180\n    1987757890374.940\n    1992791496976.040\n    1997806987677.510\n    2002814831278.980\n    2007819978379.829\n\n\n\n\n\n\nA tibble: 12 × 3\n\n    yearpoplifeExp\n    <int><int><dbl>\n\n\n    1952692777266.800\n    1957696586067.480\n    1962712986469.540\n    1967737699870.140\n    1972754420170.630\n    1977756843072.170\n    1982757461373.180\n    1987757890374.940\n    1992791496976.040\n    1997806987677.510\n    2002814831278.980\n    2007819978379.829\n\n\n\n\nYou can remove some columns using minus(operator) and add few filter conditions:\n\naustria <- gapminder %>% \n                filter(country == \"Austria\", year > 2000) %>%\n                select(-continent, -gdpPercap) %>%\n                head()\naustria\n\n\n\nA tibble: 2 × 4\n\n    countryyearlifeExppop\n    <fct><int><dbl><int>\n\n\n    Austria200278.9808148312\n    Austria200779.8298199783\n\n\n\n\nYou can insert different conditions about columns you need to select.\n\ngapminder %>%\n    select(!where(is.numeric)) %>%  # its 1704 records, because of repeating some records\n    slice(1:5)\n\n\n\nA tibble: 5 × 2\n\n    countrycontinent\n    <fct><fct>\n\n\n    AfghanistanAsia\n    AfghanistanAsia\n    AfghanistanAsia\n    AfghanistanAsia\n    AfghanistanAsia\n\n\n\n\nLet’s output all unique pairs continent -> country with distinct() function:\n\ngapminder %>%\n    select(country) %>%\n    distinct() # its 142 records now\n\n\n\nA tibble: 142 × 1\n\n    country\n    <fct>\n\n\n    Afghanistan             \n    Albania                 \n    Algeria                 \n    Angola                  \n    Argentina               \n    Australia               \n    Austria                 \n    Bahrain                 \n    Bangladesh              \n    Belgium                 \n    Benin                   \n    Bolivia                 \n    Bosnia and Herzegovina  \n    Botswana                \n    Brazil                  \n    Bulgaria                \n    Burkina Faso            \n    Burundi                 \n    Cambodia                \n    Cameroon                \n    Canada                  \n    Central African Republic\n    Chad                    \n    Chile                   \n    China                   \n    Colombia                \n    Comoros                 \n    Congo, Dem. Rep.        \n    Congo, Rep.             \n    Costa Rica              \n    ...\n    Sierra Leone       \n    Singapore          \n    Slovak Republic    \n    Slovenia           \n    Somalia            \n    South Africa       \n    Spain              \n    Sri Lanka          \n    Sudan              \n    Swaziland          \n    Sweden             \n    Switzerland        \n    Syria              \n    Taiwan             \n    Tanzania           \n    Thailand           \n    Togo               \n    Trinidad and Tobago\n    Tunisia            \n    Turkey             \n    Uganda             \n    United Kingdom     \n    United States      \n    Uruguay            \n    Venezuela          \n    Vietnam            \n    West Bank and Gaza \n    Yemen, Rep.        \n    Zambia             \n    Zimbabwe           \n\n\n\n\n\n\n\n10.2.5 Selecting random \\(N\\) rows\nThe sample_n() function selects random rows from a data frame\n\ngapminder %>% sample_n(5)\n\n\n\nA tibble: 5 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    Norway          Europe  197274.340393300418965.0555\n    Liberia         Africa  197743.7641703617  640.3224\n    Jamaica         Americas199772.2622531311 7121.9247\n    Jamaica         Americas195762.6101535090 4756.5258\n    Hong Kong, ChinaAsia    200281.495676247630209.0152\n\n\n\n\nIf you want make pseudo-random generation reprodusable use set.seed(). Seed is start point of random generation. Different seeds give different output.\n\nset.seed(2021) # example, seed = 2021\n\nThe sample_frac() function selects random fraction rows from a data frame. Let’s select \\(1\\%\\) of data\n\nset.seed(2021) # output not changing, uncomment it \ngapminder %>% sample_frac(0.1)\n\n\n\nA tibble: 170 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    Libya              Africa  196247.808 1441863 6757.0308\n    Botswana           Africa  199752.556 1536536 8647.1423\n    Swaziland          Africa  195743.424  326741 1244.7084\n    Dominican Republic Americas199769.957 7992357 3614.1013\n    Iraq               Asia    200257.04624001816 4390.7173\n    Libya              Africa  198766.234 379984511770.5898\n    Montenegro         Europe  196767.178  501035 5907.8509\n    New Zealand        Oceania 195770.260 222940712247.3953\n    Bulgaria           Europe  200773.005 732285810680.7928\n    Malawi             Africa  199747.49510419991  692.2758\n    Venezuela          Americas197767.4561350356313143.9510\n    Guinea             Africa  199751.455 8048834  869.4498\n    Congo, Dem. Rep.   Africa  195239.14314100005  780.5423\n    Eritrea            Africa  196240.158 1666618  380.9958\n    Bangladesh         Asia    198250.00993074406  676.9819\n    Cote d'Ivoire      Africa  195240.477 2977019 1388.5947\n    Trinidad and TobagoAmericas200268.976 110183211460.6002\n    Sierra Leone       Africa  200742.568 6144562  862.5408\n    Malaysia           Asia    200273.0442266236510206.9779\n    Mali               Africa  198746.364 7634008  684.1716\n    Pakistan           Asia    197754.04378152686 1175.9212\n    Norway             Europe  198275.970 411478726298.6353\n    Peru               Americas200771.42128674757 7408.9056\n    Haiti              Americas195237.579 3201488 1840.3669\n    Cuba               Americas196265.246 7254373 5180.7559\n    Costa Rica         Americas200778.782 4133884 9645.0614\n    France             Europe  199778.6405862342825889.7849\n    Botswana           Africa  198763.622 1151184 6205.8839\n    Bangladesh         Asia    197245.25270759295  630.2336\n    Congo, Rep.        Africa  197755.625 1536769 3259.1790\n    ..................\n    Mozambique Africa  197240.328  9809596  724.9178\n    Nicaragua  Americas197255.151  2182908 4688.5933\n    Turkey     Europe  197257.005 37492953 3450.6964\n    Gabon      Africa  200256.761  129930412521.7139\n    Ecuador    Americas195751.356  4058385 3780.5467\n    Honduras   Americas197253.884  2965146 2529.8423\n    Paraguay   Americas195262.649  1555876 1952.3087\n    Guinea     Africa  200756.007  9947814  942.6542\n    SwitzerlandEurope  197775.390  631642426982.2905\n    Honduras   Americas196248.041  2090162 2291.1568\n    Thailand   Asia    195250.848 21289402  757.7974\n    Portugal   Europe  195761.510  8817650 3774.5717\n    Cuba       Americas198774.174 10239839 7532.9248\n    Brazil     Americas199769.388168546719 7957.9808\n    Madagascar Africa  196240.848  5703324 1643.3871\n    Mauritius  Africa  198266.711   992040 3688.0377\n    Ghana      Africa  198253.744 11400338  876.0326\n    Congo, Rep.Africa  198757.470  2064095 4201.1949\n    Haiti      Americas197749.923  4908554 1874.2989\n    Portugal   Europe  195259.820  8526050 3068.3199\n    Angola     Africa  198239.942  7016384 2756.9537\n    Swaziland  Africa  199754.289  1054486 3876.7685\n    Norway     Europe  199277.320  428635733965.6611\n    Austria    Europe  197270.630  754420116661.6256\n    Croatia    Europe  195764.770  3991242 4338.2316\n    New ZealandOceania 195269.390  199479410556.5757\n    Angola     Africa  200742.731 12420476 4797.2313\n    Sudan      Africa  200758.556 42292929 2602.3950\n    Poland     Europe  196267.640 30329617 5338.7521\n    Japan      Asia    197273.42010718827314778.7864\n\n\n\n\n\n\n\n10.2.6 Subset rows using their positions with slice()\nDescription\n\nslice() lets you index rows by their (integer) locations. It allows you to select, remove, and duplicate rows. It is accompanied by a number of helpers for common use cases:\nslice_head() and slice_tail() select the first or last rows.\nslice_sample() randomly selects rows.\nslice_min() and slice_max() select rows with highest or lowest values of a variable.\n\nIf .data is a grouped_df, the operation will be performed on each group, so that (e.g.) slice_head(df, n = 5) will select the first five rows in each group.\nSamples\n\ngapminder %>% slice(1) # top 1 row\n\n\n\nA tibble: 1 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia195228.8018425333779.4453\n\n\n\n\n\ngapminder %>% slice(1:6) # top n = 6\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453\n    AfghanistanAsia195730.332 9240934820.8530\n    AfghanistanAsia196231.99710267083853.1007\n    AfghanistanAsia196734.02011537966836.1971\n    AfghanistanAsia197236.08813079460739.9811\n    AfghanistanAsia197738.43814880372786.1134\n\n\n\n\n\ngapminder %>% slice_head(n = 6) # works like head()\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453\n    AfghanistanAsia195730.332 9240934820.8530\n    AfghanistanAsia196231.99710267083853.1007\n    AfghanistanAsia196734.02011537966836.1971\n    AfghanistanAsia197236.08813079460739.9811\n    AfghanistanAsia197738.43814880372786.1134\n\n\n\n\n\ngapminder %>% slice_tail(n = 5) # works like tail()\n\n\n\nA tibble: 5 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    ZimbabweAfrica198762.351 9216418706.1573\n    ZimbabweAfrica199260.37710704340693.4208\n    ZimbabweAfrica199746.80911404948792.4500\n    ZimbabweAfrica200239.98911926563672.0386\n    ZimbabweAfrica200743.48712311143469.7093\n\n\n\n\nYou can drop some recods with negative indexes:\n\ngapminder %>% slice(-c(1:3,5)) %>% # remove Afganistan years 1952, 1957, 1962 and 1972 \n    head(6)\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia196734.02011537966836.1971\n    AfghanistanAsia197738.43814880372786.1134\n    AfghanistanAsia198239.85412881816978.0114\n    AfghanistanAsia198740.82213867957852.3959\n    AfghanistanAsia199241.67416317921649.3414\n    AfghanistanAsia199741.76322227415635.3414\n\n\n\n\n\n# Random rows selection with slice_sample()\ngapminder %>% slice_sample(n = 5) #use set.seed() to fix random\n\n\n\nA tibble: 5 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    Cambodia        Asia  200256.75212926707  896.2260\n    Poland          Europe200274.6703862597612002.2391\n    Bulgaria        Europe197270.900 8576200 6597.4944\n    Congo, Dem. Rep.Africa195239.14314100005  780.5423\n    Chad            Africa200250.525 8835739 1156.1819\n\n\n\n\n\n# Rows with minimum and maximum values of a variable\n# Lets find top 5 records with minimum and maximum lifeExp in all dataset\ngapminder %>% slice_min(lifeExp, n = 5)\ngapminder %>% slice_max(lifeExp, n = 5)\n\n\n\nA tibble: 5 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    Rwanda      Africa199223.5997290203 737.0686\n    Afghanistan Asia  195228.8018425333 779.4453\n    Gambia      Africa195230.000 284320 485.2307\n    Angola      Africa195230.01542320953520.6103\n    Sierra LeoneAfrica195230.3312143249 879.7877\n\n\n\n\n\n\nA tibble: 5 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    Japan           Asia  200782.60312746797231656.07\n    Hong Kong, ChinaAsia  200782.208  698041239724.98\n    Japan           Asia  200282.00012706584128604.59\n    Iceland         Europe200781.757   30193136180.79\n    Switzerland     Europe200781.701  755466137506.42\n\n\n\n\n\n\n\n10.2.7 Sorting with arrange()\narrange(.data, …) function order rows by values of a column or columns (low to high)You can use with desc() to order from high to low.\nFor example, we need to select top 10 countries in 2002 by lifeExp variable.\n\ndata2002 <- gapminder %>% \n                filter(year == 2002) %>%\n                top_n(10, lifeExp) # select top 10 by lifeExp value\ndata2002\n\n\n\nA tibble: 10 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    Australia       Oceania 200280.370 1954679230687.75\n    Canada          Americas200279.770 3190226833328.97\n    Hong Kong, ChinaAsia    200281.495  676247630209.02\n    Iceland         Europe  200280.500   28803031163.20\n    Israel          Asia    200279.696  602952921905.60\n    Italy           Europe  200280.240 5792699927968.10\n    Japan           Asia    200282.00012706584128604.59\n    Spain           Europe  200279.780 4015251724835.47\n    Sweden          Europe  200280.040  895417529341.63\n    Switzerland     Europe  200280.620  736175734480.96\n\n\n\n\n\n# sort by pop\nt <- gapminder %>% arrange(continent)\nt <- gapminder %>% arrange(continent,  country)\nt <- gapminder %>% arrange(continent, desc( country))\nhead(t)\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    ZimbabweAfrica195248.4513080907406.8841\n    ZimbabweAfrica195750.4693646340518.7643\n    ZimbabweAfrica196252.3584277736527.2722\n    ZimbabweAfrica196753.9954995432569.7951\n    ZimbabweAfrica197255.6355861135799.3622\n    ZimbabweAfrica197757.6746642107685.5877\n\n\n\n\n\n\n\n10.2.8 Create new variables with mutate()\nmutate(.data, …) compute new column(s). Lets compute new column for data2002 \\(gdpTotal = gdpPercap * pop / 1000000\\).\n\ngapminder %>% \n    mutate(gdpTotal = gdpPercap * pop) %>%\n    head(10)\n\n\n\nA tibble: 10 × 7\n\n    countrycontinentyearlifeExppopgdpPercapgdpTotal\n    <fct><fct><int><dbl><int><dbl><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453 6567086330\n    AfghanistanAsia195730.332 9240934820.8530 7585448670\n    AfghanistanAsia196231.99710267083853.1007 8758855797\n    AfghanistanAsia196734.02011537966836.1971 9648014150\n    AfghanistanAsia197236.08813079460739.9811 9678553274\n    AfghanistanAsia197738.43814880372786.113411697659231\n    AfghanistanAsia198239.85412881816978.011412598563401\n    AfghanistanAsia198740.82213867957852.395911820990309\n    AfghanistanAsia199241.67416317921649.341410595901589\n    AfghanistanAsia199741.76322227415635.341414121995875\n\n\n\n\ntransmute(.data, …) compute new column(s), drop others.\n\ngapminder %>% \n    transmute(gdpTotal = gdpPercap * pop) %>%\n    head(10)\n\n\n\nA tibble: 10 × 1\n\n    gdpTotal\n    <dbl>\n\n\n     6567086330\n     7585448670\n     8758855797\n     9648014150\n     9678553274\n    11697659231\n    12598563401\n    11820990309\n    10595901589\n    14121995875\n\n\n\n\nYou can mutate many columns at once:\n\ngapminder %>% \n    mutate(gdpTotal = gdpPercap * pop,\n           countryUpper = toupper(country), # uppercase country\n           lifeExpRounded = round(lifeExp)) %>%\n    head(10)\n\n\n\nA tibble: 10 × 9\n\n    countrycontinentyearlifeExppopgdpPercapgdpTotalcountryUpperlifeExpRounded\n    <fct><fct><int><dbl><int><dbl><dbl><chr><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453 6567086330AFGHANISTAN29\n    AfghanistanAsia195730.332 9240934820.8530 7585448670AFGHANISTAN30\n    AfghanistanAsia196231.99710267083853.1007 8758855797AFGHANISTAN32\n    AfghanistanAsia196734.02011537966836.1971 9648014150AFGHANISTAN34\n    AfghanistanAsia197236.08813079460739.9811 9678553274AFGHANISTAN36\n    AfghanistanAsia197738.43814880372786.113411697659231AFGHANISTAN38\n    AfghanistanAsia198239.85412881816978.011412598563401AFGHANISTAN40\n    AfghanistanAsia198740.82213867957852.395911820990309AFGHANISTAN41\n    AfghanistanAsia199241.67416317921649.341410595901589AFGHANISTAN42\n    AfghanistanAsia199741.76322227415635.341414121995875AFGHANISTAN42\n\n\n\n\nYou also can edit existing column (let’s change continent Europe to EU in dataframe):\n\ndata2002 %>%\n    mutate(continent = as.character(continent), # convert factor -> character \n           continent = ifelse(continent == \"Europe\", \"EU\", continent))\n\n\n\nA tibble: 10 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><chr><int><dbl><int><dbl>\n\n\n    Australia       Oceania 200280.370 1954679230687.75\n    Canada          Americas200279.770 3190226833328.97\n    Hong Kong, ChinaAsia    200281.495  676247630209.02\n    Iceland         EU      200280.500   28803031163.20\n    Israel          Asia    200279.696  602952921905.60\n    Italy           EU      200280.240 5792699927968.10\n    Japan           Asia    200282.00012706584128604.59\n    Spain           EU      200279.780 4015251724835.47\n    Sweden          EU      200280.040  895417529341.63\n    Switzerland     EU      200280.620  736175734480.96\n\n\n\n\n\n\n\n10.2.9 Renaming columns with rename()\nrename(.data, …) rename columns. Let’s rename column pop to poulation:\n\ngapminder %>% \n    rename(population = pop) %>%\n    head(10)\n\n\n\nA tibble: 10 × 6\n\n    countrycontinentyearlifeExppopulationgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453\n    AfghanistanAsia195730.332 9240934820.8530\n    AfghanistanAsia196231.99710267083853.1007\n    AfghanistanAsia196734.02011537966836.1971\n    AfghanistanAsia197236.08813079460739.9811\n    AfghanistanAsia197738.43814880372786.1134\n    AfghanistanAsia198239.85412881816978.0114\n    AfghanistanAsia198740.82213867957852.3959\n    AfghanistanAsia199241.67416317921649.3414\n    AfghanistanAsia199741.76322227415635.3414\n\n\n\n\n\n\n\n10.2.10 Calculations with group_by() + summarise()\ngroup_by(.data, ..., add = FALSE) returns copy of table grouped by defined columns.\nLet’s find average by lifeExp for each continent in 2002 (ouput is continent, lifeExpAvg2002, countriesCount, year = 2002):\n\ngapminder %>%\n    filter(year == 2002) %>% # year\n    group_by(continent, year) %>% # grouping condition\n    summarise(\n        lifeExpAvg2002 = mean(lifeExp),\n        countriesCount = n() # n() count of rows in group  \n        ) \n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n\n\nA grouped_df: 5 × 4\n\n    continentyearlifeExpAvg2002countriesCount\n    <fct><int><dbl><int>\n\n\n    Africa  200253.3252352\n    Americas200272.4220425\n    Asia    200269.2338833\n    Europe  200276.7006030\n    Oceania 200279.74000 2\n\n\n\n\nLet’s find total population for each continent in 2002 (ouput is continent, totalPop, year):\n\ngapminder %>%\n    filter(year == 2002) %>% # year\n    group_by(continent, year) %>% # grouping condition\n    summarise(totalPop = sum(pop), .groups = \"keep\") \n\n\n\nA grouped_df: 5 × 3\n\n    continentyeartotalPop\n    <fct><int><dbl>\n\n\n    Africa  2002 833723916\n    Americas2002 849772762\n    Asia    20023601802203\n    Europe  2002 578223869\n    Oceania 2002  23454829\n\n\n\n\nThere are additional variations of summarise():\n\nsummarise_all() - Apply funs to every column.\nsummarise_at() - Apply funs to specific columns.\n\nsummarise_if() - Apply funs to all cols of one type.\n\n\n\n\n10.2.11 Task on Credits\n\nlibrary(ISLR)\n\ngroup_inc <- aggregate(Income ~ Age + Gender, data = Credit, mean)\n\nm_data <- group_inc[group_inc$Gender == \" Male\", ]\nnrow(m_data)\n\nf_data <- group_inc[group_inc$Gender == \"Female\", ]\nnrow(f_data)\nwith(m_data, plot(Age, Income, type = \"l\", col=\"red\"))\nwith(f_data, lines(Age, Income, type = \"l\", col =\"blue\"))\n\n63\n\n\n62\n\n\n\n\n\n\ncd <- Credit %>%\nselect(Income, Age, Gender) %>%\ngroup_by(Age, Gender) %>%\nsummarize(Income = mean(Income))\n\nm_data <- cd %>% filter(Gender == \" Male\")\nnrow(m_data)\n\nf_data <- cd %>% filter(Gender == \"Female\")\nnrow(f_data)\n\nwith(m_data, plot(Age, Income, type = \"l\", col=\"red\"))\nwith(f_data, lines(Age, Income, type = \"l\", col =\"blue\"))\n\n`summarise()` has grouped output by 'Age'. You can override using the `.groups`\nargument.\n\n\n63\n\n\n62\n\n\n\n\n\n\n\n\n10.2.12 Binding rows and columns\nbind_rows(.data, …) helps to unite two dataframes with the same columns order and names.\nSo, if we need add one data frame to an other vertically (bind rows) we shoul use bind_rows:\n\nd2002 <- gapminder %>%\n            filter(year == 2002) %>% # year\n            group_by(continent, year) %>% # grouping condition\n            summarise(\n                lifeExpAvg = mean(lifeExp),\n                countriesCount = n() # n() count of rows in group                \n            )\nhead(d2002)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n\n\nA grouped_df: 5 × 4\n\n    continentyearlifeExpAvgcountriesCount\n    <fct><int><dbl><int>\n\n\n    Africa  200253.3252352\n    Americas200272.4220425\n    Asia    200269.2338833\n    Europe  200276.7006030\n    Oceania 200279.74000 2\n\n\n\n\n\nd2007 <- gapminder %>%\n            filter(year == 2007) %>% # year\n            group_by(continent, year) %>% # grouping condition\n            summarise(\n                lifeExpAvg = mean(lifeExp),\n                countriesCount = n() # n() count of rows in group                \n            )\nhead(d2007)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n\n\nA grouped_df: 5 × 4\n\n    continentyearlifeExpAvgcountriesCount\n    <fct><int><dbl><int>\n\n\n    Africa  200754.8060452\n    Americas200773.6081225\n    Asia    200770.7284833\n    Europe  200777.6486030\n    Oceania 200780.71950 2\n\n\n\n\nUnite them:\n\nd2002 %>% bind_rows(d2007) ## bind rows\n\n\n\nA grouped_df: 10 × 4\n\n    continentyearlifeExpAvgcountriesCount\n    <fct><int><dbl><int>\n\n\n    Africa  200253.3252352\n    Americas200272.4220425\n    Asia    200269.2338833\n    Europe  200276.7006030\n    Oceania 200279.74000 2\n    Africa  200754.8060452\n    Americas200773.6081225\n    Asia    200770.7284833\n    Europe  200777.6486030\n    Oceania 200780.71950 2\n\n\n\n\nbind_cols(.data, …) helps to unite two dataframes with the same rows count.\n\ngrouped_data2002pop <- gapminder %>%\n    filter(year == 2002) %>% # year\n    group_by(continent) %>% # grouping condition\n    summarise(totalPop = sum(pop)) %>%\n    mutate(year = 2002)\ngrouped_data2002pop\n\n\n\nA tibble: 5 × 3\n\n    continenttotalPopyear\n    <fct><dbl><dbl>\n\n\n    Africa   8337239162002\n    Americas 8497727622002\n    Asia    36018022032002\n    Europe   5782238692002\n    Oceania   234548292002\n\n\n\n\nLet’s combine d2002 and grouped_data2002pop:\n\ngrouped_data <- d2002 %>% \n    bind_cols(grouped_data2002pop)\ngrouped_data\n\n# columns with the same name were renamed!\n\nNew names:\n* `continent` -> `continent...1`\n* `year` -> `year...2`\n* `continent` -> `continent...5`\n* `year` -> `year...7`\n\n\n\n\nA tibble: 5 × 7\n\n    continent...1year...2lifeExpAvgcountriesCountcontinent...5totalPopyear...7\n    <fct><int><dbl><int><fct><dbl><dbl>\n\n\n    Africa  200253.3252352Africa   8337239162002\n    Americas200272.4220425Americas 8497727622002\n    Asia    200269.2338833Asia    36018022032002\n    Europe  200276.7006030Europe   5782238692002\n    Oceania 200279.74000 2Oceania   234548292002\n\n\n\n\nYou can remove same named variables before binding:\n\ngrouped_data <- d2002 %>% \n    bind_cols(grouped_data2002pop %>%\n              select(-continent, -year))\ngrouped_data\n\n# better, but continents order is not the same in both frames \n# your data is going to be damaged\n\n\n\nA grouped_df: 5 × 5\n\n    continentyearlifeExpAvgcountriesCounttotalPop\n    <fct><int><dbl><int><dbl>\n\n\n    Africa  200253.3252352 833723916\n    Americas200272.4220425 849772762\n    Asia    200269.23388333601802203\n    Europe  200276.7006030 578223869\n    Oceania 200279.74000 2  23454829\n\n\n\n\n\ngrouped_data2002pop <- grouped_data2002pop %>% \n    arrange(totalPop)\n\ngrouped_data <- d2002 %>% \n    bind_cols(grouped_data2002pop)\ngrouped_data\n\n# you can see that continent fields different in the same row\n\nNew names:\n* `continent` -> `continent...1`\n* `year` -> `year...2`\n* `continent` -> `continent...5`\n* `year` -> `year...7`\n\n\n\n\nA tibble: 5 × 7\n\n    continent...1year...2lifeExpAvgcountriesCountcontinent...5totalPopyear...7\n    <fct><int><dbl><int><fct><dbl><dbl>\n\n\n    Africa  200253.3252352Oceania   234548292002\n    Americas200272.4220425Europe   5782238692002\n    Asia    200269.2338833Africa   8337239162002\n    Europe  200276.7006030Americas 8497727622002\n    Oceania 200279.74000 2Asia    36018022032002\n\n\n\n\n\n\n\n10.2.13 Join()ing data\nTo solve previous problem you can use set of join()-functions. left_join() can solve our previous example:\n\ngrouped_data2002pop <- grouped_data2002pop %>% \n    arrange(totalPop)\n\ngrouped_data <- d2002 %>% \n    left_join(grouped_data2002pop, by = \"continent\")\ngrouped_data\n\n# but we have duplicated year\n\n\n\nA grouped_df: 5 × 6\n\n    continentyear.xlifeExpAvgcountriesCounttotalPopyear.y\n    <fct><int><dbl><int><dbl><dbl>\n\n\n    Africa  200253.3252352 8337239162002\n    Americas200272.4220425 8497727622002\n    Asia    200269.233883336018022032002\n    Europe  200276.7006030 5782238692002\n    Oceania 200279.74000 2  234548292002\n\n\n\n\n\ngrouped_data2002pop <- grouped_data2002pop %>% \n    arrange(totalPop)\n\ngrouped_data <- d2002 %>% \n    left_join(grouped_data2002pop, by = c(\"continent\", \"year\"))\ngrouped_data\n\n#ok\n\n\n\nA grouped_df: 5 × 5\n\n    continentyearlifeExpAvgcountriesCounttotalPop\n    <fct><dbl><dbl><int><dbl>\n\n\n    Africa  200253.3252352 833723916\n    Americas200272.4220425 849772762\n    Asia    200269.23388333601802203\n    Europe  200276.7006030 578223869\n    Oceania 200279.74000 2  23454829\n\n\n\n\nLet’s make a different data sets for testing join() fucntions:\n\nfirst_df <- data.frame(Letter = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                      Value = c(1:5))\n\nsecond_df <- data.frame(Letter = c(\"A\", \"B\", \"C\", \"D\", \"F\"),\n                      Value = c(12, 7, 4, 1, 5))\nfirst_df\nsecond_df \n\n\n\nA data.frame: 5 × 2\n\n    LetterValue\n    <chr><int>\n\n\n    A1\n    B2\n    C3\n    D4\n    E5\n\n\n\n\n\n\nA data.frame: 5 × 2\n\n    LetterValue\n    <chr><dbl>\n\n\n    A12\n    B 7\n    C 4\n    D 1\n    F 5\n\n\n\n\nYou can see that the last row Letter is different in dataframes. left_join() test is next.\n\nfirst_df %>% \n    left_join(second_df, by = \"Letter\")\n# there is no F letter, becouse first_db joined only known first_df Letters.\n\n\n\nA data.frame: 5 × 3\n\n    LetterValue.xValue.y\n    <chr><int><dbl>\n\n\n    A112\n    B2 7\n    C3 4\n    D4 1\n    E5NA\n\n\n\n\n\nfirst_df %>% \n    right_join(second_df, by = \"Letter\")\n# right_join! there is no E letter, becouse first_db joined only known second_df Letters.\n\n\n\nA data.frame: 5 × 3\n\n    LetterValue.xValue.y\n    <chr><int><dbl>\n\n\n    A 112\n    B 2 7\n    C 3 4\n    D 4 1\n    FNA 5\n\n\n\n\n\nfirst_df %>% \n    inner_join(second_df, by = \"Letter\")\n# inner_join! there is no E and F Letters, \n# only known both first_df and second_df are left here.\n\n\n\nA data.frame: 4 × 3\n\n    LetterValue.xValue.y\n    <chr><int><dbl>\n\n\n    A112\n    B2 7\n    C3 4\n    D4 1\n\n\n\n\n\nfirst_df %>% \n    full_join(second_df, by = \"Letter\")\n# all are here, but unknown values replaced by NA, it's ok.\n\n\n\nA data.frame: 6 × 3\n\n    LetterValue.xValue.y\n    <chr><int><dbl>\n\n\n    A 112\n    B 2 7\n    C 3 4\n    D 4 1\n    E 5NA\n    FNA 5\n\n\n\n\nShort description of reviewed functions:\n\n\n\n\n\n\n\n\n\nFunction\nObjectives\nArguments\nMultiple keys\n\n\n\n\nleft_join()\nMerge two datasets. Keep all observations from the origin table\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)\n\n\nright_join()\nMerge two datasets. Keep all observations from the destination table\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)\n\n\ninner_join()\nMerge two datasets. Excludes all unmatched rows\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)\n\n\nfull_join()\nMerge two datasets. Keeps all observations\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)\n\n\n\n\n\n\n10.2.14 Data cleaning with gather()\nSome times your data is not in tidy format. Peole can collect data year by year in each column. It’s problem to use such data for feature engeniering and building prediction models. Let’s generate such data sample (quaterly salary of some people).\n\nnot_good_data <- data.frame(Name = c(\"Nick\", \"Jake\", \"Anna\", \"Jane\", \"Dina\"),\n                           q1_2021 = c(12442, 22131, 21343, 22111, 14123),\n                           q2_2021 = c(13442, 22871, 20343, 22222, 14456),\n                           q3_2021 = c(15482, 22031, 22456, 22444, 14533),\n                           q4_2021 = c(14511, 20031, 21741, 22333, 14511))\nnot_good_data\n\n\n\nA data.frame: 5 × 5\n\n    Nameq1_2021q2_2021q3_2021q4_2021\n    <chr><dbl><dbl><dbl><dbl>\n\n\n    Nick12442134421548214511\n    Jake22131228712203120031\n    Anna21343203432245621741\n    Jane22111222222244422333\n    Dina14123144561453314511\n\n\n\n\n\nbetter_data <- not_good_data %>%\n                gather(quater, salary, 2:5)\n                # gather(quater, salary, q1_2021:q4_2021) possible code too\nbetter_data\n\n\n\nA data.frame: 20 × 3\n\n    Namequatersalary\n    <chr><chr><dbl>\n\n\n    Nickq1_202112442\n    Jakeq1_202122131\n    Annaq1_202121343\n    Janeq1_202122111\n    Dinaq1_202114123\n    Nickq2_202113442\n    Jakeq2_202122871\n    Annaq2_202120343\n    Janeq2_202122222\n    Dinaq2_202114456\n    Nickq3_202115482\n    Jakeq3_202122031\n    Annaq3_202122456\n    Janeq3_202122444\n    Dinaq3_202114533\n    Nickq4_202114511\n    Jakeq4_202120031\n    Annaq4_202121741\n    Janeq4_202122333\n    Dinaq4_202114511\n\n\n\n\nTo make our data tidier separate() can split quater column into 2 (quater and year):\n\nbest_data <- better_data %>%\n    separate(quater, c(\"quater\", \"year\"), sep = \"_\") %>% # separate\n    mutate(year = as.integer(year), # convert year to integer\n           quater = substr(better_data$quater, 2,2), # trim `q` from start\n           quater = as.integer(quater), # convert quater to integer\n          ) %>%\n    head(10)\nbest_data\n\n\n\nA data.frame: 10 × 4\n\n    Namequateryearsalary\n    <chr><int><int><dbl>\n\n\n    1Nick1202112442\n    2Jake1202122131\n    3Anna1202121343\n    4Jane1202122111\n    5Dina1202114123\n    6Nick2202113442\n    7Jake2202122871\n    8Anna2202120343\n    9Jane2202122222\n    10Dina2202114456\n\n\n\n\nThe unite() function concanates two columns into one:\n\nunited_data <- best_data %>%\n                unite(Qt, quater, year, sep = \"#\")\nunited_data\n\n\n\nA data.frame: 10 × 3\n\n    NameQtsalary\n    <chr><chr><dbl>\n\n\n    1Nick1#202112442\n    2Jake1#202122131\n    3Anna1#202121343\n    4Jane1#202122111\n    5Dina1#202114123\n    6Nick2#202113442\n    7Jake2#202122871\n    8Anna2#202120343\n    9Jane2#202122222\n    10Dina2#202114456\n\n\n\n\n\n# if dont want remove old columns use remove param\nunited_data <- best_data %>%\n                unite(Qt, quater, year, sep = \"#\", remove = F)\nunited_data\n\n\n\nA data.frame: 10 × 5\n\n    NameQtquateryearsalary\n    <chr><chr><int><int><dbl>\n\n\n    1Nick1#20211202112442\n    2Jake1#20211202122131\n    3Anna1#20211202121343\n    4Jane1#20211202122111\n    5Dina1#20211202114123\n    6Nick2#20212202113442\n    7Jake2#20212202122871\n    8Anna2#20212202120343\n    9Jane2#20212202122222\n    10Dina2#20212202114456\n\n\n\n\nIf you need to make table like initial use spread() function:\n\nnot_good_data2 <- better_data %>%\n                    spread(quater, salary)\nnot_good_data2\n\n\n\nA data.frame: 5 × 5\n\n    Nameq1_2021q2_2021q3_2021q4_2021\n    <chr><dbl><dbl><dbl><dbl>\n\n\n    Anna21343203432245621741\n    Dina14123144561453314511\n    Jake22131228712203120031\n    Jane22111222222244422333\n    Nick12442134421548214511\n\n\n\n\nLet’s try to spread() feild pop of gapminder by year:\n\ngapminder %>% select(country, pop, year) %>%\n                spread(year, pop) %>%\n                head() # for shorter code\n\n# now you can easy send data to your director in excel :)\n\n\n\nA tibble: 6 × 13\n\n    country195219571962196719721977198219871992199720022007\n    <fct><int><int><int><int><int><int><int><int><int><int><int><int>\n\n\n    Afghanistan 8425333 924093410267083115379661307946014880372128818161386795716317921222274152526840531889923\n    Albania     1282697 1476505 1728137 1984060 2263554 2509048 2780097 3075321 3326498 3428038 3508512 3600523\n    Algeria     92795251027085611000948127604991476078717152804200337532325495626298373290720153128714233333216\n    Angola      4232095 4561361 4826015 5247469 5894858 6162675 7016384 7874230 8735988 98750241086610612420476\n    Argentina  178769561961053821283783229342252477979926983828293413743162091833958947362034633833112140301927\n    Australia   8691212 971256910794968118722641317700014074100151842001625724917481977185652431954679220434176\n\n\n\n\nFunctions table:\n\n\n\n\n\n\n\n\nFunction\nObjectives\nArguments\n\n\n\n\ngather()\nTransform the data from wide to long\n(data, key, value, na.rm = FALSE)\n\n\nspread()\nTransform the data from long to wide\n(data, key, value)\n\n\nseparate()\nSplit one variables into two\n(data, col, into, sep= ““, remove = TRUE)\n\n\nunite()\nUnite two variables into one\n(data, col, conc ,sep= ““, remove = TRUE)"
  },
  {
    "objectID": "etl-dplyr.html#refences",
    "href": "etl-dplyr.html#refences",
    "title": "10  Manipulate data with dplyr",
    "section": "10.3 Refences",
    "text": "10.3 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "20  Підготовка та очистка даних у R",
    "section": "",
    "text": "Матеріали розділу описують інформацію про виміри оцінки якості даних, підходи до визначення та обробки пропущених значень, а також розглядаються способи боротьби зі статистичними викидами.\nДо початку роботи варто інстралювати наступні пакети:"
  },
  {
    "objectID": "data-cleaning.html#поняття-та-виміри-оцінки-якості-даних",
    "href": "data-cleaning.html#поняття-та-виміри-оцінки-якості-даних",
    "title": "20  Підготовка та очистка даних у R",
    "section": "20.1 Поняття та виміри оцінки якості даних",
    "text": "20.1 Поняття та виміри оцінки якості даних\nЯкість даних залежить від очищення та коригування даних, які відсутні, некоректні, недійсні або нечитабельні. Для забезпечення достовірності даних важливо зрозуміти ключові аспекти якості даних, щоб оцінити, наскільки дані погані/хороші.\n\n20.1.1 Що таке валідація даних?\nВалідація даних відноситься до процесу забезпечення точності та якості даних. Він реалізується шляхом вбудовування кількох перевірок у систему або звітування для забезпечення логічної узгодженості введених і збережених даних.\nНа перший погляд, очевидно, що перетворення даних до якісних полягає в очищенні поганих даних – даних, які відсутні, неправильні або якимось чином недійсні. Але щоб переконатися, що дані заслуговують довіри, важливо розуміти ключові виміри якості даних, щоб оцінити, наскільки дані є «поганими».\nОкремі компанії мають внутрішні документи, що визначають виміри оцінки якості даних та порядок його проведення - Data Validation Framework або Data Quality Framework.\nКоли говорять про якість даних, то мається на увазі їх оцінка у кількох вимірах. Розглянемо коротко ці виміри:\n\nПравильність / Accuracy\nПовнота / Completeness\nУзгодженість / Consistency\nВідповідність / Conformity\nЦілісність / Integrity\nСвоєчасність / Timeliness\nУнікальність / Uniqueness\n\n\n\n\n20.1.2 Правильність / (Accuracy)\nПравильність — це ступінь, до якого дані правильно відображають реальний об’єкт АБО описувану подію.\nПриклади: - [x] Реальною вартістю є ціна продажу одиниці товару. - [x] Адреса співробітника в базі даних співробітників є справжньою адресою.\nЗапитання, які ви можете задати собі:\n\nЧи об’єкти даних точно представляють значення «реального світу», які вони повинні моделювати? Наприклад, чи правильно вказувати вік у сотнях тисяч років?\nЧи присутнє неправильне написання назв товарів чи осіб, адрес і навіть несвоєчасних чи неактуальних даних?\n\nЦі проблеми можуть вплинути на результатати аналітичних звітів, наприклад, неправильні середні значення певних показників.\n\n\n\n20.1.3 Повнота / (Completeness)\nПовнота визначається як очікувана всебічність. Дані можуть бути повними, навіть якщо додаткові дані відсутні. Поки дані відповідають очікуванням, вони вважаються повними.\nНаприклад, ім’я та прізвище замовника є обов’язковими, але прізвище необов’язково; тому запис можна вважати повним, навіть якщо прізвища не існує.\nПитання, які ви можете задати собі:\n\nЧи доступна вся необхідна інформація?\nЧи мають якісь дані відсутні елементи?\nАбо вони перебувають у непридатному для роботи вигляді?\n\n\n\n\n20.1.4 Узгодженість / Consistency\nУзгодженість означає, що дані в усіх системах/таблицях відображають однакову інформацію та синхронізовані між собою.\nПриклади: - [x] Статус бізнес-підрозділу “закритий”, але є продажі для цього підрозділу. - [x] Статус працівника “звільнено”, але статус випалати заробіної плати містить суму відмінну від 0 за той самий період. - [x] Зафіксовано, що клієнт має у банку депозити, але у даних про депозити записи по клієнту відсутні.\nЗапитання, які ви можете поставити собі:\n\nЧи однакові значення даних у наборах даних?\nЧи існують якісь різні випадки, коли однакові екземпляри даних надають суперечливу інформацію?\n\n\n\n\n20.1.5 Відповідність / Conformity\nВідповідність означає, що дані відповідають набору стандартних визначень даних, як-от тип даних, розмір і формат. Наприклад, дата народження клієнта у форматі dd/mm/yyyy або відстань у км числом 100, а не записом 100км.\nЗапитання, які ви можете задати собі: - [x] Чи відповідають значення даних зазначеним форматам? - [x] Якщо так, то чи всі значення даних відповідають цим форматам?\nВажливо підтримувати відповідність конкретним форматам.\n\n\n\n20.1.6 Цілісність / Integrity\nЦілісність означає достовірність даних у взаємозв’язках і гарантує, що всі дані в базі даних можна відстежити та з’єднати з іншими даними.\nНаприклад, у базі даних клієнтів має бути дійсний клієнт, адреси та відношення/зв’язки між ними. Якщо є дані про зв’язок адреси без клієнта, то ці дані недійсні й вважаються загубленим записом.\nЗапитайте себе: - [x] Чи є якісь дані без важливих зв’язків?\nНеможливість пов’язати записи разом може призвести до дублювання у ваших системах.\n\n\n\n20.1.7 Своєчасність / Timeliness\nСвоєчасність показує, чи є інформація доступною, коли вона очікується та потрібна. Своєчасність даних дуже важлива.\nЦе відображається в: - [x] Компанії, які зобов’язані публікувати свої квартальні результати протягом певного періоду часу - [x] Обслуговування клієнтів надає клієнтам актуальну інформацію - [x] Кредитна система перевіряє активність рахунку кредитної картки в режимі реального часу\nСвоєчасність залежить від очікувань користувача. Доступність даних в Інтернеті може знадобитися для системи розподілу номерів у сфері готельного бізнесу.\nЯк бачите, якість даних є важливим питанням, яке слід враховувати, починаючи від етапу визначення цілей проекту, аж до впровадження, обслуговування та використання готово рішення у виробничі процесі підприємства."
  },
  {
    "objectID": "data-cleaning.html#робота-з-неіменованими-та-поганоіменованими-даними",
    "href": "data-cleaning.html#робота-з-неіменованими-та-поганоіменованими-даними",
    "title": "20  Підготовка та очистка даних у R",
    "section": "20.2 Робота з неіменованими та “поганоіменованими” даними",
    "text": "20.2 Робота з неіменованими та “поганоіменованими” даними\n\n20.2.1 Іменування даних\nПершим прикладом проблем у даних можна розгянути читання неіменованих даних, тобто стопці таблиці не мають заголовків у файлі.\nСтворимо такий файл у блокноті і зчитаємо його:\n\ndata <- read.csv(\"../../data/untitled.csv\")\ndata\n\n\n\nA data.frame: 5 × 4\n\n    X23X185X85.7Male\n    <int><chr><dbl><chr>\n\n\n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\nЗверніть увагу, що у якості стовпців взято перший рядок даних у додано X на початку. Зчитаємо дані із параметром, що вказує на відсутність заголовків:\n\ndata <- read.csv(\"../../data/untitled.csv\", header = FALSE)\ndata\n\n\n\nA data.frame: 6 × 4\n\n    V1V2V3V4\n    <int><chr><dbl><chr>\n\n\n    23185 85.7Male            \n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\nПроблема іменування не вирішена, дані ми уже не втратили. Передамо одночасно з читанням інформацію про назви стовпців:\n\ndata <- read.csv(\"../../data/untitled.csv\", \n            header = FALSE,\n            col.names = c(\"Age\",\"Height\", \"Weight\", \"Gender\"))\ndata\n\n\n\nA data.frame: 6 × 4\n\n    AgeHeightWeightGender\n    <int><chr><dbl><chr>\n\n\n    23185 85.7Male            \n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\nЩе одним варіантом задання назв стовпців є використання функції colnames() як для усіх різом, так і для окремого:\n\ncolnames(data) <- c(\"age\", \"height\", \"width\", \"gender\")\ndata\ncolnames(data)[2] <- \"HEIGHT\"\ndata\n\n\n\nA data.frame: 6 × 4\n\n    ageheightwidthgender\n    <int><chr><dbl><chr>\n\n\n    23185 85.7Male            \n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\n\n\nA data.frame: 6 × 4\n\n    ageHEIGHTwidthgender\n    <int><chr><dbl><chr>\n\n\n    23185 85.7Male            \n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\nТакож змінювати назви стовпців можна за допомогою функції rename() з пакету dplyr:\n\nlibrary(dplyr)\n\ndata <- data |> rename(AGE = age) # %>%\ndata\n\n\n\nA data.frame: 6 × 4\n\n    AGEHEIGHTwidthgender\n    <int><chr><dbl><chr>\n\n\n    23185 85.7Male            \n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\n\n\n\n20.2.2 Заміна назв стовпців data.frame \nЗчитаємо файл, що містить інформацію про осіб, але уже має іменовані стовтці:\n\ndata <- read.csv(\"../../data/badtitled.csv\")\ndata\n\n\n\nA data.frame: 13 × 5\n\n    Person.AgePerson__Heightperson.WeightPerson.Genderempty\n    <int><chr><dbl><chr><lgl>\n\n\n    23185   NAMale  NA\n    41175 68.3   M  NA\n    11142*55.4FemaleNA\n    12NA  48.2Man   NA\n    54191   NAfemaleNA\n    32168 78.0F     NA\n    22NA  54.0male. NA\n    21165   NAm     NA\n    14NA  90.2Man   NA\n    51250   NAfemaleNA\n    4120  81.0F     NA\n    66NA  59.0male. NA\n    71171   NAm     NA\n\n\n\n\nШвидко змінити назви стовпців та привести їх до однакового стилю можна за домогою бібліотеки janitor:\n\nlibrary(janitor)\nclean <- clean_names(data)\ncolnames(clean)\n\n\n'person_age''person_height''person_weight''person_gender''empty'\n\n\n\n\n\n20.2.3 Підготовка та очистка текстової інформації\nЗчитаємо інформацію про стать з попереднього прикладу:\n\ndata <- read.csv(\"../../data/badtitled.csv\")\ndata <- clean_names(data)\ndata <- as.data.frame(data$person_gender)\ncolnames(data) <- c(\"gender\")\nunlist(data)\n\ngender1'Male'gender2'   M'gender3'Female'gender4'Man'gender5'female'gender6'F    'gender7'male.'gender8'm'gender9'Man'gender10'female'gender11'F    'gender12'male.'gender13'm'\n\n\nСхоже, що ці дані насправді мають всього 2 записи, проте у базу даних їх вносили різні люди або вони були зібрані з різних джерел інформації. Це досить поширена проблема у роботі з даними. Особливо коли відбуваєть заміна людей на рочих місцях або перехід на інше програмне забезпечення.\nЯкщо це буде розглядатися як факторна змінна без будь-якої попередньої обробки, очевидно, що 8, а не 2 класи будуть збережені. Тому завдання полягає в тому, щоб автоматично розпізнавати наведені вище дані про те, чи відноситься кожен елемент до чоловічої чи жіночої статі. У статистичних контекстах класифікацію таких “безладні” текстові рядки в ряд фіксованих категорій часто називають кодуванням.\nОпишемо два взаємодоповнюючих підходи до кодування рядків: нормалізація (string normalization) рядків і аналіз схожості тексту (approximate text matching).\nРозглянемо наступні підходи до очистки текстових даних:\n– [x] Видалення пробілів на початку або в кінці\n– [x] Обрізання/збільшення рядків до певної ширини\n– [x] Перетворення у верхній/нижній регістр.\n– [x] Пошук рядків, що містять прості шаблони (підрядки).\n– [x] Апроксимація рядків на основі \"відстаней\".\nРобота з текстом у R здійснюється за допомогою пакету stringr.\nВидалення пробілів на початку або в кінці здійснюється за допомогою функції str_trim().\n\nlibrary(stringr)\nstr_trim(\" ostroh academy  \")\nstr_trim(\" ostroh academy \", side = \"left\")\nstr_trim(\" ostroh academy \", side = \"right\")\n\n'ostroh academy'\n\n\n'ostroh academy '\n\n\n' ostroh academy'\n\n\nОбрізання/збільшення рядків до певної ширини здійснюється за допомогою функції str_pad().\n\nstr_pad(57, width = 6, side = \"left\", pad = 0)\n\n'000057'\n\n\n\nstr_pad(\"ostroh\", width = 10, side = \"right\", pad = \"_\")\n\n'ostroh____'\n\n\nПеретворення у верхній/нижній регістр\n\ntext <- \"Ostroh Academy!\"\ntoupper(text)\ntolower(text)\n\n'OSTROH ACADEMY!'\n\n\n'ostroh academy!'\n\n\nПошук рядків, що містять прості шаблони (підрядки)\nСкористаємося функцієя grep() та grepl() для пошуку підрядків у інформації про стать:\n\ngrepl(\"m\", data$gender) # Повертає TRUE/FALSE, якщо знахоить входження рядка\ngrep(\"m\", data$gender) # Повертає номери рядків, по яких є входження\n\n\nFALSEFALSETRUEFALSETRUEFALSETRUETRUEFALSETRUEFALSETRUETRUE\n\n\n\n3578101213\n\n\n\ngrepl(\"m\", data$gender, ignore.case = TRUE) # не враховує регістр букв\ngrepl(\"m\", tolower(data$gender))\n\n\nTRUETRUETRUETRUETRUEFALSETRUETRUETRUETRUEFALSETRUETRUE\n\n\n\nTRUETRUETRUETRUETRUEFALSETRUETRUETRUETRUEFALSETRUETRUE\n\n\n\ndata$gender\ngrepl(\"^m\", data$gender, ignore.case = TRUE) # Показує усі збіги, що починаються з вказаної літери\n\n\n'Male''   M''Female''Man''female''F    ''male.''m''Man''female''F    ''male.''m'\n\n\n\nTRUEFALSEFALSETRUEFALSEFALSETRUETRUETRUEFALSEFALSETRUETRUE\n\n\nПошук “відстані” між ряжками - це аналіз рядків на схожіть з визначенням рівня співпадінь.\n\nadist(\"ao\", \"ao\")\nadist(\"ao\", \"oa\")\nadist(\"ao\", \"45fb\")\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0\n\n\n\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2\n\n\n\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    4\n\n\n\n\nДавайте проаналізуємо інформацію про стать з точки зору схожості текстів:\n\nm <- c(\"male\", \"female\")\nadj_m <- adist(data$gender, m)\n#adj_m <- adist(tolower(data$gender), m)\n#adj_m <- adist(str_trim(tolower(data$gender), side=\"both\"), m)\ncolnames(adj_m) <- m \nrownames(adj_m) <- data$gender\nadj_m\n\n\n\nA matrix: 13 × 2 of type dbl\n\n    malefemale\n\n\n    Male13\n       M46\n    Female21\n    Man35\n    female20\n    F    56\n    male.13\n    m35\n    Man35\n    female20\n    F    56\n    male.13\n    m35\n\n\n\n\n\n# Видалимо повтори\nadj_m |> as.data.frame() |> dplyr::distinct()\n\n\n\nA data.frame: 6 × 2\n\n    malefemale\n    <dbl><dbl>\n\n\n    Male13\n    X...M46\n    Female21\n    Man35\n    female20\n    F....56\n\n\n\n\nПримінимо інформацію про відстані до “нечистих” даних про стать:\n\nnums <- apply(adj_m, 1, which.min) # Знайдемо найближчі значення\nnums\n\nMale1   M1Female2Man1female2F    1male.1m1Man1female2F    1male.1m1\n\n\n\ndata.frame(initial = data$gender, coded = m[nums]) # FFFFFFFFFFFFFF - проблема!\n\n\n\nA data.frame: 13 × 2\n\n    initialcoded\n    <chr><chr>\n\n\n    Male  male  \n       M  male  \n    Femalefemale\n    Man   male  \n    femalefemale\n    F     male  \n    male. male  \n    m     male  \n    Man   male  \n    femalefemale\n    F     male  \n    male. male  \n    m     male  \n\n\n\n\nЯк альтернативу для знаходження відстаней між рядками можна використовувати функції з бібліотеки stringdist.\n\nlibrary(stringdist)\nadist(\"ao\", \"oa\")\nstringdist(\"oa\", \"ao\") # 1, а було 2\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2\n\n\n\n\n1\n\n\nСпробуємо “очистити” дані, які ми отримали з допомогою функції amatch():\n\nnums <- amatch(data$gender,  c(\"male\", \"female\"), maxDist = 4) # Знайдемо найближчі значення\nnums\n\n\n11212<NA>1112<NA>11\n\n\n\ndata.frame(initial = data$gender, coded = m[nums]) # FFFFFFFFFFFFFF - проблема!\n\n\n\nA data.frame: 13 × 2\n\n    initialcoded\n    <chr><chr>\n\n\n    Male  male  \n       M  male  \n    Femalefemale\n    Man   male  \n    femalefemale\n    F     NA    \n    male. male  \n    m     male  \n    Man   male  \n    femalefemale\n    F     NA    \n    male. male  \n    m     male  \n\n\n\n\n\ndata <- data |> mutate(gender = ifelse(gender == \"F\", \"female\", gender)) # ????? # Space\ndata\ndata <- data |> mutate(gender = ifelse(str_trim(gender) == \"F\", \"female\", gender))\ndata\nnums <- amatch(data$gender,  c(\"male\", \"female\"), maxDist = 4)\ndata.frame(initial = data$gender, coded = m[nums]) \n\n\n\nA data.frame: 13 × 1\n\n    gender\n    <chr>\n\n\n    Male  \n       M  \n    Female\n    Man   \n    female\n    F     \n    male. \n    m     \n    Man   \n    female\n    F     \n    male. \n    m     \n\n\n\n\n\n\nA data.frame: 13 × 1\n\n    gender\n    <chr>\n\n\n    Male  \n       M  \n    Female\n    Man   \n    female\n    female\n    male. \n    m     \n    Man   \n    female\n    female\n    male. \n    m     \n\n\n\n\n\n\nA data.frame: 13 × 2\n\n    initialcoded\n    <chr><chr>\n\n\n    Male  male  \n       M  male  \n    Femalefemale\n    Man   male  \n    femalefemale\n    femalefemale\n    male. male  \n    m     male  \n    Man   male  \n    femalefemale\n    femalefemale\n    male. male  \n    m     male  \n\n\n\n\nМісія виконана! Замінимо та збережемо інформацію у файл для майбутніх експериментів по цій темі:\n\ndata <- read.csv(\"../../data/badtitled.csv\")\ndata <- clean_names(data)\nhead(data, 2)\n\n\n\nA data.frame: 2 × 5\n\n    person_ageperson_heightperson_weightperson_genderempty\n    <int><chr><dbl><chr><lgl>\n\n\n    123185  NAMaleNA\n    24117568.3   MNA\n\n\n\n\n\ndata <- data |> mutate(person_gender = ifelse(str_trim(person_gender) == \"F\", \"female\", person_gender))\nm <- c(\"male\", \"female\")\nnums <- amatch(data$person_gender, m, maxDist = 4)\ndata <- data |> mutate(person_gender = m[nums])\ndata\n\n\n\nA data.frame: 13 × 5\n\n    person_ageperson_heightperson_weightperson_genderempty\n    <int><chr><dbl><chr><lgl>\n\n\n    23185   NAmale  NA\n    41175 68.3male  NA\n    11142*55.4femaleNA\n    12NA  48.2male  NA\n    54191   NAfemaleNA\n    32168 78.0femaleNA\n    22NA  54.0male  NA\n    21165   NAmale  NA\n    14NA  90.2male  NA\n    51250   NAfemaleNA\n    4120  81.0femaleNA\n    66NA  59.0male  NA\n    71171   NAmale  NA\n\n\n\n\nЗамінимо також висоту на числове значення, а не текст:\n\ndata <- data |> \n    mutate(person_height = str_remove(data$person_height, pattern = \"[*]\"))\ndata\n\nERROR: Error in mutate(data, person_height = str_remove(data$person_height, pattern = \"[*]\")): could not find function \"mutate\"\n\n\n\ndata <- data |> mutate(person_height = as.numeric(person_height))\ndata\n\n\n\nA data.frame: 13 × 5\n\n    person_ageperson_heightperson_weightperson_genderempty\n    <int><dbl><dbl><chr><lgl>\n\n\n    23185  NAmale  NA\n    4117568.3male  NA\n    1114255.4femaleNA\n    12 NA48.2male  NA\n    54191  NAfemaleNA\n    3216878.0femaleNA\n    22 NA54.0male  NA\n    21165  NAmale  NA\n    14 NA90.2male  NA\n    51250  NAfemaleNA\n    41 2081.0femaleNA\n    66 NA59.0male  NA\n    71171  NAmale  NA\n\n\n\n\n\nwrite.csv(data, file = \"../../data/cleaned_titled.csv\", row.names = F)"
  },
  {
    "objectID": "data-cleaning.html#заміна-пропусків-у-даних-missing-value-imputation",
    "href": "data-cleaning.html#заміна-пропусків-у-даних-missing-value-imputation",
    "title": "20  Підготовка та очистка даних у R",
    "section": "20.3 Заміна пропусків у даних (Missing Value Imputation)",
    "text": "20.3 Заміна пропусків у даних (Missing Value Imputation)\nДані реального світу часто мають відсутні значення. Дані можуть мати відсутні значення з ряду причин, таких як спостереження, які не були записані, пошкодження даних тощо.\nПроблема - [x] Обробка відсутніх даних важлива, оскільки багато алгоритмів машинного навчання або програм для візуалізації та аналізу данихне підтримують дані з відсутніми значеннями.\nРішення\n\nВидалити рядки з відсутніми даними з набору даних.\nЗамінити відсутні значення середніми/медіанними значеннями.\n\nПримітка\n\nВикористовуйте бізнес-логіку/знання для окремого підходу до кожної змінної\nУ разі малого розміру вибірки або великої частки спостережень із відсутніми значеннями бажано замінювати, а видаляти\n\nНекоректна інформація в даних може бути записана різними способами, наприклад у датасеті ці дані можуть бутьу визначені як NA <NA> NULL undefinded Undefined. Перед обробкою таких даних усі невизначені записи варто конвертувати у NA.\nЩоб переглянути список усіх стовпців, що мають пропуски даних можна скористатися наступним кодом:\n\ndata <- data <- read.csv(\"../../data/cleaned_titled.csv\", na.strings = c(\"<NA>\", \"NA\", \"null\", \"undefined\", \"NULL\", \"\"))\nglimpse(data)\n\nRows: 13\nColumns: 5\n$ person_age    <int> 23, 41, 11, 12, 54, 32, 22, 21, 14, 51, 41, 66, 71\n$ person_height <int> 185, 175, 142, NA, 191, 168, NA, 165, NA, 250, 20, NA, 1~\n$ person_weight <dbl> NA, 68.3, 55.4, 48.2, NA, 78.0, 54.0, NA, 90.2, NA, 81.0~\n$ person_gender <chr> \"male\", \"male\", \"female\", \"male\", \"female\", \"female\", \"m~\n$ empty         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA\n\n\n\n20.3.1 Перевірка наявності пропусків у даних\nПакет MICE (Multivariate Imputation via Chained Equations)\n\nlibrary(mice)\nmd.pattern(data)\n\n\n\nA matrix: 4 × 6 of type dbl\n\n    person_ageperson_genderperson_heightperson_weightempty\n\n\n    41111 0 1\n    51110 0 2\n    41101 0 2\n    00451322\n\n\n\n\n\n\n\n\nlibrary(VIM)\nmice_plot <- aggr(data, \n                  col=c('navyblue','yellow'),\n                  numbers=TRUE, \n                  sortVars=TRUE,\n                  labels=names(data), \n                  cex.axis=.7,\n                  gap=3, \n                  ylab=c(\"Missing data\",\"Pattern\"))\nmice_plot\n\n\n Variables sorted by number of missings: \n      Variable     Count\n         empty 1.0000000\n person_weight 0.3846154\n person_height 0.3076923\n    person_age 0.0000000\n person_gender 0.0000000\n\n\n\n Missings in variables:\n      Variable Count\n person_height     4\n person_weight     5\n         empty    13\n\n\n\n\n\n\nlibrary(Amelia)\nAmelia::missmap(data)\n\nWarning message in is.na(obj):\n\"is.na() applied to non-(list or vector) of type 'closure'\"\n\n\nERROR: Error in colMeans(is.na(obj)): 'x' must be an array of at least two dimensions\n\n\nТакож можна скористатися альтернативними макетами: missForest, mi.\n\n\n\n20.3.2 Видалення пустих рядків та сповпців у data.frame\nПереглянемо стовпці, що містять пропуски:\n\n# Переглянемо список стовпців з пропусками\ncolnames(data)[apply(data, 2, anyNA)]\n\n\n'person_height''person_weight''empty'\n\n\nФункція complete.cases повертає логічні значення\n\ncomplete.cases(data) # бо є стовпець Empty\n\n\nFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSE\n\n\nТакож видаляти стовпці та рядки з data.frame можна за допомогою пакету janitor.\n\nlibrary(janitor)\ndata_cleaned <- remove_empty(data, which = c(\"rows\",\"cols\"), quiet = FALSE)\ndata_cleaned\n\nNo empty rows to remove.\n\nRemoving 1 empty columns of 5 columns total (Removed: empty).\n\n\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    123185  NAmale  \n    24117568.3male  \n    31114255.4female\n    412 NA48.2male  \n    554191  NAfemale\n    63216878.0female\n    722 NA54.0male  \n    821165  NAmale  \n    914 NA90.2male  \n    1051250  NAfemale\n    1141 2081.0female\n    1266 NA59.0male  \n    1371171  NAmale  \n\n\n\n\n\nwrite.csv(data_cleaned, file = \"../../data/cleaned_titled2.csv\", row.names = F)\n\nЯк бачимо, колонка empty була видалена.\nЩоб переглянути усі записи, що не мають пропусків скористаємося функцією na.omit():\n\nna.omit(data_cleaned)\n\n\n\nA data.frame: 4 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    24117568.3male  \n    31114255.4female\n    63216878.0female\n    1141 2081.0female\n\n\n\n\nТаким чином пропущені значення будуть видалені з датасети, якщо інформацію переприсвоїти data <- na.omit(data)\n\n\n\n20.3.3 Заміна пропусків у data.frame\nІснує ряд підходів, що використовуються для заміни пропущених значень у датасеті:\nЗаміна на 0 * Вставте пропущені значення нулем\nЗаміна на медіану/середнє значення * Для числових змінних - середнє або медіана, мінімум, максимум * Для категоріальних змінних - мода (бувають випадки, коли моду доцільно використовувати і для цислових)\nСегментна заміна * Визначення сегментів * Обчислення середнього/медіани/моди для сегментів * Замінити значення по сегментах * Наприклад, ми можемо сказати, що кількість опадів майже не змінюється для міст у певній області України, у такому випадку ми можемо для усіх міст з пропусками записати значення середнє по регіону.\nІнтелектуальна заміна (Частковий випадок сегментної заміни) * Заміна значень з використанням методів машинного навчання\n\n20.3.3.1 3.3.1. Заміна пропусків на нуль (0)\n\ndata <- read.csv(\"../../data/cleaned_titled2.csv\")\ndata\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    23185  NAmale  \n    4117568.3male  \n    1114255.4female\n    12 NA48.2male  \n    54191  NAfemale\n    3216878.0female\n    22 NA54.0male  \n    21165  NAmale  \n    14 NA90.2male  \n    51250  NAfemale\n    41 2081.0female\n    66 NA59.0male  \n    71171  NAmale  \n\n\n\n\nЗамінимо інформацію про вагу з пропусками на 0:\n\ndata_w0 <- data |> mutate(person_weight = ifelse(is.na(person_weight), 0, person_weight))\ndata_w0\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    23185 0.0male  \n    4117568.3male  \n    1114255.4female\n    12 NA48.2male  \n    54191 0.0female\n    3216878.0female\n    22 NA54.0male  \n    21165 0.0male  \n    14 NA90.2male  \n    51250 0.0female\n    41 2081.0female\n    66 NA59.0male  \n    71171 0.0male  \n\n\n\n\n\n# Без dplyr\ndata_w0 <- data\ndata_w0[is.na(data_w0$person_weight), \"person_weight\"] <- 0\ndata_w0\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    23185 0.0male  \n    4117568.3male  \n    1114255.4female\n    12 NA48.2male  \n    54191 0.0female\n    3216878.0female\n    22 NA54.0male  \n    21165 0.0male  \n    14 NA90.2male  \n    51250 0.0female\n    41 2081.0female\n    66 NA59.0male  \n    71171 0.0male  \n\n\n\n\nЗробити заміну для усіх числових стовпців:\n\nlibrary(tidyr) # for replace_na()\ndata_all <- data |> mutate_if(is.numeric , replace_na, replace = 0)\ndata_all\n\n\nAttaching package: 'tidyr'\n\n\nThe following object is masked from 'package:stringdist':\n\n    extract\n\n\n\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    23185 0.0male  \n    4117568.3male  \n    1114255.4female\n    12  048.2male  \n    54191 0.0female\n    3216878.0female\n    22  054.0male  \n    21165 0.0male  \n    14  090.2male  \n    51250 0.0female\n    41 2081.0female\n    66  059.0male  \n    71171 0.0male  \n\n\n\n\n\n\n\n20.3.3.2 Базова числова заміна пропусків\nЗаміна на константи або обчислені значення є стандарним підходом. Так, наприклад, заміна певного значення на середнє матиме вигляд:\n\ndata_m <- data |> \n    mutate(person_weight = ifelse(is.na(person_weight), mean(data$person_weight, na.rm = T), person_weight))\ndata_m\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    2318566.7625male  \n    4117568.3000male  \n    1114255.4000female\n    12 NA48.2000male  \n    5419166.7625female\n    3216878.0000female\n    22 NA54.0000male  \n    2116566.7625male  \n    14 NA90.2000male  \n    5125066.7625female\n    41 2081.0000female\n    66 NA59.0000male  \n    7117166.7625male  \n\n\n\n\nЗаміна на min, max, median не відрізняється.\nЯкщо виникає потреба замінити, наприклад, усі значення на медіану у всіх стовпцях за один прохід можна скористатися функцією mutate_if():\n\ndata_all <- data |> \n    mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm = T), x))\ndata_all\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    2318563.65male  \n    4117568.30male  \n    1114255.40female\n    1217148.20male  \n    5419163.65female\n    3216878.00female\n    2217154.00male  \n    2116563.65male  \n    1417190.20male  \n    5125063.65female\n    41 2081.00female\n    6617159.00male  \n    7117163.65male  \n\n\n\n\nРозгялнемо кілька бібліотек для перевірки даних на наявність про\nЩе одним із варіантів заміни значень може бути використання бібліотеки Hmisc:\n\nlibrary(Hmisc)\ndata_wm <- data |> mutate(person_weight = impute(data$person_weight, fun = mean)) # mean imputation\n# Аналогічно можна замінити на min,max, median чи інші функції\ndata_wm \n# * Значення із * - замінені\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><impute><chr>\n\n\n    2318566.7625male  \n    4117568.3000male  \n    1114255.4000female\n    12 NA48.2000male  \n    5419166.7625female\n    3216878.0000female\n    22 NA54.0000male  \n    2116566.7625male  \n    14 NA90.2000male  \n    5125066.7625female\n    41 2081.0000female\n    66 NA59.0000male  \n    7117166.7625male  \n\n\n\n\n\n\n\n20.3.3.3 Hot deck imputation (як перекласти???)\nМетод Hot deck imputation передбачає, що пропущені значення обчислюються шляхом копіювання значень із подібних записів у тому ж наборі даних.\nОсновне питання при Hot deck imputation полягає в тому, як вибрати значення заміни. Одним із поширених підходів є випадковий відбір:\n\n# set.seed(1)\ndata_hot <- data |> mutate(person_weight = impute(data$person_weight, \"random\")) \ndata_hot \n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><impute><chr>\n\n\n    2318559.0male  \n    4117568.3male  \n    1114255.4female\n    12 NA48.2male  \n    5419178.0female\n    3216878.0female\n    22 NA54.0male  \n    2116568.3male  \n    14 NA90.2male  \n    5125048.2female\n    41 2081.0female\n    66 NA59.0male  \n    7117190.2male  \n\n\n\n\nВихідне значення залежить від значення seed.\n\n\n\n20.3.3.4 Сегментна заміна пропусків\nЗаміна по сегментах часто дозволяє будувати точніші математичні моделі, адже групові середні краще описують явища і процеси, ніж загальні для всієї вибірки.\nЗнайдемо середні значення ваги за статтю та використаємо ці значення для заміни пропусків у даних.\n\ndata_sgm <- data |> \n                group_by(person_gender) |>\n                mutate(person_weight = replace_na(person_weight, mean(person_weight, na.rm = TRUE)))\ndata_sgm\n\n\n\nA grouped_df: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    2318563.94000male  \n    4117568.30000male  \n    1114255.40000female\n    12 NA48.20000male  \n    5419171.46667female\n    3216878.00000female\n    22 NA54.00000male  \n    2116563.94000male  \n    14 NA90.20000male  \n    5125071.46667female\n    41 2081.00000female\n    66 NA59.00000male  \n    7117163.94000male  \n\n\n\n\nТакож можна здійснити заміну значень по усіх стовпцях датасету за один раз. Проте не варто такий підхід використовувати постійно, а враховувати бізнес-логіку процесів, що вивчаються.\n\ndata_sgm2 <- data %>% \n  group_by(person_gender) %>% \n    mutate(\n      across(everything(), ~replace_na(.x, min(.x, na.rm = TRUE)))\n    )\ndata_sgm2\n\n\n\nA grouped_df: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    2318548.2male  \n    4117568.3male  \n    1114255.4female\n    1216548.2male  \n    5419155.4female\n    3216878.0female\n    2216554.0male  \n    2116548.2male  \n    1416590.2male  \n    5125055.4female\n    41 2081.0female\n    6616559.0male  \n    7117148.2male  \n\n\n\n\nЯкщо ж є потреба замінювати по окремих стовпцях, то їх можна вказати замість everything(): across(c(\"person_height\", \"person_weight\"), ~replace_na(.x, min(.x, na.rm = TRUE))).\nІншим варіантом може бути вказання номерів колонок: across(c(1,3), ~replace_na(.x, min(.x, na.rm = TRUE)))\n\n\n\n20.3.3.5 Інтелектуальні методи заміни\nТеоретично інтелектуальні методи заміни пропусків є найкращими, адже враховують математичні залежності у даних.\n\nlibrary(VIM)\ndata_knn <- kNN(data)\ndata_knn\ndata\n\n\n\nA data.frame: 13 × 8\n\n    person_ageperson_heightperson_weightperson_genderperson_age_impperson_height_impperson_weight_impperson_gender_imp\n    <int><int><dbl><chr><lgl><lgl><lgl><lgl>\n\n\n    2318559.0male  FALSEFALSE TRUEFALSE\n    4117568.3male  FALSEFALSEFALSEFALSE\n    1114255.4femaleFALSEFALSEFALSEFALSE\n    1216848.2male  FALSE TRUEFALSEFALSE\n    5419168.3femaleFALSEFALSE TRUEFALSE\n    3216878.0femaleFALSEFALSEFALSEFALSE\n    2216854.0male  FALSE TRUEFALSEFALSE\n    2116559.0male  FALSEFALSE TRUEFALSE\n    1416890.2male  FALSE TRUEFALSEFALSE\n    5125068.3femaleFALSEFALSE TRUEFALSE\n    41 2081.0femaleFALSEFALSEFALSEFALSE\n    6616859.0male  FALSE TRUEFALSEFALSE\n    7117159.0male  FALSEFALSE TRUEFALSE\n\n\n\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    23185  NAmale  \n    4117568.3male  \n    1114255.4female\n    12 NA48.2male  \n    54191  NAfemale\n    3216878.0female\n    22 NA54.0male  \n    21165  NAmale  \n    14 NA90.2male  \n    51250  NAfemale\n    41 2081.0female\n    66 NA59.0male  \n    71171  NAmale  \n\n\n\n\nЩе одним схожим методом заміни пропусків може бути здійснення прогнозів на основі регресії чи складніших математичних методів пропусків."
  },
  {
    "objectID": "data-cleaning.html#аналіз-та-обробка-статистичних-викидів-у-даних",
    "href": "data-cleaning.html#аналіз-та-обробка-статистичних-викидів-у-даних",
    "title": "20  Підготовка та очистка даних у R",
    "section": "20.4 Аналіз та обробка статистичних викидів у даних",
    "text": "20.4 Аналіз та обробка статистичних викидів у даних\nВиявлення аномалій — це сукупність методів, призначених для виявлення незвичайних точок даних, які мають вирішальне значення для виявлення шахрайства та захисту комп’ютерних мереж від зловмисної діяльності.\nАномалія - точка даних або набір точок даних, які не мають таку саму структуру та поведінку, що й інші дані.\nАномалії у даних можуть мати різну природу та по різному себе проявляти:\n\nТочкова аномалія\nЄдина точка даних\nНезвично в порівнянні з іншими даними\n\n\nПриклад: одна добова висока температура 41°С серед ряду звичайних весняних днів\n\ntemp <- c(15, 17, 19, 12, 30, 41, 17, 20)\nboxplot(temp, ylab = \"Celcium\")\n\n\n\n\n\nКолективна аномалія\nАномальна колекція екземплярів даних\nНезвично, якщо розглядати разом\n\n\nОпишемо набір даних, до використовуватиметься надалі для прикладів.\nriver_eco - це data.frame, що містить такі три стовпці: - [x] index - цілі числа, що описують порядок спостережень нітратів; - [x] nitrate - місячні концентрації розчинених нітратів у річці; - [x] month - змінна, що містить місяць для кожного спостереження нітратів\nНам потрібно дослідити стовпець nitrate, щоб оцінити наявність точкових аномалій у даних.\n\nriver_data <- read.csv(\"../../data/river_eco.csv\")\nhead(river_data)\n\n\n\nA data.frame: 6 × 3\n\n    indexnitratemonths\n    <int><dbl><chr>\n\n\n    111.581January \n    221.323February\n    331.140March   \n    441.245April   \n    551.072May     \n    661.483June    \n\n\n\n\nПереглянемо описову статистику показника нітрати:\n\nsummary(river_data$nitrate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5920  0.9485  1.0680  1.0649  1.1700  1.8970 \n\n\nЯк видно, медіана та середнє відрізняються не дуже.\nДалі перевіримо наявність викидів у даних за допомогою boxplot:\n\nboxplot(river_data$nitrate)\n# Додамо лінії 1 та 3 квантилів\nabline(h=quantile(river_data$nitrate,0.25),col=\"red\",lty=2)\nabline(h=quantile(river_data$nitrate,0.75),col=\"red\",lty=2)\n\n\n\n\nТакож виведемо номери рядків спостереженнь, що є викидами:\n\nboxplot.stats(river_data$nitrate)$out\n\n\n1.5811.6431.5331.5171.8970.592\n\n\n\nhist(river_data$nitrate, xlab = \"Nitrate concentration\", breaks = 40)\n\n\n\n\n\nplot(nitrate ~ index, data = river_data, type = \"o\")\n\n\n\n\n\n# Середньомісячний вміст нітратів у річці\nriver_grouped <- river_data |> group_by(months) |> summarise(mean = mean(nitrate))\nriver_grouped \n\n\n\nA tibble: 12 × 2\n\n    monthsmean\n    <chr><dbl>\n\n\n    April    1.0166250\n    August   0.9380833\n    December 1.2264167\n    February 1.1838400\n    January  1.2163600\n    July     0.9810417\n    June     0.9792083\n    March    1.1050400\n    May      0.9978333\n    November 1.0962500\n    October  1.0360000\n    September0.9885833\n\n\n\n\n\nplot(river_grouped$mean, type = \"o\", xlab = \"Month\", ylab = \"Monthly mean\")\n\n\n\n\n\nboxplot(nitrate ~ months, data = river_data)\n\n\n\n\nМіж Q1 та Q2 зосереджено 50% усіх спостережень. Персентиль відображає кількість спостережень, що зосереджені з ним включно. Нижче розміщено більше інформації для ознайомлення з інформацією про квантилі.\nQuantile. Wikipedia\n\nДжерело: https://en.wikipedia.org/wiki/Interquartile_range\n\nДжерело: https://makemeanalyst.com/explore-your-data-range-interquartile-range-and-box-plot/\nВизначивши викиди у даних з ними можна здійснити кілька операцій:\n\nЗаміна на деякі значення (impute)\nЗаміна на границі квантилей\n\n\nlower_bound <- quantile(river_data$nitrate, 0.025)\nlower_bound\n\nupper_bound <- quantile(river_data$nitrate, 0.975)\nupper_bound\n\n2.5%: 0.75475\n\n\n97.5%: 1.4095\n\n\n\noutlier_index <- which(river_data$nitrate < lower_bound | river_data$nitrate > upper_bound)\noutlier_index\n\n\n163653104119121156159167199200269270281282\n\n\n\nriver_data[outlier_index, ]\n\n\n\nA data.frame: 16 × 3\n\n    indexnitratemonths\n    <int><dbl><chr>\n\n\n    1  11.581January \n    6  61.483June    \n    36 361.643December\n    53 531.533May     \n    1041040.671August  \n    1191191.517November\n    1211211.414January \n    1561561.897December\n    1591591.414March   \n    1671670.671November\n    1991990.748July    \n    2002000.592August  \n    2692690.700May     \n    2702700.673June    \n    2812810.730May     \n    2822820.693June    \n\n\n\n\nТаким чином, усі значення вище та нище деякого показника можемо замінити на потрібні нам значення, наприклад, середні за поточний місяць.\nЗдійснимо заміну значень у наборі даних на основі квантилей:\n\nriver_data$nitrate_upd <- river_data$nitrate\nqnt <- quantile(river_data$nitrate_upd, probs=c(.05, .95), na.rm = T)\nH <- 1.5 * IQR(qnt[1], na.rm = T)\nriver_data$nitrate_upd[river_data$nitrate_upd < (qnt[1] - H)] <- qnt[1]\nriver_data$nitrate_upd[river_data$nitrate_upd > (qnt[2] + H)] <- qnt[2]\n\nqnt\n\n5%0.80595%1.3325\n\n\n\nboxplot(river_data$nitrate_upd)\n\n\n\n\n\nboxplot(nitrate_upd ~ months, data = river_data)\n\n\n\n\n\nplot(nitrate_upd ~ index, data = river_data, type = \"o\")"
  },
  {
    "objectID": "data-cleaning.html#додаткові-прийоми-очистки-даних",
    "href": "data-cleaning.html#додаткові-прийоми-очистки-даних",
    "title": "20  Підготовка та очистка даних у R",
    "section": "20.5 Додаткові прийоми очистки даних ",
    "text": "20.5 Додаткові прийоми очистки даних \n\n20.5.1 Видалення дублікатів\n\ndf <- data.frame(X = c(1,1,2,1,3,2,1), Y = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\"))\ndf\n\n\n\nA data.frame: 7 × 2\n\n    XY\n    <dbl><chr>\n\n\n    1A\n    1B\n    2C\n    1A\n    3B\n    2C\n    1A\n\n\n\n\n\ndf |> distinct()\n\n\n\nA data.frame: 4 × 2\n\n    XY\n    <dbl><chr>\n\n\n    1A\n    1B\n    2C\n    3B"
  },
  {
    "objectID": "data-cleaning.html#завдання-для-практики",
    "href": "data-cleaning.html#завдання-для-практики",
    "title": "20  Підготовка та очистка даних у R",
    "section": "20.6 Завдання для практики",
    "text": "20.6 Завдання для практики\n!!!"
  },
  {
    "objectID": "data-cleaning.html#набори-даних",
    "href": "data-cleaning.html#набори-даних",
    "title": "20  Підготовка та очистка даних у R",
    "section": "20.7 Набори даних",
    "text": "20.7 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv"
  },
  {
    "objectID": "data-cleaning.html#використані-та-додаткові-джерела",
    "href": "data-cleaning.html#використані-та-додаткові-джерела",
    "title": "20  Підготовка та очистка даних у R",
    "section": "20.8 Використані та додаткові джерела",
    "text": "20.8 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia"
  },
  {
    "objectID": "30-r-what-is-dplyr.html",
    "href": "30-r-what-is-dplyr.html",
    "title": "10  What’s dplyr package [EN]",
    "section": "",
    "text": "author: Юрій Клебан\nThe dplyr package is one of the most powerful and popular package in R for data manipulation.\nWorking with data:\nThe dplyr package makes these steps fast and easy:\nBefore use you should install package:\nNext step is loading package:\ndplyr functions work with pipes and expect tidy data. In tidy data:\nAlternative way is to load tidyverse package with other attached:"
  },
  {
    "objectID": "30-r-what-is-dplyr.html#refences",
    "href": "30-r-what-is-dplyr.html#refences",
    "title": "10  What’s dplyr package [EN]",
    "section": "10.1 Refences",
    "text": "10.1 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "31-r-data-explore.html",
    "href": "31-r-data-explore.html",
    "title": "11  Exploring data with dplyr",
    "section": "",
    "text": "author: Юрій Клебан"
  },
  {
    "objectID": "31-r-data-explore.html#basic-funtions-and-dataset-explore",
    "href": "31-r-data-explore.html#basic-funtions-and-dataset-explore",
    "title": "11  Exploring data with dplyr",
    "section": "11.1 Basic funtions and dataset explore",
    "text": "11.1 Basic funtions and dataset explore\nThere are most popular functions in dplyr is listed in table.\n\n\n\ndplyr Function\nDescription\nEquivalent SQL\n\n\n\n\nselect()\nSelecting columns (variables)\nSELECT\n\n\nfilter()\nFilter (subset) rows.\nWHERE\n\n\ngroup_by()\nGroup the data\nGROUP BY\n\n\nsummarise()\nSummarise (or aggregate) data\n-\n\n\narrange()\nSort the data\nORDER BY\n\n\njoin()\nJoining data frames (tables)\nJOIN\n\n\nmutate()\nCreating New Variables\nCOLUMN ALIAS\n\n\n\nFor the next sample we are going to use gapminder dataset. Go to gapminder dataset description\nThe gapminder data frame include six variables:\n\n\n\nvariable\nmeaning\n\n\n\n\ncountry\n-\n\n\ncontinent\n-\n\n\nyear\n-\n\n\nlifeExp\nlife expectancy at birth\n\n\npop\ntotal population\n\n\ngdpPercap\nper-capita GDP\n\n\n\nPer-capita GDP (Gross domestic product) is given in units of international dollars, a hypothetical unit of currency that has the same purchasing power parity that the U.S. dollar had in the United States at a given point in time – 2005, in this case.\nThe gapminder data frame is a special kind of data frame: a tibble.\n\nlibrary(dplyr) # for demos\n#install.packages(\"gapminder\")\nlibrary(gapminder)  # load package and dataset\nclass(gapminder)\n\n\n'tbl_df''tbl''data.frame'\n\n\nLet’s preview it with functions str(), glimpse(), head(), tail(), summary().\n\nstr(gapminder)\n\ntibble [1,704 x 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\n\n\n\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   <fct> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", ~\n$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, ~\n$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, ~\n$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8~\n$ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12~\n$ gdpPercap <dbl> 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, ~\n\n\n\nhead(gapminder) #shows first n-rows, 6 by default\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453\n    AfghanistanAsia195730.332 9240934820.8530\n    AfghanistanAsia196231.99710267083853.1007\n    AfghanistanAsia196734.02011537966836.1971\n    AfghanistanAsia197236.08813079460739.9811\n    AfghanistanAsia197738.43814880372786.1134\n\n\n\n\n\ntail(gapminder) #shows last n-rows, 6 by default\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    ZimbabweAfrica198260.363 7636524788.8550\n    ZimbabweAfrica198762.351 9216418706.1573\n    ZimbabweAfrica199260.37710704340693.4208\n    ZimbabweAfrica199746.80911404948792.4500\n    ZimbabweAfrica200239.98911926563672.0386\n    ZimbabweAfrica200743.48712311143469.7093\n\n\n\n\n\nsummary(gapminder)\n\n        country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1"
  },
  {
    "objectID": "31-r-data-explore.html#filter-function",
    "href": "31-r-data-explore.html#filter-function",
    "title": "11  Exploring data with dplyr",
    "section": "11.2 filter() function",
    "text": "11.2 filter() function\n\naustria <- filter(gapminder, country == \"Austria\")\naustria\n\n\n\nA tibble: 12 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AustriaEurope195266.8006927772 6137.076\n    AustriaEurope195767.4806965860 8842.598\n    AustriaEurope196269.540712986410750.721\n    AustriaEurope196770.140737699812834.602\n    AustriaEurope197270.630754420116661.626\n    AustriaEurope197772.170756843019749.422\n    AustriaEurope198273.180757461321597.084\n    AustriaEurope198774.940757890323687.826\n    AustriaEurope199276.040791496927042.019\n    AustriaEurope199777.510806987629095.921\n    AustriaEurope200278.980814831232417.608\n    AustriaEurope200779.829819978336126.493\n\n\n\n\nfilter() takes logical expressions and returns the rows for which all are TRUE.\n\n# task: select rows with lifeExp less than 31\nfilter(gapminder, lifeExp < 31)\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    Afghanistan Asia  195228.8018425333 779.4453\n    Afghanistan Asia  195730.3329240934 820.8530\n    Angola      Africa195230.01542320953520.6103\n    Gambia      Africa195230.000 284320 485.2307\n    Rwanda      Africa199223.5997290203 737.0686\n    Sierra LeoneAfrica195230.3312143249 879.7877\n\n\n\n\n\n# task: select Austria only and year after 1980\nfilter(gapminder, country == \"Austria\", year > 1980)\n\n\n\nA tibble: 6 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AustriaEurope198273.180757461321597.08\n    AustriaEurope198774.940757890323687.83\n    AustriaEurope199276.040791496927042.02\n    AustriaEurope199777.510806987629095.92\n    AustriaEurope200278.980814831232417.61\n    AustriaEurope200779.829819978336126.49\n\n\n\n\n\n# task: select Austria and Belgium\nfilter(gapminder, country %in% c(\"Austria\", \"Belgium\"))\n\n\n\nA tibble: 24 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AustriaEurope195266.800 6927772 6137.076\n    AustriaEurope195767.480 6965860 8842.598\n    AustriaEurope196269.540 712986410750.721\n    AustriaEurope196770.140 737699812834.602\n    AustriaEurope197270.630 754420116661.626\n    AustriaEurope197772.170 756843019749.422\n    AustriaEurope198273.180 757461321597.084\n    AustriaEurope198774.940 757890323687.826\n    AustriaEurope199276.040 791496927042.019\n    AustriaEurope199777.510 806987629095.921\n    AustriaEurope200278.980 814831232417.608\n    AustriaEurope200779.829 819978336126.493\n    BelgiumEurope195268.000 8730405 8343.105\n    BelgiumEurope195769.240 8989111 9714.961\n    BelgiumEurope196270.250 921840010991.207\n    BelgiumEurope196770.940 955650013149.041\n    BelgiumEurope197271.440 970910016672.144\n    BelgiumEurope197772.800 982180019117.974\n    BelgiumEurope198273.930 985630320979.846\n    BelgiumEurope198775.350 987020022525.563\n    BelgiumEurope199276.4601004562225575.571\n    BelgiumEurope199777.5301019978727561.197\n    BelgiumEurope200278.3201031197030485.884\n    BelgiumEurope200779.4411039222633692.605\n\n\n\n\nLets rewrite initial code and record it to the variable/data.frame:"
  },
  {
    "objectID": "31-r-data-explore.html#pipe-operator",
    "href": "31-r-data-explore.html#pipe-operator",
    "title": "11  Exploring data with dplyr",
    "section": "11.3 Pipe (%>%/|>) operator",
    "text": "11.3 Pipe (%>%/|>) operator\n%>% is pipe operator. The pipe operator takes the thing on the left-hand-side and pipes it into the function call on the right-hand-side – literally, drops it in as the first argument.\nhead() function without pipe and top 4 items:\n\nIn R version before 4.1.0 pipe %>% operator is not a language build-in and you should install magrittr package:\n\n\nPipe opertor in R 4.1+ |>, using this is preferable\n\n\n#install.packages(\"magrittr\") # for pipe %>% operator\nlibrary(magrittr)\n\n\nhead(gapminder, n = 4)\n\n\n\nA tibble: 4 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453\n    AfghanistanAsia195730.332 9240934820.8530\n    AfghanistanAsia196231.99710267083853.1007\n    AfghanistanAsia196734.02011537966836.1971\n\n\n\n\nhead() function with pipe and top 4 items:\n\ngapminder %>% head(4)\n\n\n\nA tibble: 4 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AfghanistanAsia195228.801 8425333779.4453\n    AfghanistanAsia195730.332 9240934820.8530\n    AfghanistanAsia196231.99710267083853.1007\n    AfghanistanAsia196734.02011537966836.1971\n\n\n\n\nOutput is the same. So, let’s rewrire filtering for Austria with pipe:\n\naustria <- gapminder |> filter(country == \"Austria\")\naustria\n\n\n\nA tibble: 12 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AustriaEurope195266.8006927772 6137.076\n    AustriaEurope195767.4806965860 8842.598\n    AustriaEurope196269.540712986410750.721\n    AustriaEurope196770.140737699812834.602\n    AustriaEurope197270.630754420116661.626\n    AustriaEurope197772.170756843019749.422\n    AustriaEurope198273.180757461321597.084\n    AustriaEurope198774.940757890323687.826\n    AustriaEurope199276.040791496927042.019\n    AustriaEurope199777.510806987629095.921\n    AustriaEurope200278.980814831232417.608\n    AustriaEurope200779.829819978336126.493\n\n\n\n\n\n# add more conditions in filter\naustria <- gapminder |> filter(country == \"Austria\", year > 2000)\naustria\n\n\n\nA tibble: 2 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    AustriaEurope200278.980814831232417.61\n    AustriaEurope200779.829819978336126.49"
  },
  {
    "objectID": "31-r-data-explore.html#select-function",
    "href": "31-r-data-explore.html#select-function",
    "title": "11  Exploring data with dplyr",
    "section": "11.4 select() function",
    "text": "11.4 select() function\nUse select() to subset the data on variables/columns by names or index. You also can define order of columns with select().\n\ngapminder |> \nselect(year, country, pop) |>\nslice(1: 10)\n\n\n\nA tibble: 10 × 3\n\n    yearcountrypop\n    <int><fct><int>\n\n\n    1952Afghanistan 8425333\n    1957Afghanistan 9240934\n    1962Afghanistan10267083\n    1967Afghanistan11537966\n    1972Afghanistan13079460\n    1977Afghanistan14880372\n    1982Afghanistan12881816\n    1987Afghanistan13867957\n    1992Afghanistan16317921\n    1997Afghanistan22227415\n\n\n\n\nLets combine few functions with pipe (%>%):\nFinally, lest extend our filtering:\n\n# compare dplyr syntax with base R call\ngapminder[gapminder$country == \"Austria\", c(\"year\", \"pop\", \"lifeExp\")]\n\ngapminder |> \n    filter(country == \"Austria\") |>\n    select(year, pop, lifeExp)\n\n\n\nA tibble: 12 × 3\n\n    yearpoplifeExp\n    <int><int><dbl>\n\n\n    1952692777266.800\n    1957696586067.480\n    1962712986469.540\n    1967737699870.140\n    1972754420170.630\n    1977756843072.170\n    1982757461373.180\n    1987757890374.940\n    1992791496976.040\n    1997806987677.510\n    2002814831278.980\n    2007819978379.829\n\n\n\n\n\n\nA tibble: 12 × 3\n\n    yearpoplifeExp\n    <int><int><dbl>\n\n\n    1952692777266.800\n    1957696586067.480\n    1962712986469.540\n    1967737699870.140\n    1972754420170.630\n    1977756843072.170\n    1982757461373.180\n    1987757890374.940\n    1992791496976.040\n    1997806987677.510\n    2002814831278.980\n    2007819978379.829\n\n\n\n\nYou can remove some columns using minus(operator) and add few filter conditions:\n\naustria <- gapminder |> \n                filter(country == \"Austria\", year > 2000) |>\n                select(-continent, -gdpPercap) |>\n                head()\naustria\n\n\n\nA tibble: 2 × 4\n\n    countryyearlifeExppop\n    <fct><int><dbl><int>\n\n\n    Austria200278.9808148312\n    Austria200779.8298199783\n\n\n\n\nYou can insert different conditions about columns you need to select.\n\ngapminder |>\n    select(!where(is.numeric)) |>  # its 1704 records, because of repeating some records\n    slice(1:5)\n\n\n\nA tibble: 5 × 2\n\n    countrycontinent\n    <fct><fct>\n\n\n    AfghanistanAsia\n    AfghanistanAsia\n    AfghanistanAsia\n    AfghanistanAsia\n    AfghanistanAsia\n\n\n\n\nLet’s output all unique pairs continent -> country with distinct() function:\n\ngapminder |>\n    select(country) |>\n    distinct() # its 142 records now\n\n\n\nA tibble: 142 × 1\n\n    country\n    <fct>\n\n\n    Afghanistan             \n    Albania                 \n    Algeria                 \n    Angola                  \n    Argentina               \n    Australia               \n    Austria                 \n    Bahrain                 \n    Bangladesh              \n    Belgium                 \n    Benin                   \n    Bolivia                 \n    Bosnia and Herzegovina  \n    Botswana                \n    Brazil                  \n    Bulgaria                \n    Burkina Faso            \n    Burundi                 \n    Cambodia                \n    Cameroon                \n    Canada                  \n    Central African Republic\n    Chad                    \n    Chile                   \n    China                   \n    Colombia                \n    Comoros                 \n    Congo, Dem. Rep.        \n    Congo, Rep.             \n    Costa Rica              \n    ⋮\n    Sierra Leone       \n    Singapore          \n    Slovak Republic    \n    Slovenia           \n    Somalia            \n    South Africa       \n    Spain              \n    Sri Lanka          \n    Sudan              \n    Swaziland          \n    Sweden             \n    Switzerland        \n    Syria              \n    Taiwan             \n    Tanzania           \n    Thailand           \n    Togo               \n    Trinidad and Tobago\n    Tunisia            \n    Turkey             \n    Uganda             \n    United Kingdom     \n    United States      \n    Uruguay            \n    Venezuela          \n    Vietnam            \n    West Bank and Gaza \n    Yemen, Rep.        \n    Zambia             \n    Zimbabwe"
  },
  {
    "objectID": "31-r-data-explore.html#selecting-random-n-rows",
    "href": "31-r-data-explore.html#selecting-random-n-rows",
    "title": "11  Exploring data with dplyr",
    "section": "11.5 Selecting random \\(N\\) rows",
    "text": "11.5 Selecting random \\(N\\) rows\nThe sample_n() function selects random rows from a data frame\n\ngapminder |> sample_n(5)\n\n\n\nA tibble: 5 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    Norway                  Europe  196774.080378601916361.8765\n    Central African RepublicAfrica  200243.3084048013  738.6906\n    Uruguay                 Americas200776.384344749610611.4630\n    Togo                    Africa  199758.3904320890  982.2869\n    Paraguay                Americas200270.7555884491 3783.6742\n\n\n\n\nIf you want make pseudo-random generation reprodusable use set.seed(). Seed is start point of random generation. Different seeds give different output.\n\nset.seed(2023) # example, seed = 2023\n\nThe sample_frac() function selects random fraction rows from a data frame. Let’s select \\(1\\%\\) of data\n\nset.seed(2023) # output not changing, uncomment it \ngapminder %>% sample_frac(0.1)\n\n\n\nA tibble: 170 × 6\n\n    countrycontinentyearlifeExppopgdpPercap\n    <fct><fct><int><dbl><int><dbl>\n\n\n    Switzerland          Europe  200781.701 7554661 37506.4191\n    Djibouti             Africa  200253.373  447416  1908.2609\n    Slovenia             Europe  197269.820 1694510 12383.4862\n    Sao Tome and PrincipeAfrica  199763.306  145608  1339.0760\n    Turkey               Europe  198763.10852881328  5089.0437\n    Lebanon              Asia    195759.489 1647412  6089.7869\n    Eritrea              Africa  197244.142 2260187   514.3242\n    Philippines          Asia    197258.06540850141  1989.3741\n    Tunisia              Africa  197255.602 5303507  2753.2860\n    Uganda               Africa  195239.978 5824797   734.7535\n    Oman                 Asia    197252.143  829050 10618.0385\n    Australia            Oceania 200781.23520434176 34435.3674\n    Mali                 Africa  200251.81810580176   951.4098\n    Equatorial Guinea    Africa  195735.983  232922   426.0964\n    South Africa         Africa  198258.16131140029  8568.2662\n    Burundi              Africa  196242.045 2961915   355.2032\n    Angola               Africa  199240.647 8735988  2627.8457\n    Yemen, Rep.          Asia    195232.548 4963829   781.7176\n    Croatia              Europe  200775.748 4493312 14619.2227\n    Oman                 Asia    199271.197 1915208 18616.7069\n    Thailand             Asia    196256.06129263397  1002.1992\n    Comoros              Africa  195240.715  153936  1102.9909\n    Eritrea              Africa  195738.047 1542611   344.1619\n    Zambia               Africa  200239.19310595811  1071.6139\n    Cote d'Ivoire        Africa  198754.65510761098  2156.9561\n    South Africa         Africa  195747.98516151549  5487.1042\n    Paraguay             Americas195763.196 1770902  2046.1547\n    Kuwait               Asia    195255.565  160000108382.3529\n    Brazil               Americas195250.91756602560  2108.9444\n    Canada               Americas195769.96017010154 12489.9501\n    ⋮⋮⋮⋮⋮⋮\n    Swaziland            Africa  199754.289 1054486 3876.7685\n    Myanmar              Asia    200259.90845598081  611.0000\n    Sao Tome and PrincipeAfrica  198761.728  110812 1516.5255\n    Ghana                Africa  197751.75610538093  993.2240\n    Guinea-Bissau        Africa  199744.873 1193708  796.6645\n    Guinea               Africa  199248.576 6990574  794.3484\n    Haiti                Americas195740.696 3507701 1726.8879\n    Sao Tome and PrincipeAfrica  200765.528  199579 1598.4351\n    Comoros              Africa  199760.660  527982 1173.6182\n    Equatorial Guinea    Africa  197240.516  277603  672.4123\n    Oman                 Asia    198262.728 130104812954.7910\n    Namibia              Africa  197756.437  977026 3876.4860\n    Congo, Dem. Rep.     Africa  195239.14314100005  780.5423\n    Hong Kong, China     Asia    197773.600 458370011186.1413\n    Bolivia              Americas199762.050 7693188 3326.1432\n    Panama               Americas200274.712 2990875 7356.0319\n    Nigeria              Africa  195236.32433119096 1077.2819\n    Malaysia             Asia    200774.2412482128612451.6558\n    Japan                Asia    195263.03086459025 3216.9563\n    Albania              Europe  196766.220 1984060 2760.1969\n    Portugal             Europe  199775.9701015641517641.0316\n    Uruguay              Americas195266.071 2252965 5716.7667\n    Afghanistan          Asia    197236.08813079460  739.9811\n    Syria                Asia    198766.97411242847 3116.7743\n    Libya                Africa  200272.737 5368585 9534.6775\n    Mauritania           Africa  196244.248 1146757 1055.8960\n    Trinidad and Tobago  Americas199269.862 1183669 7370.9909\n    Netherlands          Europe  196273.2301180568912790.8496\n    Reunion              Africa  200776.442  798094 7670.1226\n    Honduras             Americas195744.665 1770390 2220.4877"
  },
  {
    "objectID": "31-r-data-explore.html#refences",
    "href": "31-r-data-explore.html#refences",
    "title": "11  Exploring data with dplyr",
    "section": "11.6 Refences",
    "text": "11.6 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "32-r-data-slice.html",
    "href": "32-r-data-slice.html",
    "title": "12  Subset rows with slice()",
    "section": "",
    "text": "author: Юрій Клебан\nBefore start load packages\nDescription\nIf .data is a grouped_df, the operation will be performed on each group, so that (e.g.) slice_head(df, n = 5) will select the first five rows in each group.\nSamples\nYou can drop some recods with negative indexes:"
  },
  {
    "objectID": "32-r-data-slice.html#refences",
    "href": "32-r-data-slice.html#refences",
    "title": "12  Subset rows with slice()",
    "section": "12.1 Refences",
    "text": "12.1 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "33-r-data-sorting.html",
    "href": "33-r-data-sorting.html",
    "title": "13  Sorting with arrange()",
    "section": "",
    "text": "author: Юрій Клебан\nBefore start load packages\narrange(.data, …) function order rows by values of a column or columns (low to high)You can use with desc() to order from high to low.\nFor example, we need to select top 10 countries in 2002 by lifeExp variable."
  },
  {
    "objectID": "33-r-data-sorting.html#refences",
    "href": "33-r-data-sorting.html#refences",
    "title": "13  Sorting with arrange()",
    "section": "13.1 Refences",
    "text": "13.1 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "34-r-data-mutating.html",
    "href": "34-r-data-mutating.html",
    "title": "14  Create new variables with mutate()",
    "section": "",
    "text": "author: Юрій Клебан\nBefore start load packages\nmutate(.data, …) compute new column(s). Lets compute new column for gapminder\n\\(gdpTotal = gdpPercap * pop / 1000000\\).\ntransmute(.data, …) compute new column(s), drop others.\nYou can mutate many columns at once:\nYou also can edit existing column (let’s change continent Europe to EU in dataframe):"
  },
  {
    "objectID": "34-r-data-mutating.html#refences",
    "href": "34-r-data-mutating.html#refences",
    "title": "14  Create new variables with mutate()",
    "section": "14.1 Refences",
    "text": "14.1 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "35-r-data-rename.html",
    "href": "35-r-data-rename.html",
    "title": "15  Renaming columns with rename()",
    "section": "",
    "text": "author: Юрій Клебан\nBefore start load packages\nrename(.data, …) rename columns. Let’s rename column pop to poulation:\nAlso check functions rename_if and rename_at."
  },
  {
    "objectID": "35-r-data-rename.html#refences",
    "href": "35-r-data-rename.html#refences",
    "title": "15  Renaming columns with rename()",
    "section": "15.1 Refences",
    "text": "15.1 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "36-r-data-grouping.html",
    "href": "36-r-data-grouping.html",
    "title": "16  Grouping columns with dplyr",
    "section": "",
    "text": "author: Юрій Клебан\nBefore start load packages"
  },
  {
    "objectID": "36-r-data-grouping.html#group_by-summarise",
    "href": "36-r-data-grouping.html#group_by-summarise",
    "title": "16  Grouping columns with dplyr",
    "section": "16.1 group_by() + summarise()",
    "text": "16.1 group_by() + summarise()\ngroup_by(.data, ..., add = FALSE) returns copy of table grouped by defined columns.\nLet’s find average by lifeExp for each continent in 2002 (ouput is continent, lifeExpAvg2002, countriesCount, year = 2002):\n\ngapminder |>\n    filter(year == 2002) |> # year\n    group_by(continent) |> # grouping condition, you ca\n    summarise(\n        lifeExpAvg2002 = mean(lifeExp),\n        countriesCount = n() # n() count of rows in group  \n        ) \n\n\n\nA tibble: 5 × 3\n\n    continentlifeExpAvg2002countriesCount\n    <fct><dbl><int>\n\n\n    Africa  53.3252352\n    Americas72.4220425\n    Asia    69.2338833\n    Europe  76.7006030\n    Oceania 79.74000 2\n\n\n\n\nLet’s find total population for each continent in 2002 (ouput is continent, totalPop, year):\n\ngapminder |>\n    filter(year == 2002) |> # year\n    group_by(continent, year) |> # grouping condition\n    summarise(totalPop = sum(pop), .groups = \"keep\") \n\n\n\nA grouped_df: 5 × 3\n\n    continentyeartotalPop\n    <fct><int><dbl>\n\n\n    Africa  2002 833723916\n    Americas2002 849772762\n    Asia    20023601802203\n    Europe  2002 578223869\n    Oceania 2002  23454829\n\n\n\n\nThere are additional variations of summarise():\n\nsummarise_all() - Apply funs to every column.\nsummarise_at() - Apply funs to specific columns.\n\nsummarise_if() - Apply funs to all cols of one type.\n\n\n\n16.1.1 Task on Credits (rewrite it)\n\nlibrary(ISLR)\n\ngroup_inc <- aggregate(Income ~ Age + Gender, data = Credit, mean)\n\nm_data <- group_inc[group_inc$Gender == \" Male\", ]\nnrow(m_data)\n\nf_data <- group_inc[group_inc$Gender == \"Female\", ]\nnrow(f_data)\nwith(m_data, plot(Age, Income, type = \"l\", col=\"red\"))\nwith(f_data, lines(Age, Income, type = \"l\", col =\"blue\"))\n\n63\n\n\n62\n\n\n\n\n\n\ncd <- Credit %>%\nselect(Income, Age, Gender) %>%\ngroup_by(Age, Gender) %>%\nsummarize(Income = mean(Income))\n\nm_data <- cd %>% filter(Gender == \" Male\")\nnrow(m_data)\n\nf_data <- cd %>% filter(Gender == \"Female\")\nnrow(f_data)\n\nwith(m_data, plot(Age, Income, type = \"l\", col=\"red\"))\nwith(f_data, lines(Age, Income, type = \"l\", col =\"blue\"))\n\n`summarise()` has grouped output by 'Age'. You can override using the `.groups`\nargument.\n\n\n63\n\n\n62"
  },
  {
    "objectID": "36-r-data-grouping.html#refences",
    "href": "36-r-data-grouping.html#refences",
    "title": "16  Grouping columns with dplyr",
    "section": "16.2 Refences",
    "text": "16.2 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "37-r-data-bind.html",
    "href": "37-r-data-bind.html",
    "title": "17  Binding rows and columns",
    "section": "",
    "text": "author: Юрій Клебан\nBefore start load packages"
  },
  {
    "objectID": "37-r-data-bind.html#bind_rows",
    "href": "37-r-data-bind.html#bind_rows",
    "title": "17  Binding rows and columns",
    "section": "17.1 bind_rows",
    "text": "17.1 bind_rows\nbind_rows(.data, …) helps to unite two dataframes with the same columns order and names.\nSo, if we need add one data frame to an other vertically (bind rows) we shoul use bind_rows:\n\nd2002 <- gapminder %>%\n            filter(year == 2002) %>% # year\n            group_by(continent, year) %>% # grouping condition\n            summarise(\n                lifeExpAvg = mean(lifeExp),\n                countriesCount = n(), # n() count of rows in group \n                .groups = 'drop'\n            )\nhead(d2002)\n\n\n\nA tibble: 5 × 4\n\n    continentyearlifeExpAvgcountriesCount\n    <fct><int><dbl><int>\n\n\n    Africa  200253.3252352\n    Americas200272.4220425\n    Asia    200269.2338833\n    Europe  200276.7006030\n    Oceania 200279.74000 2\n\n\n\n\n\nd2007 <- gapminder %>%\n            filter(year == 2007) %>% # year\n            group_by(continent, year) %>% # grouping condition\n            summarise(\n                lifeExpAvg = mean(lifeExp),\n                countriesCount = n() # n() count of rows in group                \n            )\nhead(d2007)\n\n`summarise()` has grouped output by 'continent'. You can override using the `.groups` argument.\n\n\n\n\nA grouped_df: 5 × 4\n\n    continentyearlifeExpAvgcountriesCount\n    <fct><int><dbl><int>\n\n\n    Africa  200754.8060452\n    Americas200773.6081225\n    Asia    200770.7284833\n    Europe  200777.6486030\n    Oceania 200780.71950 2\n\n\n\n\nUnite them:\n\nd2002 %>% bind_rows(d2007) ## bind rows\n\n\n\nA tibble: 10 × 4\n\n    continentyearlifeExpAvgcountriesCount\n    <fct><int><dbl><int>\n\n\n    Africa  200253.3252352\n    Americas200272.4220425\n    Asia    200269.2338833\n    Europe  200276.7006030\n    Oceania 200279.74000 2\n    Africa  200754.8060452\n    Americas200773.6081225\n    Asia    200770.7284833\n    Europe  200777.6486030\n    Oceania 200780.71950 2"
  },
  {
    "objectID": "37-r-data-bind.html#bind_cols",
    "href": "37-r-data-bind.html#bind_cols",
    "title": "17  Binding rows and columns",
    "section": "17.2 bind_cols",
    "text": "17.2 bind_cols\nbind_cols(.data, …) helps to unite two dataframes with the same rows count.\n\ngrouped_data2002pop <- gapminder %>%\n    filter(year == 2002) %>% # year\n    group_by(continent) %>% # grouping condition\n    summarise(totalPop = sum(pop)) %>%\n    mutate(year = 2002)\ngrouped_data2002pop\n\n\n\nA tibble: 5 × 3\n\n    continenttotalPopyear\n    <fct><dbl><dbl>\n\n\n    Africa   8337239162002\n    Americas 8497727622002\n    Asia    36018022032002\n    Europe   5782238692002\n    Oceania   234548292002\n\n\n\n\nLet’s combine d2002 and grouped_data2002pop:\n\ngrouped_data <- d2002 %>% \n    bind_cols(grouped_data2002pop)\ngrouped_data\n\n# columns with the same name were renamed!\n\nNew names:\n* `continent` -> `continent...1`\n* `year` -> `year...2`\n* `continent` -> `continent...5`\n* `year` -> `year...7`\n\n\n\n\nA tibble: 5 × 7\n\n    continent...1year...2lifeExpAvgcountriesCountcontinent...5totalPopyear...7\n    <fct><int><dbl><int><fct><dbl><dbl>\n\n\n    Africa  200253.3252352Africa   8337239162002\n    Americas200272.4220425Americas 8497727622002\n    Asia    200269.2338833Asia    36018022032002\n    Europe  200276.7006030Europe   5782238692002\n    Oceania 200279.74000 2Oceania   234548292002\n\n\n\n\nYou can remove same named variables before binding:\n\ngrouped_data <- d2002 %>% \n    bind_cols(grouped_data2002pop %>%\n              select(-continent, -year))\ngrouped_data\n\n# better, but continents order is not the same in both frames \n# your data is going to be damaged\n\n\n\nA tibble: 5 × 5\n\n    continentyearlifeExpAvgcountriesCounttotalPop\n    <fct><int><dbl><int><dbl>\n\n\n    Africa  200253.3252352 833723916\n    Americas200272.4220425 849772762\n    Asia    200269.23388333601802203\n    Europe  200276.7006030 578223869\n    Oceania 200279.74000 2  23454829\n\n\n\n\n\ngrouped_data2002pop <- grouped_data2002pop %>% \n    arrange(totalPop)\n\ngrouped_data <- d2002 %>% \n    bind_cols(grouped_data2002pop)\ngrouped_data\n\n# you can see that continent fields different in the same row\n\nNew names:\n* `continent` -> `continent...1`\n* `year` -> `year...2`\n* `continent` -> `continent...5`\n* `year` -> `year...7`\n\n\n\n\nA tibble: 5 × 7\n\n    continent...1year...2lifeExpAvgcountriesCountcontinent...5totalPopyear...7\n    <fct><int><dbl><int><fct><dbl><dbl>\n\n\n    Africa  200253.3252352Oceania   234548292002\n    Americas200272.4220425Europe   5782238692002\n    Asia    200269.2338833Africa   8337239162002\n    Europe  200276.7006030Americas 8497727622002\n    Oceania 200279.74000 2Asia    36018022032002\n\n\n\n\nHow to solve this? Join functions issolution."
  },
  {
    "objectID": "37-r-data-bind.html#refences",
    "href": "37-r-data-bind.html#refences",
    "title": "17  Binding rows and columns",
    "section": "17.3 Refences",
    "text": "17.3 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "38-r-data-join.html",
    "href": "38-r-data-join.html",
    "title": "18  Join()-ing data",
    "section": "",
    "text": "author: Юрій Клебан\nBefore start load packages"
  },
  {
    "objectID": "38-r-data-join.html#join-types",
    "href": "38-r-data-join.html#join-types",
    "title": "18  Join()-ing data",
    "section": "18.1 Join types",
    "text": "18.1 Join types\nLets check join operations as set opretations\n\nJoins on table are look like this:\n\n\n\n\n\nSource: https://marcus116.blogspot.com/2019/07/cheatsheets-sql-join-cheat-sheets.html"
  },
  {
    "objectID": "38-r-data-join.html#join-functions",
    "href": "38-r-data-join.html#join-functions",
    "title": "18  Join()-ing data",
    "section": "18.2 Join functions",
    "text": "18.2 Join functions\nTo solve previous problem you can use set of join()-functions. left_join() can solve our previous example:\n\nd2002 <- gapminder %>%\n            filter(year == 2002) %>% # year\n            group_by(continent, year) %>% # grouping condition\n            summarise(\n                lifeExpAvg = mean(lifeExp),\n                countriesCount = n(), # n() count of rows in group \n                .groups = 'drop'\n            )\nd2002 |> head()\n\n\n\nA tibble: 5 × 4\n\n    continentyearlifeExpAvgcountriesCount\n    <fct><int><dbl><int>\n\n\n    Africa  200253.3252352\n    Americas200272.4220425\n    Asia    200269.2338833\n    Europe  200276.7006030\n    Oceania 200279.74000 2\n\n\n\n\n\ngrouped_data2002pop <- gapminder %>%\n    filter(year == 2002) %>% # year\n    group_by(continent) %>% # grouping condition\n    summarise(totalPop = sum(pop),\n             year = min(year))\n\ngrouped_data2002pop |> head()\n\n\n\nA tibble: 5 × 3\n\n    continenttotalPopyear\n    <fct><dbl><int>\n\n\n    Africa   8337239162002\n    Americas 8497727622002\n    Asia    36018022032002\n    Europe   5782238692002\n    Oceania   234548292002\n\n\n\n\n\ngrouped_data2002pop <- grouped_data2002pop %>% \n    arrange(totalPop)\n\ngrouped_data <- d2002 %>% \n    left_join(grouped_data2002pop, by = \"continent\")\ngrouped_data\n\n# but we have duplicated year\n\n\n\nA tibble: 5 × 6\n\n    continentyear.xlifeExpAvgcountriesCounttotalPopyear.y\n    <fct><int><dbl><int><dbl><int>\n\n\n    Africa  200253.3252352 8337239162002\n    Americas200272.4220425 8497727622002\n    Asia    200269.233883336018022032002\n    Europe  200276.7006030 5782238692002\n    Oceania 200279.74000 2  234548292002\n\n\n\n\n\ngrouped_data2002pop <- grouped_data2002pop %>% \n    arrange(totalPop)\n\ngrouped_data <- d2002 %>% \n    left_join(grouped_data2002pop, by = c(\"continent\", \"year\"))\ngrouped_data\n\n#ok\n\n\n\nA tibble: 5 × 5\n\n    continentyearlifeExpAvgcountriesCounttotalPop\n    <fct><int><dbl><int><dbl>\n\n\n    Africa  200253.3252352 833723916\n    Americas200272.4220425 849772762\n    Asia    200269.23388333601802203\n    Europe  200276.7006030 578223869\n    Oceania 200279.74000 2  23454829\n\n\n\n\nLet’s make a different data sets for testing join() fucntions:\n\nfirst_df <- data.frame(Letter = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                      Value = c(1:5))\n\nsecond_df <- data.frame(Letter = c(\"A\", \"B\", \"C\", \"D\", \"F\"),\n                      Value = c(12, 7, 4, 1, 5))\nfirst_df\nsecond_df \n\n\n\nA data.frame: 5 × 2\n\n    LetterValue\n    <chr><int>\n\n\n    A1\n    B2\n    C3\n    D4\n    E5\n\n\n\n\n\n\nA data.frame: 5 × 2\n\n    LetterValue\n    <chr><dbl>\n\n\n    A12\n    B 7\n    C 4\n    D 1\n    F 5\n\n\n\n\nYou can see that the last row Letter is different in dataframes. left_join() test is next.\n\nfirst_df %>% \n    left_join(second_df, by = \"Letter\")\n# there is no F letter, becouse first_db joined only known first_df Letters.\n\n\n\nA data.frame: 5 × 3\n\n    LetterValue.xValue.y\n    <chr><int><dbl>\n\n\n    A112\n    B2 7\n    C3 4\n    D4 1\n    E5NA\n\n\n\n\n\nfirst_df %>% \n    right_join(second_df, by = \"Letter\")\n# right_join! there is no E letter, becouse first_db joined only known second_df Letters.\n\n\n\nA data.frame: 5 × 3\n\n    LetterValue.xValue.y\n    <chr><int><dbl>\n\n\n    A 112\n    B 2 7\n    C 3 4\n    D 4 1\n    FNA 5\n\n\n\n\n\nfirst_df %>% \n    inner_join(second_df, by = \"Letter\")\n# inner_join! there is no E and F Letters, \n# only known both first_df and second_df are left here.\n\n\n\nA data.frame: 4 × 3\n\n    LetterValue.xValue.y\n    <chr><int><dbl>\n\n\n    A112\n    B2 7\n    C3 4\n    D4 1\n\n\n\n\n\nfirst_df %>% \n    full_join(second_df, by = \"Letter\")\n# all are here, but unknown values replaced by NA, it's ok.\n\n\n\nA data.frame: 6 × 3\n\n    LetterValue.xValue.y\n    <chr><int><dbl>\n\n\n    A 112\n    B 2 7\n    C 3 4\n    D 4 1\n    E 5NA\n    FNA 5\n\n\n\n\nShort description of reviewed functions:\n\n\n\n\n\n\n\n\n\nFunction\nObjectives\nArguments\nMultiple keys\n\n\n\n\nleft_join()\nMerge two datasets. Keep all observations from the origin table\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)\n\n\nright_join()\nMerge two datasets. Keep all observations from the destination table\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)\n\n\ninner_join()\nMerge two datasets. Excludes all unmatched rows\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)\n\n\nfull_join()\nMerge two datasets. Keeps all observations\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)"
  },
  {
    "objectID": "38-r-data-join.html#refences",
    "href": "38-r-data-join.html#refences",
    "title": "18  Join()-ing data",
    "section": "18.3 Refences",
    "text": "18.3 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "39-r-wide-to-long.html",
    "href": "39-r-wide-to-long.html",
    "title": "19  Wide-to-long tables",
    "section": "",
    "text": "author: Юрій Клебан\nBefore start load packages\nSome times your data is not in tidy format. Peole can collect data year by year in each column. It’s problem to use such data for feature engeniering and building prediction models. Let’s generate such data sample (quaterly salary of some people).\nTo make our data tidier separate() can split quater column into 2 (quater and year):\nThe unite() function concanates two columns into one:\nIf you need to make table like initial use spread() function:\nLet’s try to spread() feild pop of gapminder by year:\nFunctions table:"
  },
  {
    "objectID": "39-r-wide-to-long.html#refences",
    "href": "39-r-wide-to-long.html#refences",
    "title": "19  Wide-to-long tables",
    "section": "19.1 Refences",
    "text": "19.1 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle"
  },
  {
    "objectID": "40-r-data-quality.html",
    "href": "40-r-data-quality.html",
    "title": "20  Оцінки якості даних",
    "section": "",
    "text": "author: Юрій Клебан\nМатеріали розділу описують інформацію про виміри оцінки якості даних, підходи до визначення та обробки пропущених значень, а також розглядаються способи боротьби зі статистичними викидами."
  },
  {
    "objectID": "40-r-data-quality.html#що-таке-валідація-даних",
    "href": "40-r-data-quality.html#що-таке-валідація-даних",
    "title": "20  Оцінки якості даних",
    "section": "20.1 Що таке валідація даних?",
    "text": "20.1 Що таке валідація даних?\nВалідація даних відноситься до процесу забезпечення точності та якості даних. Він реалізується шляхом вбудовування кількох перевірок у систему або звітування для забезпечення логічної узгодженості введених і збережених даних.\n\nЯкість даних залежить від очищення та коригування даних, які відсутні, некоректні, недійсні або нечитабельні. Для забезпечення достовірності даних важливо зрозуміти ключові аспекти якості даних, щоб оцінити, наскільки дані погані/хороші.\nНа перший погляд, очевидно, що перетворення даних до якісних полягає в очищенні поганих даних – даних, які відсутні, неправильні або якимось чином недійсні. Але щоб переконатися, що дані заслуговують довіри, важливо розуміти ключові виміри якості даних, щоб оцінити, наскільки дані є «поганими».\nОкремі компанії мають внутрішні документи, що визначають виміри оцінки якості даних та порядок його проведення - Data Validation Framework або Data Quality Framework.\nКоли говорять про якість даних, то мається на увазі їх оцінка у кількох вимірах. Розглянемо коротко ці виміри:\n\nПравильність / Accuracy\nПовнота / Completeness\nУзгодженість / Consistency\nВідповідність / Conformity\nЦілісність / Integrity\nСвоєчасність / Timeliness\nУнікальність / Uniqueness"
  },
  {
    "objectID": "40-r-data-quality.html#правильність-accuracy",
    "href": "40-r-data-quality.html#правильність-accuracy",
    "title": "20  Оцінки якості даних",
    "section": "20.2 Правильність / (Accuracy)",
    "text": "20.2 Правильність / (Accuracy)\nПравильність — це ступінь, до якого дані правильно відображають реальний об’єкт АБО описувану подію.\nПриклади: - [x] Реальною вартістю є ціна продажу одиниці товару. - [x] Адреса співробітника в базі даних співробітників є справжньою адресою.\nЗапитання, які ви можете задати собі:\n\nЧи об’єкти даних точно представляють значення «реального світу», які вони повинні моделювати? Наприклад, чи правильно вказувати вік у сотнях тисяч років?\nЧи присутнє неправильне написання назв товарів чи осіб, адрес і навіть несвоєчасних чи неактуальних даних?\n\nЦі проблеми можуть вплинути на результатати аналітичних звітів, наприклад, неправильні середні значення певних показників."
  },
  {
    "objectID": "40-r-data-quality.html#повнота-completeness",
    "href": "40-r-data-quality.html#повнота-completeness",
    "title": "20  Оцінки якості даних",
    "section": "20.3 Повнота / (Completeness)",
    "text": "20.3 Повнота / (Completeness)\nПовнота визначається як очікувана всебічність. Дані можуть бути повними, навіть якщо додаткові дані відсутні. Поки дані відповідають очікуванням, вони вважаються повними.\nНаприклад, ім’я та прізвище замовника є обов’язковими, але прізвище необов’язково; тому запис можна вважати повним, навіть якщо прізвища не існує.\nПитання, які ви можете задати собі:\n\nЧи доступна вся необхідна інформація?\nЧи мають якісь дані відсутні елементи?\nАбо вони перебувають у непридатному для роботи вигляді?"
  },
  {
    "objectID": "40-r-data-quality.html#узгодженість-consistency",
    "href": "40-r-data-quality.html#узгодженість-consistency",
    "title": "20  Оцінки якості даних",
    "section": "20.4 Узгодженість / Consistency",
    "text": "20.4 Узгодженість / Consistency\nУзгодженість означає, що дані в усіх системах/таблицях відображають однакову інформацію та синхронізовані між собою.\nПриклади: - [x] Статус бізнес-підрозділу “закритий”, але є продажі для цього підрозділу. - [x] Статус працівника “звільнено”, але статус випалати заробіної плати містить суму відмінну від 0 за той самий період. - [x] Зафіксовано, що клієнт має у банку депозити, але у даних про депозити записи по клієнту відсутні.\nЗапитання, які ви можете поставити собі:\n\nЧи однакові значення даних у наборах даних?\nЧи існують якісь різні випадки, коли однакові екземпляри даних надають суперечливу інформацію?"
  },
  {
    "objectID": "40-r-data-quality.html#відповідність-conformity",
    "href": "40-r-data-quality.html#відповідність-conformity",
    "title": "20  Оцінки якості даних",
    "section": "20.5 Відповідність / Conformity",
    "text": "20.5 Відповідність / Conformity\nВідповідність означає, що дані відповідають набору стандартних визначень даних, як-от тип даних, розмір і формат. Наприклад, дата народження клієнта у форматі dd/mm/yyyy або відстань у км числом 100, а не записом 100км.\nЗапитання, які ви можете задати собі: - [x] Чи відповідають значення даних зазначеним форматам? - [x] Якщо так, то чи всі значення даних відповідають цим форматам?\nВажливо підтримувати відповідність конкретним форматам."
  },
  {
    "objectID": "40-r-data-quality.html#цілісність-integrity",
    "href": "40-r-data-quality.html#цілісність-integrity",
    "title": "20  Оцінки якості даних",
    "section": "20.6 Цілісність / Integrity",
    "text": "20.6 Цілісність / Integrity\nЦілісність означає достовірність даних у взаємозв’язках і гарантує, що всі дані в базі даних можна відстежити та з’єднати з іншими даними.\nНаприклад, у базі даних клієнтів має бути дійсний клієнт, адреси та відношення/зв’язки між ними. Якщо є дані про зв’язок адреси без клієнта, то ці дані недійсні й вважаються загубленим записом.\nЗапитайте себе: - [x] Чи є якісь дані без важливих зв’язків?\nНеможливість пов’язати записи разом може призвести до дублювання у ваших системах."
  },
  {
    "objectID": "40-r-data-quality.html#своєчасність-timeliness",
    "href": "40-r-data-quality.html#своєчасність-timeliness",
    "title": "20  Оцінки якості даних",
    "section": "20.7 Своєчасність / Timeliness",
    "text": "20.7 Своєчасність / Timeliness\nСвоєчасність показує, чи є інформація доступною, коли вона очікується та потрібна. Своєчасність даних дуже важлива.\nЦе відображається в: - [x] Компанії, які зобов’язані публікувати свої квартальні результати протягом певного періоду часу - [x] Обслуговування клієнтів надає клієнтам актуальну інформацію - [x] Кредитна система перевіряє активність рахунку кредитної картки в режимі реального часу\nСвоєчасність залежить від очікувань користувача. Доступність даних в Інтернеті може знадобитися для системи розподілу номерів у сфері готельного бізнесу.\nЯк бачите, якість даних є важливим питанням, яке слід враховувати, починаючи від етапу визначення цілей проекту, аж до впровадження, обслуговування та використання готово рішення у виробничі процесі підприємства."
  },
  {
    "objectID": "40-r-data-quality.html#набори-даних",
    "href": "40-r-data-quality.html#набори-даних",
    "title": "20  Оцінки якості даних",
    "section": "20.8 Набори даних",
    "text": "20.8 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv"
  },
  {
    "objectID": "40-r-data-quality.html#використані-та-додаткові-джерела",
    "href": "40-r-data-quality.html#використані-та-додаткові-джерела",
    "title": "20  Оцінки якості даних",
    "section": "20.9 Використані та додаткові джерела",
    "text": "20.9 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia"
  },
  {
    "objectID": "41-r-data-naming.html",
    "href": "41-r-data-naming.html",
    "title": "21  Робота з неіменованими та “поганоіменованими” даними",
    "section": "",
    "text": "author: Юрій Клебан"
  },
  {
    "objectID": "41-r-data-naming.html#іменування-даних",
    "href": "41-r-data-naming.html#іменування-даних",
    "title": "21  Робота з неіменованими та “поганоіменованими” даними",
    "section": "21.1 Іменування даних",
    "text": "21.1 Іменування даних\nПершим прикладом проблем у даних можна розгянути читання неіменованих даних, тобто стопці таблиці не мають заголовків у файлі.\nСтворимо такий файл у блокноті і зчитаємо його:\n\ndata <- read.csv(\"data/untitled.csv\")\ndata\n\n\n\nA data.frame: 5 × 4\n\n    X23X185X85.7Male\n    <int><chr><dbl><chr>\n\n\n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\nЗверніть увагу, що у якості стовпців взято перший рядок даних у додано X на початку. Зчитаємо дані із параметром, що вказує на відсутність заголовків:\n\ndata <- read.csv(\"data/untitled.csv\", header = FALSE)\ndata\n\n\n\nA data.frame: 6 × 4\n\n    V1V2V3V4\n    <int><chr><dbl><chr>\n\n\n    23185 85.7Male            \n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\nПроблема іменування не вирішена, дані ми уже не втратили. Передамо одночасно з читанням інформацію про назви стовпців:\n\ndata <- read.csv(\"data/untitled.csv\", \n            header = FALSE,\n            col.names = c(\"Age\",\"Height\", \"Weight\", \"Gender\"))\ndata\n\n\n\nA data.frame: 6 × 4\n\n    AgeHeightWeightGender\n    <int><chr><dbl><chr>\n\n\n    23185 85.7Male            \n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\nЩе одним варіантом задання назв стовпців є використання функції colnames() як для усіх різом, так і для окремого:\n\ncolnames(data) <- c(\"age\", \"height\", \"width\", \"gender\")\ndata\ncolnames(data)[2] <- \"HEIGHT\"\ndata\n\n\n\nA data.frame: 6 × 4\n\n    ageheightwidthgender\n    <int><chr><dbl><chr>\n\n\n    23185 85.7Male            \n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\n\n\nA data.frame: 6 × 4\n\n    ageHEIGHTwidthgender\n    <int><chr><dbl><chr>\n\n\n    23185 85.7Male            \n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F               \n\n\n\n\nТакож змінювати назви стовпців можна за допомогою функції rename() з пакету dplyr:\n\nlibrary(dplyr)\n\ndata <- data |> rename(AGE = age)\ndata\n\n\n\nA data.frame: 6 × 4\n\n    AGEHEIGHTwidthgender\n    <int><chr><dbl><chr>\n\n\n    23185 85.7Male            \n    41175 68.3M               \n    11142*55.4Female          \n    12NA  48.2Man             \n    54171   NALooks like a man\n    32168 78.0F"
  },
  {
    "objectID": "41-r-data-naming.html#заміна-назв-стовпців-data.frame",
    "href": "41-r-data-naming.html#заміна-назв-стовпців-data.frame",
    "title": "21  Робота з неіменованими та “поганоіменованими” даними",
    "section": "21.2 Заміна назв стовпців data.frame ",
    "text": "21.2 Заміна назв стовпців data.frame \nЗчитаємо файл, що містить інформацію про осіб, але уже має іменовані стовпці:\n\ndata <- read.csv(\"data/badtitled.csv\")\ndata\n\n\n\nA data.frame: 13 × 5\n\n    Person.AgePerson__Heightperson.WeightPerson.Genderempty\n    <int><chr><dbl><chr><lgl>\n\n\n    23185   NAMale  NA\n    41175 68.3   M  NA\n    11142*55.4FemaleNA\n    12NA  48.2Man   NA\n    54191   NAfemaleNA\n    32168 78.0F     NA\n    22NA  54.0male. NA\n    21165   NAm     NA\n    14NA  90.2Man   NA\n    51250   NAfemaleNA\n    4120  81.0F     NA\n    66NA  59.0male. NA\n    71171   NAm     NA\n\n\n\n\nШвидко змінити назви стовпців та привести їх до однакового стилю можна за домогою бібліотеки janitor:\n\n#install.packages(\"janitor\")\n\n\nlibrary(janitor)\nclean <- clean_names(data)\ncolnames(clean)\n\n\n'person_age''person_height''person_weight''person_gender''empty'"
  },
  {
    "objectID": "41-r-data-naming.html#набори-даних",
    "href": "41-r-data-naming.html#набори-даних",
    "title": "21  Робота з неіменованими та “поганоіменованими” даними",
    "section": "21.3 Набори даних",
    "text": "21.3 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv"
  },
  {
    "objectID": "41-r-data-naming.html#використані-та-додаткові-джерела",
    "href": "41-r-data-naming.html#використані-та-додаткові-джерела",
    "title": "21  Робота з неіменованими та “поганоіменованими” даними",
    "section": "21.4 Використані та додаткові джерела",
    "text": "21.4 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia"
  },
  {
    "objectID": "42-r-cleaning-text.html",
    "href": "42-r-cleaning-text.html",
    "title": "22  Підготовка та очистка текстової інформації",
    "section": "",
    "text": "author: Юрій Клебан\nЗчитаємо інформацію про стать з попереднього прикладу:\nСхоже, що ці дані насправді мають всього 2 записи, проте у базу даних їх вносили різні люди або вони були зібрані з різних джерел інформації. Це досить поширена проблема у роботі з даними. Особливо коли відбуваєть заміна людей на рочих місцях або перехід на інше програмне забезпечення.\nЯкщо це буде розглядатися як факторна змінна без будь-якої попередньої обробки, очевидно, що 8, а не 2 класи будуть збережені. Тому завдання полягає в тому, щоб автоматично розпізнавати наведені вище дані про те, чи відноситься кожен елемент до чоловічої чи жіночої статі. У статистичних контекстах класифікацію таких “безладні” текстові рядки в ряд фіксованих категорій часто називають кодуванням.\nОпишемо два взаємодоповнюючих підходи до кодування рядків: нормалізація (string normalization) рядків і аналіз схожості тексту (approximate text matching).\nРозглянемо наступні підходи до очистки текстових даних:\nРобота з текстом у R здійснюється за допомогою пакету stringr.\nВидалення пробілів на початку або в кінці здійснюється за допомогою функції str_trim().\nОбрізання/збільшення рядків до певної ширини здійснюється за допомогою функції str_pad().\nПеретворення у верхній/нижній регістр\nПошук рядків, що містять прості шаблони (підрядки)\nСкористаємося функцієя grep() та grepl() для пошуку підрядків у інформації про стать:\nПошук “відстані” між ряжками - це аналіз рядків на схожіть з визначенням рівня співпадінь.\nДавайте проаналізуємо інформацію про стать з точки зору схожості текстів:\nПримінимо інформацію про відстані до “нечистих” даних про стать:\nЯк альтернативу для знаходження відстаней між рядками можна використовувати функції з бібліотеки stringdist.\nСпробуємо “очистити” дані, які ми отримали з допомогою функції amatch():\nМісія виконана! Замінимо та збережемо інформацію у файл для майбутніх експериментів по цій темі:\nЗамінимо також висоту на числове значення, а не текст:"
  },
  {
    "objectID": "42-r-cleaning-text.html#набори-даних",
    "href": "42-r-cleaning-text.html#набори-даних",
    "title": "22  Підготовка та очистка текстової інформації",
    "section": "22.1 Набори даних",
    "text": "22.1 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv"
  },
  {
    "objectID": "42-r-cleaning-text.html#використані-та-додаткові-джерела",
    "href": "42-r-cleaning-text.html#використані-та-додаткові-джерела",
    "title": "22  Підготовка та очистка текстової інформації",
    "section": "22.2 Використані та додаткові джерела",
    "text": "22.2 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia"
  },
  {
    "objectID": "43-r-missing-data.html",
    "href": "43-r-missing-data.html",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "",
    "text": "author: Юрій Клебан\nДані реального світу часто мають відсутні значення. Дані можуть мати відсутні значення з ряду причин, таких як спостереження, які не були записані, пошкодження даних, неспівставність форматів даних тощо.\nПроблема - [x] Обробка відсутніх даних важлива, оскільки багато алгоритмів машинного навчання або програм для візуалізації та аналізу даних не підтримують дані з відсутніми значеннями.\nРішення\nПримітка\nНекоректна інформація в даних може бути записана різними способами, наприклад у датасеті ці дані можуть бутьу визначені як NA <NA> NULL undefinded Undefined. Перед обробкою таких даних усі невизначені записи варто конвертувати у NA.\nЩоб переглянути список усіх стовпців, що мають пропуски даних можна скористатися наступним кодом:"
  },
  {
    "objectID": "43-r-missing-data.html#перевірка-наявності-пропусків-у-даних",
    "href": "43-r-missing-data.html#перевірка-наявності-пропусків-у-даних",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.1 Перевірка наявності пропусків у даних",
    "text": "23.1 Перевірка наявності пропусків у даних\nПакет MICE (Multivariate Imputation via Chained Equations)\n\nlibrary(mice)\nmd.pattern(data)\n\n\n\nA matrix: 4 × 6 of type dbl\n\n    person_ageperson_genderperson_heightperson_weightempty\n\n\n    41111 0 1\n    51110 0 2\n    41101 0 2\n    00451322\n\n\n\n\n\n\n\n\n#install.packages(\"VIM\")\n\n\nlibrary(VIM)\nmice_plot <- aggr(data, \n                  col=c('navyblue','yellow'),\n                  numbers=TRUE, \n                  sortVars=TRUE,\n                  labels=names(data), \n                  cex.axis=.7,\n                  gap=3, \n                  ylab=c(\"Missing data\",\"Pattern\"))\nmice_plot\n\n\n Variables sorted by number of missings: \n      Variable     Count\n         empty 1.0000000\n person_weight 0.3846154\n person_height 0.3076923\n    person_age 0.0000000\n person_gender 0.0000000\n\n\n\n Missings in variables:\n      Variable Count\n person_height     4\n person_weight     5\n         empty    13\n\n\n\n\n\n\n#install.packages(\"Amalia\")\n\n\nlibrary(Amelia)\nAmelia::missmap(data)\n\n\n\n\nТакож можна скористатися альтернативними макетами: missForest, mi."
  },
  {
    "objectID": "43-r-missing-data.html#видалення-пустих-рядків-та-сповпців-у-data.frame",
    "href": "43-r-missing-data.html#видалення-пустих-рядків-та-сповпців-у-data.frame",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.2 Видалення пустих рядків та сповпців у data.frame",
    "text": "23.2 Видалення пустих рядків та сповпців у data.frame\nПереглянемо стовпці, що містять пропуски:\n\n# Переглянемо список стовпців з пропусками\ncolnames(data)[apply(data, 2, anyNA)]\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\n\n'person_height''person_weight''empty'\n\n\nФункція complete.cases повертає логічні значення\n\ncomplete.cases(data) # бо є стовпець Empty\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\n\nFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSE\n\n\nТакож видаляти стовпці та рядки з data.frame можна за допомогою пакету janitor.\n\nlibrary(janitor)\ndata_cleaned <- remove_empty(data, which = c(\"rows\",\"cols\"), quiet = FALSE)\ndata_cleaned\n# Видаляємо повністю пусті\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windowsNo empty rows to remove.\n\nRemoving 1 empty columns of 5 columns total (Removed: empty).\n\n\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    123185  NAmale  \n    24117568.3male  \n    31114255.4female\n    412 NA48.2male  \n    554191  NAfemale\n    63216878.0female\n    722 NA54.0male  \n    821165  NAmale  \n    914 NA90.2male  \n    1051250  NAfemale\n    1141 2081.0female\n    1266 NA59.0male  \n    1371171  NAmale  \n\n\n\n\n\nwrite.csv(data_cleaned, file = \"data/cleaned_titled2.csv\", row.names = F)\n\nЯк бачимо, колонка empty була видалена.\nЩоб переглянути усі записи, що не мають пропусків скористаємося функцією na.omit():\n\nna.omit(data_cleaned)\n\n\n\nA data.frame: 4 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    24117568.3male  \n    31114255.4female\n    63216878.0female\n    1141 2081.0female\n\n\n\n\nТаким чином пропущені значення будуть видалені з датасети, якщо інформацію переприсвоїти data <- na.omit(data)"
  },
  {
    "objectID": "43-r-missing-data.html#заміна-пропусків-у-data.frame",
    "href": "43-r-missing-data.html#заміна-пропусків-у-data.frame",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.3 Заміна пропусків у data.frame",
    "text": "23.3 Заміна пропусків у data.frame\nІснує ряд підходів, що використовуються для заміни пропущених значень у датасеті:\nЗаміна на 0 * Вставте пропущені значення нулем\nЗаміна на медіану/середнє значення * Для числових змінних - середнє або медіана, мінімум, максимум * Для категоріальних змінних - мода (бувають випадки, коли моду доцільно використовувати і для числових)\nСегментна заміна * Визначення сегментів * Обчислення середнього/медіани/моди для сегментів * Замінити значення по сегментах * Наприклад, ми можемо сказати, що кількість опадів майже не змінюється для міст у певній області України, у такому випадку ми можемо для усіх міст з пропусками записати значення середнє по регіону.\nІнтелектуальна заміна (Частковий випадок сегментної заміни) * Заміна значень з використанням методів машинного навчання\n\n23.3.1 Заміна пропусків на нуль (0)\n\ndata <- read.csv(\"data/cleaned_titled2.csv\")\ndata\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    23185  NAmale  \n    4117568.3male  \n    1114255.4female\n    12 NA48.2male  \n    54191  NAfemale\n    3216878.0female\n    22 NA54.0male  \n    21165  NAmale  \n    14 NA90.2male  \n    51250  NAfemale\n    41 2081.0female\n    66 NA59.0male  \n    71171  NAmale  \n\n\n\n\nЗамінимо інформацію про вагу з пропусками на 0:\n\ndata_w0 <- data |> \n    mutate(person_weight = ifelse(is.na(person_weight), 0, person_weight))\ndata_w0\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    23185 0.0male  \n    4117568.3male  \n    1114255.4female\n    12 NA48.2male  \n    54191 0.0female\n    3216878.0female\n    22 NA54.0male  \n    21165 0.0male  \n    14 NA90.2male  \n    51250 0.0female\n    41 2081.0female\n    66 NA59.0male  \n    71171 0.0male  \n\n\n\n\n\n# Без dplyr\ndata_w0 <- data\ndata_w0[is.na(data_w0$person_weight), \"person_weight\"] <- 0\ndata_w0\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    23185 0.0male  \n    4117568.3male  \n    1114255.4female\n    12 NA48.2male  \n    54191 0.0female\n    3216878.0female\n    22 NA54.0male  \n    21165 0.0male  \n    14 NA90.2male  \n    51250 0.0female\n    41 2081.0female\n    66 NA59.0male  \n    71171 0.0male  \n\n\n\n\nЗробити заміну для усіх числових стовпців:\n\nlibrary(tidyr) # for replace_na()\ndata_all <- data |> \n    mutate_if(is.numeric , replace_na, replace = 0)\ndata_all\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    23185 0.0male  \n    4117568.3male  \n    1114255.4female\n    12  048.2male  \n    54191 0.0female\n    3216878.0female\n    22  054.0male  \n    21165 0.0male  \n    14  090.2male  \n    51250 0.0female\n    41 2081.0female\n    66  059.0male  \n    71171 0.0male"
  },
  {
    "objectID": "43-r-missing-data.html#числова-заміна-пропусків",
    "href": "43-r-missing-data.html#числова-заміна-пропусків",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.4 Числова заміна пропусків",
    "text": "23.4 Числова заміна пропусків\nЗаміна на константи або обчислені значення є стандарним підходом. Так, наприклад, заміна певного значення на середнє матиме вигляд:\n\ndata_m <- data |> \n    mutate(person_weight = ifelse(is.na(person_weight), mean(data$person_weight, na.rm = T), person_weight))\ndata_m\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    2318566.7625male  \n    4117568.3000male  \n    1114255.4000female\n    12 NA48.2000male  \n    5419166.7625female\n    3216878.0000female\n    22 NA54.0000male  \n    2116566.7625male  \n    14 NA90.2000male  \n    5125066.7625female\n    41 2081.0000female\n    66 NA59.0000male  \n    7117166.7625male  \n\n\n\n\nЗаміна на min, max, median не відрізняється.\nЯкщо виникає потреба замінити, наприклад, усі значення на медіану у всіх стовпцях за один прохід можна скористатися функцією mutate_if():\n\ndata_all <- data |> \n    mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm = T), x))\ndata_all\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    2318563.65male  \n    4117568.30male  \n    1114255.40female\n    1217148.20male  \n    5419163.65female\n    3216878.00female\n    2217154.00male  \n    2116563.65male  \n    1417190.20male  \n    5125063.65female\n    41 2081.00female\n    6617159.00male  \n    7117163.65male  \n\n\n\n\nРозглянемо кілька бібліотек для перевірки даних на наявність пропусків…\nЩе одним із варіантів заміни значень може бути використання бібліотеки Hmisc:\n\n#install.packages(\"Hmisc\")\n\n\nlibrary(Hmisc)\ndata_wm <- data |> \n    mutate(person_weight = impute(data$person_weight, fun = mean)) # mean imputation\n# Аналогічно можна замінити на min,max, median чи інші функції\ndata_wm \n# * Значення із * - замінені\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><impute><chr>\n\n\n    2318566.7625male  \n    4117568.3000male  \n    1114255.4000female\n    12 NA48.2000male  \n    5419166.7625female\n    3216878.0000female\n    22 NA54.0000male  \n    2116566.7625male  \n    14 NA90.2000male  \n    5125066.7625female\n    41 2081.0000female\n    66 NA59.0000male  \n    7117166.7625male  \n\n\n\n\n\n\n23.4.0.1 Hot deck imputation (як перекласти???)\nМетод Hot deck imputation передбачає, що пропущені значення обчислюються шляхом копіювання значень із подібних записів у тому ж наборі даних.\nОсновне питання при Hot deck imputation полягає в тому, як вибрати значення заміни. Одним із поширених підходів є випадковий відбір:\n\n# set.seed(1)\ndata_hot <- data |> \n    mutate(person_weight = impute(data$person_weight, \"random\")) \ndata_hot \n\n\n\nA data.frame: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><impute><chr>\n\n\n    2318559.0male  \n    4117568.3male  \n    1114255.4female\n    12 NA48.2male  \n    5419155.4female\n    3216878.0female\n    22 NA54.0male  \n    2116554.0male  \n    14 NA90.2male  \n    5125090.2female\n    41 2081.0female\n    66 NA59.0male  \n    7117154.0male  \n\n\n\n\nВихідне значення залежить від значення seed."
  },
  {
    "objectID": "43-r-missing-data.html#сегментна-заміна-пропусків",
    "href": "43-r-missing-data.html#сегментна-заміна-пропусків",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.5 Сегментна заміна пропусків",
    "text": "23.5 Сегментна заміна пропусків\nЗаміна по сегментах часто дозволяє будувати точніші математичні моделі, адже групові середні краще описують явища і процеси, ніж загальні для всієї вибірки.\nЗнайдемо середні значення ваги за статтю та використаємо ці значення для заміни пропусків у даних.\n\ndata_sgm <- data |> \n                group_by(person_gender) |>\n                mutate(person_weight = replace_na(person_weight, mean(person_weight, na.rm = TRUE)))\ndata_sgm\n\n\n\nA grouped_df: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    2318563.94000male  \n    4117568.30000male  \n    1114255.40000female\n    12 NA48.20000male  \n    5419171.46667female\n    3216878.00000female\n    22 NA54.00000male  \n    2116563.94000male  \n    14 NA90.20000male  \n    5125071.46667female\n    41 2081.00000female\n    66 NA59.00000male  \n    7117163.94000male  \n\n\n\n\nТакож можна здійснити заміну значень по усіх стовпцях датасету за один раз. Проте не варто такий підхід використовувати постійно, а враховувати бізнес-логіку процесів, що вивчаються.\n\ndata_sgm2 <- data %>% \n  group_by(person_gender) %>% \n    mutate(\n      across(everything(), ~replace_na(.x, min(.x, na.rm = TRUE)))\n    )\ndata_sgm2\n\n\n\nA grouped_df: 13 × 4\n\n    person_ageperson_heightperson_weightperson_gender\n    <int><int><dbl><chr>\n\n\n    2318548.2male  \n    4117568.3male  \n    1114255.4female\n    1216548.2male  \n    5419155.4female\n    3216878.0female\n    2216554.0male  \n    2116548.2male  \n    1416590.2male  \n    5125055.4female\n    41 2081.0female\n    6616559.0male  \n    7117148.2male  \n\n\n\n\nЯкщо ж є потреба замінювати по окремих стовпцях, то їх можна вказати замість everything(): across(c(\"person_height\", \"person_weight\"), ~replace_na(.x, min(.x, na.rm = TRUE))).\nІншим варіантом може бути вказання номерів колонок: across(c(1,3), ~replace_na(.x, min(.x, na.rm = TRUE)))"
  },
  {
    "objectID": "43-r-missing-data.html#інтелектуальні-методи-заміни",
    "href": "43-r-missing-data.html#інтелектуальні-методи-заміни",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.6 Інтелектуальні методи заміни",
    "text": "23.6 Інтелектуальні методи заміни\nТеоретично інтелектуальні методи заміни пропусків є найкращими, адже враховують математичні залежності у даних.\nhttps://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4\n\nlibrary(VIM)\ndata_knn <- kNN(data)\ndata_knn\n\n\n\nA data.frame: 13 × 8\n\n    person_ageperson_heightperson_weightperson_genderperson_age_impperson_height_impperson_weight_impperson_gender_imp\n    <int><int><dbl><chr><lgl><lgl><lgl><lgl>\n\n\n    2318559.0male  FALSEFALSE TRUEFALSE\n    4117568.3male  FALSEFALSEFALSEFALSE\n    1114255.4femaleFALSEFALSEFALSEFALSE\n    1216848.2male  FALSE TRUEFALSEFALSE\n    5419168.3femaleFALSEFALSE TRUEFALSE\n    3216878.0femaleFALSEFALSEFALSEFALSE\n    2216854.0male  FALSE TRUEFALSEFALSE\n    2116559.0male  FALSEFALSE TRUEFALSE\n    1416890.2male  FALSE TRUEFALSEFALSE\n    5125068.3femaleFALSEFALSE TRUEFALSE\n    41 2081.0femaleFALSEFALSEFALSEFALSE\n    6616859.0male  FALSE TRUEFALSEFALSE\n    7117159.0male  FALSEFALSE TRUEFALSE\n\n\n\n\nЩе одним схожим методом заміни пропусків може бути здійснення прогнозів на основі регресії чи складніших математичних методів пропусків."
  },
  {
    "objectID": "43-r-missing-data.html#набори-даних",
    "href": "43-r-missing-data.html#набори-даних",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.7 Набори даних",
    "text": "23.7 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv"
  },
  {
    "objectID": "43-r-missing-data.html#використані-та-додаткові-джерела",
    "href": "43-r-missing-data.html#використані-та-додаткові-джерела",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.8 Використані та додаткові джерела",
    "text": "23.8 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia"
  },
  {
    "objectID": "44-r-outliers.html",
    "href": "44-r-outliers.html",
    "title": "24  Аналіз та обробка статистичних викидів у даних",
    "section": "",
    "text": "author: Юрій Клебан\nВиявлення аномалій — це сукупність методів, призначених для виявлення незвичайних точок даних.\nАномалія - точка даних або набір точок даних, які не мають таку саму структуру та поведінку, що й інші дані.\nАномалії у даних можуть мати різну природу та по різному себе проявляти:\nПриклад: одна добова висока температура 41°С серед ряду звичайних весняних днів\nОпишемо набір даних, до використовуватиметься надалі для прикладів.\nriver_eco - це data.frame, що містить такі три стовпці: - [x] index - цілі числа, що описують порядок спостережень нітратів; - [x] nitrate - місячні концентрації розчинених нітратів у річці; - [x] month - змінна, що містить місяць для кожного спостереження нітратів\nНам потрібно дослідити стовпець nitrate, щоб оцінити наявність точкових аномалій у даних.\nПереглянемо описову статистику показника нітрати:\nЯк видно, медіана та середнє відрізняються не дуже.\nДалі перевіримо наявність викидів у даних за допомогою boxplot:\nТакож виведемо номери рядків спостереженнь, що є викидами:\nМіж Q1 та Q3 зосереджено 50% усіх спостережень. Персентиль відображає кількість спостережень, що зосереджені з ним включно. Нижче розміщено більше інформації для ознайомлення з інформацією про квантилі.\nQuantile. Wikipedia\nДжерело: https://en.wikipedia.org/wiki/Interquartile_range\nДжерело: https://makemeanalyst.com/explore-your-data-range-interquartile-range-and-box-plot/\nВизначивши викиди у даних з ними можна здійснити кілька операцій:\nТаким чином, усі значення вище та нище деякого показника можемо замінити на потрібні нам значення, наприклад, середні за поточний місяць.\nЗдійснимо заміну значень у наборі даних на основі квантилей:"
  },
  {
    "objectID": "44-r-outliers.html#додаткові-прийоми-очистки-даних",
    "href": "44-r-outliers.html#додаткові-прийоми-очистки-даних",
    "title": "24  Аналіз та обробка статистичних викидів у даних",
    "section": "24.1 Додаткові прийоми очистки даних ",
    "text": "24.1 Додаткові прийоми очистки даних \n\n24.1.1 Видалення дублікатів\n\ndf <- data.frame(X = c(1,1,2,1,3,2,1), Y = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\"))\ndf\n\n\n\nA data.frame: 7 × 2\n\n    XY\n    <dbl><chr>\n\n\n    1A\n    1B\n    2C\n    1A\n    3B\n    2C\n    1A\n\n\n\n\n\ndf |> distinct()\n\n\n\nA data.frame: 4 × 2\n\n    XY\n    <dbl><chr>\n\n\n    1A\n    1B\n    2C\n    3B"
  },
  {
    "objectID": "44-r-outliers.html#набори-даних",
    "href": "44-r-outliers.html#набори-даних",
    "title": "24  Аналіз та обробка статистичних викидів у даних",
    "section": "24.2 Набори даних",
    "text": "24.2 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv"
  },
  {
    "objectID": "44-r-outliers.html#використані-та-додаткові-джерела",
    "href": "44-r-outliers.html#використані-та-додаткові-джерела",
    "title": "24  Аналіз та обробка статистичних викидів у даних",
    "section": "24.3 Використані та додаткові джерела",
    "text": "24.3 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia"
  },
  {
    "objectID": "etl-feature-engineering.html",
    "href": "etl-feature-engineering.html",
    "title": "25  Feature engineering in R",
    "section": "",
    "text": "You need this packages for code execution:"
  },
  {
    "objectID": "etl-feature-engineering.html#whats-feature-engineering",
    "href": "etl-feature-engineering.html#whats-feature-engineering",
    "title": "25  Feature engineering in R",
    "section": "25.1 What’s Feature Engineering?",
    "text": "25.1 What’s Feature Engineering?\nFeature engineering is the most important technique used in creating machine learning models.\nFeature Engineering is a basic term used to cover many operations that are performed on the variables(features) to fit them into the algorithm. It helps in increasing the accuracy of the model thereby enhances the results of the predictions. Feature Engineered machine learning models perform better on data than basic machine learning models. The following aspects of feature engineering are as follows [1]:\n\nFeature Scaling: It is done to get the features on the same scale( for eg. Euclidean distance).\nFeature Transformation: It is done to normalize the data(feature) by a function.\nFeature Construction: It is done to create new features based on original descriptors to improve the accuracy of the predictive model.\n\nA \"feature\" in the context of predictive modeling is just another name for a predictor variable. Feature engineering is the general term for creating and manipulating predictors so that a good predictive model can be created."
  },
  {
    "objectID": "etl-feature-engineering.html#feature-scaling",
    "href": "etl-feature-engineering.html#feature-scaling",
    "title": "25  Feature engineering in R",
    "section": "25.2 Feature Scaling",
    "text": "25.2 Feature Scaling\nFeature Scaling refers to putting the values in the same range or same scale so that no variable is dominated by the other.\nMost of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Euclidean distance between two data points in their computations, this is a problem.\nIf left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. To suppress this effect, we need to bring all features to the same level of magnitudes. This can be achieved by scaling.\nHere’s the curious thing about feature scaling – it improves (significantly) the performance of some machine learning algorithms and does not work at all for others.\nAlso, what’s the difference between normalization and standardization? These are two of the most commonly used feature scaling techniques in machine learning but a level of ambiguity exists in their understanding.\n\n\n25.2.1 Normalization\n\n25.2.1.1 Theory\nNormalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\nHere’s the formula for normalization:\n\n\\(X' = \\frac{X-X_{min}}{X_{max} - X_{min}}\\)\n\nHere, \\(X_{max}\\) and \\(X_{min}\\) are the maximum and the minimum values of the feature respectively.\nWhen the value of \\(X\\) is the minimum value in the column, the numerator will be \\(0\\), and hence \\(X'\\) is \\(0\\).\nOn the other hand, when the value of \\(X\\) is the maximum value in the column, the numerator is equal to the denominator and thus the value of \\(X'\\) is \\(1\\).\nIf the value of \\(X\\) is between the minimum and the maximum value, then the value of \\(X'\\) is between \\(0\\) and \\(1\\).\n\n\n\n25.2.1.2 Practice\nSo, let’s implement own normalization function.\n\n# Lets use client churn dataset from telco: https://www.kaggle.com/blastchar/telco-customer-churn\nchurn_data <- read.csv(\"data/telecom_users.csv\")\nhead(churn_data)\n\n\n\nA data.frame: 6 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService⋯DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>⋯<chr><chr><chr><chr><chr><chr><chr><chr><dbl><chr>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ⋯No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No \n    245289688-YGXVRFemale0No No 44YesNo              Fiber optic⋯Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    363449286-DOJGFFemale1YesNo 38YesYes             Fiber optic⋯No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    467396994-KERXLMale  0No No  4YesNo              DSL        ⋯No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No \n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ⋯Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    622154312-GVYNHFemale0YesNo 70No No phone serviceDSL        ⋯Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\n\nstr(churn_data)\n\n'data.frame':   5986 obs. of  22 variables:\n $ X               : int  1869 4528 6344 6739 432 2215 5260 6001 1480 5137 ...\n $ customerID      : chr  \"7010-BRBUU\" \"9688-YGXVR\" \"9286-DOJGF\" \"6994-KERXL\" ...\n $ gender          : chr  \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ SeniorCitizen   : int  0 0 1 0 0 0 0 0 0 1 ...\n $ Partner         : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Dependents      : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n $ tenure          : int  72 44 38 4 2 70 33 1 39 55 ...\n $ PhoneService    : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ MultipleLines   : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ InternetService : chr  \"No\" \"Fiber optic\" \"Fiber optic\" \"DSL\" ...\n $ OnlineSecurity  : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ OnlineBackup    : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ DeviceProtection: chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ TechSupport     : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ StreamingTV     : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ StreamingMovies : chr  \"No internet service\" \"No\" \"No\" \"Yes\" ...\n $ Contract        : chr  \"Two year\" \"Month-to-month\" \"Month-to-month\" \"Month-to-month\" ...\n $ PaperlessBilling: chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ PaymentMethod   : chr  \"Credit card (automatic)\" \"Credit card (automatic)\" \"Bank transfer (automatic)\" \"Electronic check\" ...\n $ MonthlyCharges  : chr  \"24.1\" \"88.15\" \"74.95\" \"55.9\" ...\n $ TotalCharges    : num  1735 3973 2870 238 120 ...\n $ Churn           : chr  \"No\" \"No\" \"Yes\" \"No\" ...\n\n\n\n# next check summary of values\nsummary(churn_data)\n\n# check TotalCharges field\n\n       X         customerID           gender          SeniorCitizen   \n Min.   :   0   Length:5986        Length:5986        Min.   :0.0000  \n 1st Qu.:1777   Class :character   Class :character   1st Qu.:0.0000  \n Median :3546   Mode  :character   Mode  :character   Median :0.0000  \n Mean   :3534                                         Mean   :0.1614  \n 3rd Qu.:5292                                         3rd Qu.:0.0000  \n Max.   :7042                                         Max.   :1.0000  \n                                                                      \n   Partner           Dependents            tenure      PhoneService      \n Length:5986        Length:5986        Min.   : 0.00   Length:5986       \n Class :character   Class :character   1st Qu.: 9.00   Class :character  \n Mode  :character   Mode  :character   Median :29.00   Mode  :character  \n                                       Mean   :32.47                     \n                                       3rd Qu.:56.00                     \n                                       Max.   :72.00                     \n                                                                         \n MultipleLines      InternetService    OnlineSecurity     OnlineBackup      \n Length:5986        Length:5986        Length:5986        Length:5986       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n DeviceProtection   TechSupport        StreamingTV        StreamingMovies   \n Length:5986        Length:5986        Length:5986        Length:5986       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Contract         PaperlessBilling   PaymentMethod      MonthlyCharges    \n Length:5986        Length:5986        Length:5986        Length:5986       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  TotalCharges       Churn          \n Min.   :  18.8   Length:5986       \n 1st Qu.: 404.3   Class :character  \n Median :1412.2   Mode  :character  \n Mean   :2298.1                     \n 3rd Qu.:3847.0                     \n Max.   :8684.8                     \n NA's   :10                         \n\n\nNext, lets build histogram of Income and check how it splited with ggplot2:\n\n#install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nggplot() - function for building charts\ndata - first parameter - dataset\naes() - authetics - visualition axis / Construct aesthetic mappings\ngeom_CHART_TYPE() - set the chart type\ngeom_histogram() - Histograms and frequency polygons\n//theme_set() - theme configutation\n\nlibrary(ggplot2)\nggplot(data = churn_data, aes(x=TotalCharges)) + geom_histogram(bins = 15)\n\n# try theme\n\n\n\n\n\n# Lets replace missing with 0 zero for TotalCharges \nlibrary(magrittr) # if pipe not loaded\nlibrary(dplyr) # for mutate function\nchurn_data <- churn_data %>%\n            mutate(TotalCharges = ifelse(is.na(TotalCharges), 0 , TotalCharges))\n\nggplot(churn_data, aes(x=TotalCharges)) + geom_histogram(bins = 15)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\n\n\n# Lets implement own normalization function by fomula explained earlie\nnormalizeData <- function(x) {\n    return ((x - min(x)) / (max(x) - min(x)))\n}\n\n\n# Normalize TotalCharges\nchurn_data <- churn_data %>%\n    mutate(TotalChargesNorm = normalizeData(TotalCharges))\n\nchurn_data %>% head() # check the last columns\n\n\n\nA data.frame: 6 × 23\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...TechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurnTotalChargesNorm\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><dbl><chr><dbl>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ...No internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No 0.19973402\n    245289688-YGXVRFemale0No No 44YesNo              Fiber optic...No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No 0.45748895\n    363449286-DOJGFFemale1YesNo 38YesYes             Fiber optic...No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes0.33044515\n    467396994-KERXLMale  0No No  4YesNo              DSL        ...No                 No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No 0.02746177\n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ...No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No 0.01375967\n    622154312-GVYNHFemale0YesNo 70No No phone serviceDSL        ...Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No 0.38805730\n\n\n\n\n\n#summary for the last field\nsummary(churn_data$TotalChargesNorm)\n\n#its from 1 to zero\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.04624 0.16219 0.26417 0.44232 1.00000 \n\n\n\n# And lets make a histogram\nggplot(churn_data, aes(x=TotalChargesNorm)) + geom_histogram(bins = 15)\n\n\n\n\nWe observe identical histograms even though the TotalCharges / TotalChargesNorm axis is rescaled.\nTherefore we show that normalization didn’t affect the distribution properties of the rescaled data.\n\n\n\n\n25.2.2 Standardization\n\n25.2.2.1 Theory\nStandardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\nHere’s the formula for standardization:\n\n\\(X' = \\frac{X-\\mu}{\\sigma}\\)\n\nFeature scaling: \\(\\mu\\) is the mean of the feature values and Feature scaling: \\(\\sigma\\) is the standard deviation of the feature values. Note that in this case, the values are not restricted to a particular range.\nNow, the big question in your mind must be when should we use normalization and when should we use standardization?\nNormalization vs. standardization is an eternal question among machine learning newcomers. Let me elaborate on the answer in this section.\nNormalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\nStandardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization. However, at the end of the day, the choice of using normalization or standardization will depend on your problem and the machine learning algorithm you are using. There is no hard and fast rule to tell you when to normalize or standardize your data. You can always start by fitting your model to raw, normalized and standardized data and compare the performance for best results.\nIt is a good practice to fit the scaler on the training data and then use it to transform the testing data. This would avoid any data leakage during the model testing process. Also, the scaling of target values is generally not required.\n\n\n\n25.2.2.2 Practice\nLets write own function for standartization:\n\n# if sdev is NA - calculate start deviation from data\n\nstandartize <- function(data) {    \n    sdev = sd(data, na.rm = TRUE)  \n    data <- (data - mean(data, na.rm = T)) / sdev\n    return (data)    \n}\n\n\n# Normalize TotalCharges\nchurn_data <- churn_data %>%\n    mutate(TotalChargesStand = standartize(TotalCharges))\n\nchurn_data %>% head() # check the last columns\n\n\n\nA data.frame: 6 × 24\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...StreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurnTotalChargesNormTotalChargesStand\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><dbl><chr><dbl><dbl>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ...No internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No 0.19973402-0.2460559\n    245289688-YGXVRFemale0No No 44YesNo              Fiber optic...Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No 0.45748895 0.7382838\n    363449286-DOJGFFemale1YesNo 38YesYes             Fiber optic...No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes0.33044515 0.2531165\n    467396994-KERXLMale  0No No  4YesNo              DSL        ...No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No 0.02746177-0.9039460\n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ...No                 No                 Month-to-monthNo Electronic check         53.45 119.50No 0.01375967-0.9562729\n    622154312-GVYNHFemale0YesNo 70No No phone serviceDSL        ...No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No 0.38805730 0.4731314\n\n\n\n\nLets compare data distribution fot normalization and standartization.\n\n#install.packages(\"gridExtra\") # to view 2+ ggplots \nlibrary(gridExtra)\nn_plot <- ggplot(churn_data, aes(x=TotalChargesNorm)) + geom_histogram(bins = 15)\ns_plot <- ggplot(churn_data, aes(x=TotalChargesStand)) + geom_histogram(bins = 15)\ninit_plot <- ggplot(churn_data, aes(x=TotalCharges)) + geom_histogram(bins = 15)\ngrid.arrange(n_plot, init_plot, s_plot, ncol=3) # from gridExtra\n\n# data distribution changed after standartisation scaling\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\n\n\n\n\n\nSo, lets use stardart R function for scaling and compare results:\n\n# the next \nchurn_data <- churn_data %>%\n        mutate(TotalChargesScaled = as.numeric(scale(TotalCharges))) \n\ns1_plot <- ggplot(churn_data, aes(x=TotalChargesStand)) + geom_histogram(bins = 15)\ns2_plot <- ggplot(churn_data, aes(x=TotalChargesScaled)) + geom_histogram(bins = 15)\ngrid.arrange(s1_plot, s2_plot, ncol=2) # it looks like we created the same function \n\n\n\n\n\nmean(churn_data$TotalCharges)\n\n2294.22155863682\n\n\nLook like default function has the same result as our.\n\nhead(churn_data) # check last two columns, its the same\n\n\n\nA data.frame: 6 × 25\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...StreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurnTotalChargesNormTotalChargesStandTotalChargesScaled\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><dbl><chr><dbl><dbl><dbl>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ...No internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No 0.19973402-0.2460559-0.2460559\n    245289688-YGXVRFemale0No No 44YesNo              Fiber optic...No                 Month-to-monthYesCredit card (automatic)  88.153973.20No 0.45748895 0.7382838 0.7382838\n    363449286-DOJGFFemale1YesNo 38YesYes             Fiber optic...No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes0.33044515 0.2531165 0.2531165\n    467396994-KERXLMale  0No No  4YesNo              DSL        ...Yes                Month-to-monthYesElectronic check         55.9  238.50No 0.02746177-0.9039460-0.9039460\n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ...No                 Month-to-monthNo Electronic check         53.45 119.50No 0.01375967-0.9562729-0.9562729\n    622154312-GVYNHFemale0YesNo 70No No phone serviceDSL        ...Yes                Two year      YesBank transfer (automatic)49.853370.20No 0.38805730 0.4731314 0.4731314\n\n\n\n\n\n\n\n\n25.2.3 Scaling for train/test/validation/prediction\nIf you use scaling initial parameters should be remembered somewhere for future prediction data and reimplemented for new/test/validation/prediction dataset\nFor experiment lets split our dataset for train and test:\n\nlibrary(caret)\nset.seed(2021)\n \nindex = createDataPartition(churn_data$Churn, p = 0.70, list = FALSE)\ntrain = churn_data[index, ]\ntest = churn_data[-index, ]\n\nnrow(churn_data)\nnrow(train)\nnrow(test)\n\nLoading required package: lattice\n\n\n\n5986\n\n\n4191\n\n\n1795\n\n\nLets rescale TotalCharges data for training set:\n\ntrain <- train %>% mutate(TotalChargesScaled = scale(TotalCharges))\nhead(train) # you can see that TotalChangesStand and TotalChangesScaled are different becouse of changed mean and standart deviation of data\n\n\n\nA data.frame: 6 × 25\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...StreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurnTotalChargesNormTotalChargesStandTotalChargesScaled\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><dbl><chr><dbl><dbl><dbl[,1]>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ...No internet serviceTwo year      No Credit card (automatic)24.1 1734.65No 0.19973402-0.2460559-0.2516955\n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ...No                 Month-to-monthNo Electronic check       53.45 119.50No 0.01375967-0.9562729-0.9584688\n    914808898-KASCDMale  0No No 39No No phone serviceDSL        ...No                 One year      No Mailed check           35.551309.15No 0.15074037-0.4331576-0.4378900\n    1051378016-NCFVOMale  1No No 55YesYes             Fiber optic...Yes                Month-to-monthYesElectronic check       116.56382.55No 0.73491042 1.7977280 1.7821786\n    1131694578-PHJYZMale  0YesYes52YesNo              DSL        ...No                 One year      YesElectronic check       68.753482.85No 0.40102823 0.5226661 0.5132992\n    1246532091-MJTFXFemale0YesYes30No No phone serviceDSL        ...Yes                Month-to-monthNo Credit card (automatic)51.2 1561.50Yes0.17979689-0.3221938-0.3274642\n\n\n\n\nSo, for train, test and prediction data we should use the same scaling base, in this case mean and standart deviation.\nCorrect data scaling code should be like this:\n\n# fix mean and sd\nmeanTotalCharges = mean(train$TotalCharges, na.rm = T)\nsdTotalCharges = sd(train$TotalCharges, na.rm = T)\n\ntrain <- train %>% mutate(TotalChargesScaled = scale(TotalCharges, center = meanTotalCharges, scale = sdTotalCharges)) # default\ntest <- test %>% mutate(TotalChargesScaled = scale(TotalCharges, center = meanTotalCharges, scale = sdTotalCharges)) # use parameters of train set\n\nsd(train$TotalChargesScaled)\nsd(test$TotalChargesScaled)\n\n#check the same value TotalCharges == 0 in train and set\nhead(train %>% filter(TotalCharges == 0))\nhead(test %>% filter(TotalCharges == 0))\n\n1\n\n\n0.983829325647588\n\n\n\n\nA data.frame: 6 × 25\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...StreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurnTotalChargesNormTotalChargesStandTotalChargesScaled\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><dbl><chr><dbl><dbl><dbl[,1]>\n\n\n    167542775-SEFEEMale  0No Yes0YesYes             DSL...No                 Two yearYesBank transfer (automatic)61.9 0No0-1.00882-1.010761\n    238263213-VVOLGMale  0YesYes0YesYes             No ...No internet serviceTwo yearNo Mailed check             25.350No0-1.00882-1.010761\n    352182923-ARZLGMale  0YesYes0YesNo              No ...No internet serviceOne yearYesMailed check             19.7 0No0-1.00882-1.010761\n    433317644-OMVMYMale  0YesYes0YesNo              No ...No internet serviceTwo yearNo Mailed check             19.850No0-1.00882-1.010761\n    5 7533115-CZMZDMale  0No Yes0YesNo              No ...No internet serviceTwo yearNo Mailed check             20.250No0-1.00882-1.010761\n    6 4884472-LVYGIFemale0YesYes0No No phone serviceDSL...No                 Two yearYesBank transfer (automatic)52.550No0-1.00882-1.010761\n\n\n\n\n\n\nA data.frame: 3 × 25\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...StreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurnTotalChargesNormTotalChargesStandTotalChargesScaled\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><dbl><chr><dbl><dbl><dbl[,1]>\n\n\n    113401371-DWPAZFemale0YesYes0No No phone serviceDSL...No                 Two yearNoCredit card (automatic)56.050No0-1.00882-1.010761\n    2 9365709-LVOEQFemale0YesYes0YesNo              DSL...Yes                Two yearNoMailed check           80.850No0-1.00882-1.010761\n    343802520-SGTTAFemale0YesYes0YesNo              No ...No internet serviceTwo yearNoMailed check           20.0 0No0-1.00882-1.010761\n\n\n\n\n\n#compare it with all dataset TotalCharges == 0\n\n\nfilter(churn_data, TotalCharges == 0)\n# for now TotalChargesScaled in train/test th same, but in churn data its different, because of diffrent scaling bases\n\n\n\nA data.frame: 10 × 25\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...StreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurnTotalChargesNormTotalChargesStandTotalChargesScaled\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><dbl><chr><dbl><dbl><dbl>\n\n\n    67542775-SEFEEMale  0No Yes0YesYes             DSL...No                 Two yearYesBank transfer (automatic)61.9 0No0-1.00882-1.00882\n    13401371-DWPAZFemale0YesYes0No No phone serviceDSL...No                 Two yearNo Credit card (automatic)  56.050No0-1.00882-1.00882\n    38263213-VVOLGMale  0YesYes0YesYes             No ...No internet serviceTwo yearNo Mailed check             25.350No0-1.00882-1.00882\n    52182923-ARZLGMale  0YesYes0YesNo              No ...No internet serviceOne yearYesMailed check             19.7 0No0-1.00882-1.00882\n    33317644-OMVMYMale  0YesYes0YesNo              No ...No internet serviceTwo yearNo Mailed check             19.850No0-1.00882-1.00882\n     9365709-LVOEQFemale0YesYes0YesNo              DSL...Yes                Two yearNo Mailed check             80.850No0-1.00882-1.00882\n     7533115-CZMZDMale  0No Yes0YesNo              No ...No internet serviceTwo yearNo Mailed check             20.250No0-1.00882-1.00882\n    43802520-SGTTAFemale0YesYes0YesNo              No ...No internet serviceTwo yearNo Mailed check             20.0 0No0-1.00882-1.00882\n     4884472-LVYGIFemale0YesYes0No No phone serviceDSL...No                 Two yearYesBank transfer (automatic)52.550No0-1.00882-1.00882\n    10824367-NUYAOMale  0YesYes0YesYes             No ...No internet serviceTwo yearNo Mailed check             25.750No0-1.00882-1.00882"
  },
  {
    "objectID": "etl-feature-engineering.html#feature-transformation",
    "href": "etl-feature-engineering.html#feature-transformation",
    "title": "25  Feature engineering in R",
    "section": "25.3 Feature Transformation",
    "text": "25.3 Feature Transformation\nFeature transformation involves manipulating a predictor variable in some way so as to improve its performance in the predictive model. A variety of considerations come into play when transforming models, including:\n\nThe flexibility of machine learning and statistical models in dealing with different types of data. For example, some techniques require that the input data be in numeric format, whereas others can deal with other formats, such as categorical, text, or dates.\nEase of interpretation. A predictive model where all the predictors are on the same scale (e.g., have a mean of 0 and a standard deviation of 1), can make interpretation easier.\nPredictive accuracy. Some transformations of variables can improve the accuracy of prediction (e.g., rather than including a numeric variable as a predictor, instead include both it and a second variable that is its square).\nTheory. For example, economic theory dictates that in many situations the natural logarithm of data representing prices and quantities should be used.\nComputational error. Many algorithms are written in such a way that “large” numbers cause them to give the wrong result, where “large” may not be so large (e.g., more than 10 or less than -10).\n\n\n25.3.1 Scaling based on calculations\nSometimes for changing data distribution before using in modeling or change correlation between input and output variables scientist changes data type with standart mathematical functions. Lets try transform TotalCharges with logarithm, sqrt and power up 2.\n\nlibrary(gridExtra)\nchurn_data_tmp <- churn_data %>%\n        mutate(TotalChargesLog = log(TotalCharges),\n              TotalChargesSqrt = sqrt(TotalCharges),\n              TotalChargesPow2 = TotalCharges^2)\n\nplot1 <- ggplot(churn_data_tmp, aes(x=TotalChargesLog)) + geom_histogram(bins = 15)\nplot2 <- ggplot(churn_data_tmp, aes(x=TotalChargesSqrt)) + geom_histogram(bins = 15)\nplot3 <- ggplot(churn_data_tmp, aes(x=TotalChargesPow2)) + geom_histogram(bins = 15)\ngrid.arrange(plot1, plot2, plot3, ncol=3) \n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\n\n\n\n\n\nLets try\n\n#install.packages(\"moments\")\n\npackage 'moments' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    D:\\Temp\\Rtmpum9OKD\\downloaded_packages\n\n\n\nlibrary(moments)\n\nskewness(churn_data_tmp$TotalCharges)\nskewness(churn_data_tmp$TotalChargesLog)\nskewness(churn_data_tmp$TotalChargesSqrt)\nskewness(churn_data_tmp$TotalChargesPow2)\n\n0.951033233871077\n\n\nNaN\n\n\n0.301289466558413\n\n\n1.80616147912626\n\n\nConclusion: different scaling gives different data distribution and may improve model perfomance if you have found the correct form of dependence of input/ouput parameters."
  },
  {
    "objectID": "etl-feature-engineering.html#feature-construction",
    "href": "etl-feature-engineering.html#feature-construction",
    "title": "25  Feature engineering in R",
    "section": "25.4 Feature Construction",
    "text": "25.4 Feature Construction\nThe feature Construction method helps in creating new features in the data thereby increasing model accuracy and overall predictions. It is of two types:\n\nBinning: Bins are created for continuous variables.\nEncoding: Numerical variables or features are formed from categorical variables.\n\nДодано мною, бо не знайшов ніде такого прийому) Всі одразу моделюють)) - [x] Evaluation - construction new features on raw data from datasource.\n\n25.4.1 Binning\nBinning is done to create bins for continuous variables where they are converted to categorical variables.\nBinning is the term used in scoring modeling for what is also known in Machine Learning as Discretization, the process of transforming a continuous characteristic into a finite number of intervals (the bins), which allows for a better understanding of its distribution and its relationship with a binary variable. The bins generated by the this process will eventually become the attributes of a predictive characteristic, the key component of a Scorecard.\nWhy Binning?\n\nIt allows missing data and other special calculations (e.g. divided by zero) to be included in the model.\nIt controls or mitigates the impact of outliers over the model.\nIt solves the issue of having different scales among the characteristics, making the weights of the coefficients in the final model comparable.\n\nThere are two types of binning: Unsupervised and Supervised.\nUnsupervised Binning involves Automatic and Manual binning. In Automatic Binning, bins are created without human interference and are created automatically. In Manual Binning, bins are created with human interference and we specify where the bins to be created.\nSupervised Binning involves creating bins for the continuous variable while taking the target variable into the consideration also. Supervised Discretization or Binning divides a continuous feature into groups (bins) mapped to a target variable. The central idea is to find those cutpoints that maximize the difference between the groups.\nIn the past, analysts used to iteratively move from Fine Binning to Coarse Binning, a very time consuming process of finding manually and visually the right cutpoints (if ever). Nowadays with algorithms like ChiMerge or Recursive Partitioning, two out of several techniques available, analysts can quickly find the optimal cutpoints in seconds and evaluate the relationship with the target variable using metrics such as Weight of Evidence and Information Value.\nThere are many packages for creating new variables: smbinning, scorecard, rbin, InformationValue and other.\n\n25.4.1.1 WOE binning: theory\nWeight of evidence (WOE)\nThis is basically a technique that can be applied if we have a binary response variable and any kind of predictor variable. First we perform a reasonable binning on the response variable and then decide which form of the binary response we count as positive and which as negative. Then we calculate the percentage positive cases in each bin of the total of all positive cases. For example 20 positive cases in bin A out of 100 total positive cases in all bins equals 20 %. Next we calculate the percentage of negative cases in each bin of the total of all negative cases, for example 5 negative cases in bin A out of a total of 50 negative cases in all bins equals 10%. Then we calculate the WOE by dividing the bin percentages of positive cases by the bin percentage of negative cases, and take the logarithm. For the described example log(20/10).\nRule of thump: If WOE values are negative, negative cases supersede the positive cases. If WOE values are positive, positive cases supersede the negative cases.\nThis serves the following purposes:\n\nWe eliminate any none-linear relationships\nWe automatically scale all variables too some extend\nWe convert categorical variables to contineous variables\nMissing Data can be handled as just another factor value\nWe can built a stand alone score card, that could be manually applied by a person with a pen and a printout of all relevant variables.\n\nIt has the following disadvantages:\n\nWe always loose information via binning\nScore development along single variables is not contineous and occurs in steps\nBinning requires manual revision\nCalculating Variable importance is not as straight forward as with classical logistic regression with regularly scaled variables\n\nInformation Value (IV)\nBy doing another sequence of calculations similar to the WOE calculation we can calculate the IV. Classically this serves as variable ranking method and allows us to perform feature selection, which is less compuationally demanding as other methods.\n\n\n\nInformation Value\nPredictive Power\n\n\n\n\n< 0.02\nuseless for prediction\n\n\n0.02 - 0.1\nweak predictor\n\n\n0.1 - 0.3\nmedium predictor\n\n\n0.3 - 0.5\nstrong predictor\n\n\n> 0.5\nsuspicious too good to be true\n\n\n\nAfter calculating WOE value it replaces the original values in dataset.\n\n\n25.4.1.2 scorecard package and woebin()-function\nwoebin generates optimal binning for numerical, factor and categorical variables using methods including tree-like segmentation or chi-square merge. woebin can also customizing breakpoints if the breaks_list was provided. The default woe is defined as ln(Pos_i/Neg_i). If you prefer ln(Neg_i/Pos_i), please set the argument positive as negative value, such as ‘0’ or ‘good’. If there is a zero frequency class when calculating woe, the zero will replaced by 0.99 to make the woe calculable.\n\n# lets try to bin InternetService, TotalCharges from churn_data_tmp\nchurn_data_tmp <- churn_data %>%\n        mutate(Churn = ifelse(Churn == \"Yes\", 1, 0))\n\nbin_data <- churn_data_tmp %>% select(customerID, InternetService, TotalCharges, Churn)\nhead(bin_data)\n\n#churn_data_tmp%>%select(Churn) %>% distinct()\n\n\n\nA data.frame: 6 × 4\n\n    customerIDInternetServiceTotalChargesChurn\n    <chr><chr><dbl><dbl>\n\n\n    17010-BRBUUNo         1734.650\n    29688-YGXVRFiber optic3973.200\n    39286-DOJGFFiber optic2869.851\n    46994-KERXLDSL         238.500\n    52181-UAESMDSL         119.500\n    64312-GVYNHDSL        3370.200\n\n\n\n\n\n# install.packages(\"scorecard\")\nlibrary(scorecard)\n\nbins = woebin(bin_data, # dataset\n              y = 'Churn', # target variable\n              x = c(\"InternetService\", \"TotalCharges\")) # variables for binning\n\n[INFO] creating woe binning ... \n\n\n\n# lets view our bins\nbins\n\n\n\n    $InternetService\n        \nA data.table: 3 × 12\n\n    variablebincountcount_distrnegposposprobwoebin_ivtotal_ivbreaksis_special_values\n    <chr><chr><int><dbl><int><int><dbl><dbl><dbl><dbl><chr><lgl>\n\n\n    InternetServiceNo         12910.21566991192  990.07668474-1.46873620.306361950.5897126No         FALSE\n    InternetServiceDSL        20680.34547281671 3970.19197292-0.41770940.054177550.5897126DSL        FALSE\n    InternetServiceFiber optic26270.4388573153610910.41530263 0.67744490.229173060.5897126Fiber opticFALSE\n\n\n\n    $TotalCharges\n        \nA data.table: 4 × 12\n\n    variablebincountcount_distrnegposposprobwoebin_ivtotal_ivbreaksis_special_values\n    <chr><chr><int><dbl><int><int><dbl><dbl><dbl><dbl><chr><lgl>\n\n\n    TotalCharges[-Inf,200) 10040.16772469 5174870.4850598 0.95975300.1817211730.3097399200 FALSE\n    TotalCharges[200,400)   4880.08152355 3271610.3299180 0.31097600.0084318650.3097399400 FALSE\n    TotalCharges[400,3800) 29810.4979953222667150.2398524-0.13395710.0086511460.30973993800FALSE\n    TotalCharges[3800, Inf)15130.2527564312892240.1480502-0.73044420.1109357110.3097399Inf FALSE\n\n\n\n\n\n\n\nchurn_data_tmp %>% filter(TotalCharges > 3800) %>% select(Churn) %>% group_by(Churn) %>% summarize(n())\n\n\n\nA tibble: 2 × 2\n\n    Churnn()\n    <dbl><int>\n\n\n    01289\n    1 224\n\n\n\n\n\nbins$TotalCharges %>% knitr::kable() # better view for RStudio, need knitr to be installed\n\n\n\n|variable     |bin         | count| count_distr|  neg| pos|   posprob|        woe|    bin_iv|  total_iv|breaks |is_special_values |\n|:------------|:-----------|-----:|-----------:|----:|---:|---------:|----------:|---------:|---------:|:------|:-----------------|\n|TotalCharges |[-Inf,200)  |  1004|   0.1677247|  517| 487| 0.4850598|  0.9597530| 0.1817212| 0.3097399|200    |FALSE             |\n|TotalCharges |[200,400)   |   488|   0.0815236|  327| 161| 0.3299180|  0.3109760| 0.0084319| 0.3097399|400    |FALSE             |\n|TotalCharges |[400,3800)  |  2981|   0.4979953| 2266| 715| 0.2398524| -0.1339571| 0.0086511| 0.3097399|3800   |FALSE             |\n|TotalCharges |[3800, Inf) |  1513|   0.2527564| 1289| 224| 0.1480502| -0.7304442| 0.1109357| 0.3097399|Inf    |FALSE             |\n\n\n\n# preview the plot\nwoebin_plot(bins$TotalCharges)\n\n$TotalCharges\n\n\n\n\n\nIf TotalCharges less than 200 its most risky group of customers, 48.5% of them are potential churn.\n\nbins$InternetService %>% knitr::kable() \nwoebin_plot(bins$InternetService)\n\n\n\n|variable        |bin         | count| count_distr|  neg|  pos|   posprob|        woe|    bin_iv|  total_iv|breaks      |is_special_values |\n|:---------------|:-----------|-----:|-----------:|----:|----:|---------:|----------:|---------:|---------:|:-----------|:-----------------|\n|InternetService |No          |  1291|   0.2156699| 1192|   99| 0.0766847| -1.4687362| 0.3063620| 0.5897126|No          |FALSE             |\n|InternetService |DSL         |  2068|   0.3454728| 1671|  397| 0.1919729| -0.4177094| 0.0541776| 0.5897126|DSL         |FALSE             |\n|InternetService |Fiber optic |  2627|   0.4388573| 1536| 1091| 0.4153026|  0.6774449| 0.2291731| 0.5897126|Fiber optic |FALSE             |\n\n\n$InternetService\n\n\n\n\n\nFiber optic InternetService customers are the most risky. 41.5% of them are potentiona churn.\nThe next stage is applying bins to th the variables:\n\nbin_data_woe = woebin_ply(bin_data, bins) \nhead(bin_data)\nhead(bin_data_woe) # compare how it changed\n\n[INFO] converting into woe values ... \n\n\n\n\nA data.frame: 6 × 4\n\n    customerIDInternetServiceTotalChargesChurn\n    <chr><chr><dbl><dbl>\n\n\n    17010-BRBUUNo         1734.650\n    29688-YGXVRFiber optic3973.200\n    39286-DOJGFFiber optic2869.851\n    46994-KERXLDSL         238.500\n    52181-UAESMDSL         119.500\n    64312-GVYNHDSL        3370.200\n\n\n\n\n\n\nA data.table: 6 × 4\n\n    customerIDChurnInternetService_woeTotalCharges_woe\n    <chr><dbl><dbl><dbl>\n\n\n    7010-BRBUU0-1.4687362-0.1339571\n    9688-YGXVR0 0.6774449-0.7304442\n    9286-DOJGF1 0.6774449-0.1339571\n    6994-KERXL0-0.4177094 0.3109760\n    2181-UAESM0-0.4177094 0.9597530\n    4312-GVYNH0-0.4177094-0.1339571\n\n\n\n\n\nbins$InternetService #No is replaced by WOE -1.4687362 (line 1)\n\n\n\nA data.table: 3 × 12\n\n    variablebincountcount_distrnegposposprobwoebin_ivtotal_ivbreaksis_special_values\n    <chr><chr><int><dbl><int><int><dbl><dbl><dbl><dbl><chr><lgl>\n\n\n    InternetServiceNo         12910.21566991192  990.07668474-1.46873620.306361950.5897126No         FALSE\n    InternetServiceDSL        20680.34547281671 3970.19197292-0.41770940.054177550.5897126DSL        FALSE\n    InternetServiceFiber optic26270.4388573153610910.41530263 0.67744490.229173060.5897126Fiber opticFALSE\n\n\n\n\nSo, both numerical and categorical variables are binned by woebin function.\nIn real-life project you should dou binning with the next steps:\n\nClean data\nSplit data into train/test (+prediction)\nCreate bins on train set (and save them if your model not one-time used)\nApply bins to all sets you have\n\nLest see how to make it with our dataset churn_data.\n\nlibrary(caret)\nlibrary(gmodels)\nset.seed(2021) # to fix split options\n\nchurn_data <- read.csv(\"../../data/telecom_users.csv\") # read data\nchurn_data <- churn_data %>% \n    select(customerID, gender, PaymentMethod, TotalCharges, Churn) %>% # select some columns for test + target\n    mutate(Churn = ifelse(Churn == \"Yes\", 1, 0)) # replace Churn Yes/No with 1/0 - Event/NonEvent\nhead(churn_data)\n\n\n\nA data.frame: 6 × 5\n\n    customerIDgenderPaymentMethodTotalChargesChurn\n    <chr><chr><chr><dbl><dbl>\n\n\n    17010-BRBUUMale  Credit card (automatic)  1734.650\n    29688-YGXVRFemaleCredit card (automatic)  3973.200\n    39286-DOJGFFemaleBank transfer (automatic)2869.851\n    46994-KERXLMale  Electronic check          238.500\n    52181-UAESMMale  Electronic check          119.500\n    64312-GVYNHFemaleBank transfer (automatic)3370.200\n\n\n\n\n\nindex = createDataPartition(churn_data$Churn, p = 0.60, list = FALSE) # select randomly indexes of the rows for train\ntrain = churn_data[index, ]\ntest = churn_data[-index, ]\n\n\ndata_bins = woebin(train, # dataset\n                   y = \"Churn\", # target\n                   var_skip = \"customerID\" # skip ID\n                  # x = c(\"gender\", \"PaymentMethod\", \"TotalCharges\") # select some varables\n                   #var_skip = \"customerID\" # target variable - not working in jupyter\n             )\n\n[INFO] creating woe binning ... \n\n\n\ndata_bins\n\n\n\n    $gender\n        \nA data.table: 2 × 12\n\n    variablebincountcount_distrnegposposprobwoebin_ivtotal_ivbreaksis_special_values\n    <chr><chr><int><dbl><int><int><dbl><dbl><dbl><dbl><chr><lgl>\n\n\n    genderFemale17700.492761713144560.2576271-0.025467180.00031765580.0006226087FemaleFALSE\n    genderMale  18220.507238313354870.2672887 0.024448760.00030495290.0006226087Male  FALSE\n\n\n\n    $PaymentMethod\n        \nA data.table: 3 × 12\n\n    variablebincountcount_distrnegposposprobwoebin_ivtotal_ivbreaksis_special_values\n    <chr><chr><int><dbl><int><int><dbl><dbl><dbl><dbl><chr><lgl>\n\n\n    PaymentMethodCredit card (automatic)%,%Bank transfer (automatic)15620.434855213122500.1600512-0.62497580.143850620.4324699Credit card (automatic)%,%Bank transfer (automatic)FALSE\n    PaymentMethodMailed check                                        8220.2288419 6651570.1909976-0.41067000.034721410.4324699Mailed check                                       FALSE\n    PaymentMethodElectronic check                                   12080.3363029 6725360.4437086 0.80674700.253897890.4324699Electronic check                                   FALSE\n\n\n\n    $TotalCharges\n        \nA data.table: 5 × 12\n\n    variablebincountcount_distrnegposposprobwoebin_ivtotal_ivbreaksis_special_values\n    <chr><chr><int><dbl><int><int><dbl><dbl><dbl><dbl><chr><lgl>\n\n\n    TotalChargesmissing       60.001670379   6  00.00000000-0.76998790.00093650990.3222676missing TRUE\n    TotalCharges[-Inf,400)  8930.248608018 5053880.43449048 0.76826880.16931363980.3222676400    FALSE\n    TotalCharges[400,3200) 16010.44571269512073940.24609619-0.08772040.00335673820.32226763200   FALSE\n    TotalCharges[3200,6600) 8500.236636971 7111390.16352941-0.60037660.07273913400.32226766600   FALSE\n    TotalCharges[6600, Inf) 2420.067371938 220 220.09090909-1.27076320.07592158840.3222676Inf    FALSE\n\n\n\n\n\n\n\ntrain_woe = woebin_ply(train, data_bins) \nhead(train_woe)\ntest_woe = woebin_ply(test, data_bins) \nhead(test_woe)\n\n[INFO] converting into woe values ... \n\n\n\n\nA data.table: 6 × 5\n\n    customerIDChurngender_woePaymentMethod_woeTotalCharges_woe\n    <chr><dbl><dbl><dbl><dbl>\n\n\n    7010-BRBUU0 0.02444876-0.6249758-0.0877204\n    4367-NHWMM0-0.02546718-0.4106700 0.7682688\n    8016-NCFVO0 0.02444876 0.8067470-0.6003766\n    4578-PHJYZ0 0.02444876 0.8067470-0.6003766\n    2091-MJTFX1-0.02546718-0.6249758-0.0877204\n    2277-DJJDL0 0.02444876 0.8067470-0.6003766\n\n\n\n\n[INFO] converting into woe values ... \n\n\n\n\nA data.table: 6 × 5\n\n    customerIDChurngender_woePaymentMethod_woeTotalCharges_woe\n    <chr><dbl><dbl><dbl><dbl>\n\n\n    9688-YGXVR0-0.02546718-0.6249758-0.6003766\n    9286-DOJGF1-0.02546718-0.6249758-0.0877204\n    6994-KERXL0 0.02444876 0.8067470 0.7682688\n    2181-UAESM0 0.02444876 0.8067470 0.7682688\n    4312-GVYNH0-0.02546718-0.6249758-0.6003766\n    2495-KZNFB0-0.02546718 0.8067470-0.0877204\n\n\n\n\nFor now our data is ready for modeling.\n\n\n\n25.4.1.3 Encoding\nEncoding is the process in which numerical variables or features are created from categorical variables. It is a widely used method in the industry and in every model building process. It is of two types: Label Encoding and One-hot Encoding.\nLabel Encoding involves assigning each label a unique integer or value based on alphabetical ordering. It is the most popular and widely used encoding.\nOne-hot Encoding involves creating additional features or variables on the basis of unique values in categorical variables i.e. every unique value in the category will be added as a new feature.\n\n25.4.1.3.1 Label encoding\n\n# Lets use client churn dataset from telco: https://www.kaggle.com/blastchar/telco-customer-churn\nchurn_data <- read.csv(\"data/telecom_users.csv\")\nhead(churn_data)\n\n\n\nA data.frame: 6 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService⋯DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>⋯<chr><chr><chr><chr><chr><chr><chr><chr><dbl><chr>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ⋯No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No \n    245289688-YGXVRFemale0No No 44YesNo              Fiber optic⋯Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    363449286-DOJGFFemale1YesNo 38YesYes             Fiber optic⋯No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    467396994-KERXLMale  0No No  4YesNo              DSL        ⋯No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No \n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ⋯Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    622154312-GVYNHFemale0YesNo 70No No phone serviceDSL        ⋯Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\n\n\n\n25.4.1.3.2 Encoding with factors\nIts good way to encode labels as factors in R. This approach used for categorical data encoding. Easiest way is convert character values to factor and later convert factor to numeric:\n\nchurn_data <- churn_data %>% mutate(Partner = as.factor(Partner),\n                                   Dependents = as.factor(Dependents))\nhead(churn_data) # data types changed for selected fields\n\n\n\nA data.frame: 6 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService⋯DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><fct><fct><int><chr><chr><chr>⋯<chr><chr><chr><chr><chr><chr><chr><chr><dbl><chr>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ⋯No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No \n    245289688-YGXVRFemale0No No 44YesNo              Fiber optic⋯Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    363449286-DOJGFFemale1YesNo 38YesYes             Fiber optic⋯No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    467396994-KERXLMale  0No No  4YesNo              DSL        ⋯No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No \n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ⋯Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    622154312-GVYNHFemale0YesNo 70No No phone serviceDSL        ⋯Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\n\nstr(churn_data$Partner) # check levels\n\n Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 2 1 1 1 1 ...\n\n\n\n# convert to integers\nchurn_data <- churn_data %>% mutate(Partner = as.integer(Partner),\n                                   Dependents = as.integer(Dependents))\nhead(churn_data) # data types changed for selected fields\n\n\n\nA data.frame: 6 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService⋯DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><int><int><int><chr><chr><chr>⋯<chr><chr><chr><chr><chr><chr><chr><chr><dbl><chr>\n\n\n    118697010-BRBUUMale  02272YesYes             No         ⋯No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No \n    245289688-YGXVRFemale01144YesNo              Fiber optic⋯Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    363449286-DOJGFFemale12138YesYes             Fiber optic⋯No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    467396994-KERXLMale  011 4YesNo              DSL        ⋯No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No \n    5 4322181-UAESMMale  011 2YesNo              DSL        ⋯Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    622154312-GVYNHFemale02170No No phone serviceDSL        ⋯Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\nBut you should check factor to numbers converting to synchronize train/test/prediction correct numbering. In this case you should save somewhere levels of factor. Better way is to use encoders with fitting.\n\n\n\n25.4.1.3.3 Encoding with LabelEncoder from superml package\n\n# install.packages(\"superml\") #you olso need R6 package\n\nalso installing the dependency 'BH'\n\n\n\n\npackage 'BH' successfully unpacked and MD5 sums checked\npackage 'superml' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    D:\\Temp\\Rtmpum9OKD\\downloaded_packages\n\n\n\nlibrary(superml)\n\nlabel <- LabelEncoder$new() # create new encoder\nprint(label$fit(churn_data$InternetService)) # fir the encoder\n\nLoading required package: R6\n\n\n\n[1] TRUE\n\n\n\n# encode data\nchurn_data$InternetService <- label$fit_transform(churn_data$InternetService) \n# do not re-run it, because InternetService ineteger for now\nhead(churn_data)\n\n\n\nA data.frame: 6 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><int><int><int><chr><chr><dbl>...<chr><chr><chr><chr><chr><chr><chr><chr><dbl><chr>\n\n\n    118697010-BRBUUMale  02272YesYes             0...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No \n    245289688-YGXVRFemale01144YesNo              1...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    363449286-DOJGFFemale12138YesYes             1...No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    467396994-KERXLMale  011 4YesNo              2...No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No \n    5 4322181-UAESMMale  011 2YesNo              2...Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    622154312-GVYNHFemale02170No No phone service2...Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\nNow you can save your encoding configuration and use it for test/prediction sets later.\n\n\n\n\n25.4.1.4 On-hot encoding\n\n# Lets use client churn dataset from telco: https://www.kaggle.com/blastchar/telco-customer-churn\nchurn_data <- read.csv(\"data/telecom_users.csv\")\nhead(churn_data)\n\n\n\nA data.frame: 6 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService⋯DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>⋯<chr><chr><chr><chr><chr><chr><chr><chr><dbl><chr>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ⋯No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No \n    245289688-YGXVRFemale0No No 44YesNo              Fiber optic⋯Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    363449286-DOJGFFemale1YesNo 38YesYes             Fiber optic⋯No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    467396994-KERXLMale  0No No  4YesNo              DSL        ⋯No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No \n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ⋯Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    622154312-GVYNHFemale0YesNo 70No No phone serviceDSL        ⋯Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\n\n# check possible Gender values\ngmodels::CrossTable(churn_data$gender)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  5986 \n\n \n          |    Female |      Male | \n          |-----------|-----------|\n          |      2936 |      3050 | \n          |     0.490 |     0.510 | \n          |-----------|-----------|\n\n\n\n \n\n\nLets create two additional variables Male and Female encoded by 1 / 0:\n\nCreate a Male column that encodes each row corresponding to male as 1 and set everything else to 0.\nCreate a Female column that encodes each row corresponding to female as 1 and set everything else to 0.\n\n\n# if gender is factor\n#churn_data <- churn_data %>% \n   #     mutate(gender = as.character(gender)) # convert Gender to character from Factor\n\nchurn_data <- churn_data %>% \n        mutate(\n            # create Male column\n            Male = ifelse(gender == \"Male\", 1, 0),\n            # create Female column\n            Female = ifelse(gender == \"Female\", 1, 0))\n\nhead(churn_data)\n\n\n\nA data.frame: 6 × 24\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService⋯StreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurnMaleFemale\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>⋯<chr><chr><chr><chr><chr><chr><dbl><chr><dbl><dbl>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ⋯No internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No 10\n    245289688-YGXVRFemale0No No 44YesNo              Fiber optic⋯Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No 01\n    363449286-DOJGFFemale1YesNo 38YesYes             Fiber optic⋯No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes01\n    467396994-KERXLMale  0No No  4YesNo              DSL        ⋯No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No 10\n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ⋯No                 No                 Month-to-monthNo Electronic check         53.45 119.50No 10\n    622154312-GVYNHFemale0YesNo 70No No phone serviceDSL        ⋯No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No 01\n\n\n\n\nLets create a dummy variables for InternetService column. So, what data it has for now?\n\ngmodels::CrossTable(churn_data$InternetService)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  5986 \n\n \n            |         DSL | Fiber optic |          No | \n            |-------------|-------------|-------------|\n            |        2068 |        2627 |        1291 | \n            |       0.345 |       0.439 |       0.216 | \n            |-------------|-------------|-------------|\n\n\n\n \n\n\nThe function dummyVars() from caret package can be used to generate a complete (less than full rank parameterized) set of dummy variables from one or more factors. The function takes a formula and a data set and outputs an object that can be used to create the dummy variables using the predict method.\n\nlibrary(caret)\n\nchurn_data <- read.csv(\"data/telecom_users.csv\")\n\ndummy <- dummyVars(\" ~ InternetService\", data = churn_data)\nnew_df <- data.frame(predict(dummy, newdata = churn_data)) # precit dummy variables and\nnew_df %>% head()\n\nLoading required package: lattice\n\n\n\n\n\nA data.frame: 6 × 3\n\n    InternetServiceDSLInternetServiceFiber.opticInternetServiceNo\n    <dbl><dbl><dbl>\n\n\n    1001\n    2010\n    3010\n    4100\n    5100\n    6100\n\n\n\n\nIf you want create dummy variables for all categoriacal columns just use \" ~ .\" formula.\n\ndummy <- dummyVars(\" ~ .\", data = churn_data)\nnew_df <- data.frame(predict(dummy, newdata = churn_data)) # precit dummy variables and\nnew_df %>% head()\n\n\n\nA data.frame: 6 × 7560\n\n    XcustomerID0002.ORFBOcustomerID0003.MKNFEcustomerID0004.TLHLJcustomerID0011.IGKFFcustomerID0013.EXCHZcustomerID0013.MHZWFcustomerID0013.SMEOEcustomerID0014.BMAQUcustomerID0015.UOCOJ⋯MonthlyCharges99.7MonthlyCharges99.75MonthlyCharges99.8MonthlyCharges99.85MonthlyCharges99.9MonthlyCharges99.95MonthlyChargesNULLTotalChargesChurnNoChurnYes\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>⋯<dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    11869000000000⋯00000001734.6510\n    24528000000000⋯00000003973.2010\n    36344000000000⋯00000002869.8501\n    46739000000000⋯0000000 238.5010\n    5 432000000000⋯0000000 119.5010\n    62215000000000⋯00000003370.2010\n\n\n\n\nYou can remove some variables, for example Churn, becouse its target variable\n\ndummy <- dummyVars(\" ~. -Churn\", data = churn_data)\nnew_df <- data.frame(predict(dummy, newdata = churn_data)) # precit dummy variables and\nnew_df %>% head()\n\n\n\nA data.frame: 6 × 7558\n\n    XcustomerID0002.ORFBOcustomerID0003.MKNFEcustomerID0004.TLHLJcustomerID0011.IGKFFcustomerID0013.EXCHZcustomerID0013.MHZWFcustomerID0013.SMEOEcustomerID0014.BMAQUcustomerID0015.UOCOJ...MonthlyCharges99.6MonthlyCharges99.65MonthlyCharges99.7MonthlyCharges99.75MonthlyCharges99.8MonthlyCharges99.85MonthlyCharges99.9MonthlyCharges99.95MonthlyChargesNULLTotalCharges\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>...<dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    11869000000000...0000000001734.65\n    24528000000000...0000000003973.20\n    36344000000000...0000000002869.85\n    46739000000000...000000000 238.50\n    5 432000000000...000000000 119.50\n    62215000000000...0000000003370.20\n\n\n\n\nIf you need some few do this with + operator:\n\ndummy <- dummyVars(\" ~ InternetService + PhoneService\", data = churn_data)\nnew_df <- data.frame(predict(dummy, newdata = churn_data)) # precit dummy variables and\nnew_df %>% head()\n\n\n\nA data.frame: 6 × 5\n\n    InternetServiceDSLInternetServiceFiber.opticInternetServiceNoPhoneServiceNoPhoneServiceYes\n    <dbl><dbl><dbl><dbl><dbl>\n\n\n    100101\n    201001\n    301001\n    410001\n    510001\n    610010\n\n\n\n\n\n\n\n25.4.2 Evaluate data on raw dataset\nPrevious data transformation approaches use prepared datasets. But sometimes you should create new variables from raw data than cannot be attached to you existing dataset and implemented as-is for modeling.\nLest check an example with customer transactions data.\n\nPreview data file in excel before read it to R!\n\n\n# read transaction data from dataset\n# explore file in Excel before reading to check sheet numbers and tables structure\nlibrary(xlsx)\ndemographics <- read.xlsx(\"data/transactions.xlsx\", sheetIndex = 1)\nhead(demographics)\n\njava.home option: \n\nJAVA_HOME environment variable: E:\\mlSOftware\\Anaconda3\\Library\\lib\\jvm\n\n\n\n\n\nA data.frame: 4 × 4\n\n    CustomerIDGenderEmailVisitsLastYear\n    <dbl><chr><chr><dbl>\n\n\n    1787987456Male  Yes 12\n    2456415151Male  NULL 0\n    3215454555FemaleNo  16\n    4985121122FemaleNo   4\n\n\n\n\n\n# read transactions\ntransactions <- read.xlsx(\"data/transactions.xlsx\", sheetIndex = 2)\nhead(transactions)\n\n\n\nA data.frame: 6 × 7\n\n    CustomerIDContractIDDateTimeContractorIDUsdEquiv_sumType\n    <dbl><dbl><date><dbl><chr><dbl><chr>\n\n\n    1215454555190657982019-03-15114537NULL  6293.7111credet\n    2215454555190657982019-04-05102525NULL   914.9459credet\n    3215454555190657982019-05-11 80833NULL -4655.9111debet \n    4215454555190657982019-05-16 74606NULL-13900.6889debet \n    5215454555190657982019-05-30104506NULL  2102.0570credet\n    6215454555190657982019-08-23122656NULL-16244.2333debet \n\n\n\n\nOur nex task is to construct new features based on transactions history:\n\nChurn - target parameter if client has more than 60 days absent transactions between 2021-04-30 and last transaction date.\nAverageContractSum3M_Credet - average contract sum by credet last 3 month before event Churn == 1/0\nAverageIncrease3M_Credet - average sum increase from month to month, for last 3 month, by credet\n\n\n# lets find a maximum transaction date for each client\nlibrary(tidyverse) # includes magrittr, dplyr\nmax_dates <- transactions %>%\n        group_by(CustomerID) %>%\n        summarise(MaxDate = max(Date))\nmax_dates\n\n-- Attaching packages ------------------------------------------------------------------------------- tidyverse 1.3.2 --\nv tibble  3.1.8     v purrr   0.3.5\nv tidyr   1.2.1     v stringr 1.4.1\nv readr   2.1.3     v forcats 0.5.2\n-- Conflicts ---------------------------------------------------------------------------------- tidyverse_conflicts() --\nx gridExtra::combine() masks dplyr::combine()\nx tidyr::extract()     masks magrittr::extract()\nx dplyr::filter()      masks stats::filter()\nx dplyr::lag()         masks stats::lag()\nx purrr::lift()        masks caret::lift()\nx purrr::set_names()   masks magrittr::set_names()\n\n\n\n\nA tibble: 4 × 2\n\n    CustomerIDMaxDate\n    <dbl><date>\n\n\n    2154545552021-04-30\n    4564151512021-01-22\n    7879874562019-12-06\n    9851211222020-09-09\n\n\n\n\n\nlibrary(lubridate) #for datetime manipulation\ncurrent_date <- ymd(\"2021-04-30\")\nmax_dates <- max_dates %>%\n        mutate(DaysDiff = as.period(current_date - MaxDate) %>% day()) # find period and convert it to days\nmax_dates\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n\n\n\n\nA tibble: 4 × 3\n\n    CustomerIDMaxDateDaysDiff\n    <dbl><date><dbl>\n\n\n    2154545552021-04-30  0\n    4564151512021-01-22 98\n    7879874562019-12-06511\n    9851211222020-09-09233\n\n\n\n\n\n# lests calculate Churn feature\nmax_dates <- max_dates %>%\n        mutate(Churn = ifelse(DaysDiff > 60, 1, 0))\nmax_dates\n\n\n\nA tibble: 4 × 4\n\n    CustomerIDMaxDateDaysDiffChurn\n    <dbl><date><dbl><dbl>\n\n\n    2154545552021-04-30  00\n    4564151512021-01-22 981\n    7879874562019-12-065111\n    9851211222020-09-092331\n\n\n\n\nYou can finally merge code into one query:\n\nchurn_eval <- transactions %>%\n        group_by(CustomerID) %>%\n        summarise(MaxDate = max(Date)) %>%\n        mutate(DaysDiff = as.period(current_date - MaxDate) %>% day(),\n               Churn = ifelse(DaysDiff > 60, 1, 0)) %>%\n        select(CustomerID, Churn) # Select only CustomerID and new feature / target\nchurn_eval # we will merge it with demographics later\n\n\n\nA tibble: 4 × 2\n\n    CustomerIDChurn\n    <dbl><dbl>\n\n\n    2154545550\n    4564151511\n    7879874561\n    9851211221\n\n\n\n\n\n25.4.2.1 Task. AverageContractSum3M_Credet\nTASK: calculate average sum of contract for each customer by credet for last 3 month before Churn/NotChurn event\nFor calculating this data wee should do this steps:\n\nFind max transaction date\nFind transaction date 3 month ago (max transaction date - 3 month)\nFilter only records in range (-3 month, max_transaction)\nGroup data by contracts and find every contract sum\nGroup data by customers and find every customer average\n\n\navgContractSum3m <- transactions %>%\n        filter(Type == \"credet\") %>% # only credet\n        group_by(CustomerID) %>% \n        mutate(MaxDate = max(Date), # get max transaction date\n               Month3Date = MaxDate %m+% months(-3)) %>% # get date 3 month before max transaction date\n       filter(Date >= Month3Date) %>% # only transaction more than 3 month age left\n       group_by(CustomerID, ContractID) %>% # group by customer and contract\n       summarize(ContractSum = sum(UsdEquiv_sum), .groups = 'drop') %>% # find sum by each contract\n       group_by(CustomerID) %>% # group by customer\n       summarise(AverageContractSum3M_Credet = mean(ContractSum)) # find mean by each customer\n\navgContractSum3m\n\n\n\nA tibble: 4 × 2\n\n    CustomerIDAverageContractSum3M_Credet\n    <dbl><dbl>\n\n\n    21545455545541.3335\n    456415151 1271.5930\n    787987456  116.6667\n    985121122 4444.4444\n\n\n\n\n\n\n25.4.2.2 Task. Find last 3 average credet sum increase\n\nGet dates range for -1, -2, -3 monthes by credet:\n\n\nmonthByMonth <- transactions %>%\n        filter(Type == \"credet\") %>%\n        select(CustomerID, Date, UsdEquiv_sum) %>%\n        group_by(CustomerID) %>%\n        mutate(MaxDate = max(Date),\n               Month1Before = MaxDate %m+% months(-1),\n               Month2Before = MaxDate %m+% months(-2),\n               Month3Before = MaxDate %m+% months(-3)) \nhead(monthByMonth)\n\n\n\nA grouped_df: 6 × 7\n\n    CustomerIDDateUsdEquiv_sumMaxDateMonth1BeforeMonth2BeforeMonth3Before\n    <dbl><date><dbl><date><date><date><date>\n\n\n    2154545552019-03-15 6293.71112021-04-302021-03-302021-02-282021-01-30\n    2154545552019-04-05  914.94592021-04-302021-03-302021-02-282021-01-30\n    2154545552019-05-30 2102.05702021-04-302021-03-302021-02-282021-01-30\n    2154545552019-10-0910854.76302021-04-302021-03-302021-02-282021-01-30\n    2154545552019-11-27  639.53892021-04-302021-03-302021-02-282021-01-30\n    2154545552019-12-20 2654.94592021-04-302021-03-302021-02-282021-01-30\n\n\n\n\n\nBig guery calculate sums for each month and find increases\n\n\ncredetInc <- monthByMonth %>%\n    filter(Date >= Month1Before & Date <= MaxDate) %>% # data in range from Month1Before to MaxDate\n    group_by(CustomerID) %>%\n    summarise(M1Sum = sum(UsdEquiv_sum)) %>% # sum for month -1\n    left_join(monthByMonth %>% # join with data about month -2\n              filter(Date >= Month2Before & Date <= Month1Before) %>% # data in range from Month2Before to Month1Before\n              group_by(CustomerID) %>%\n              summarise(M2Sum = sum(UsdEquiv_sum)), by = \"CustomerID\") %>%\n            left_join(monthByMonth %>% # join with data about month -3\n              filter(Date >= Month3Before & Date <= Month2Before) %>% # data in range from Month3Before to Month2Before\n              group_by(CustomerID) %>%\n              summarise(M3Sum = sum(UsdEquiv_sum)), by = \"CustomerID\") %>%\n    rowwise %>% # calculate separately for each row, not for columns\n    mutate(AverageIncrease3M_Credet = mean(c(M1Sum/M2Sum, M2Sum/M3Sum), na.rm = T)) %>% # increase is an average of sum changinf from month to month\n    select(CustomerID, AverageIncrease3M_Credet)\n        \nhead(credetInc)\n\n\n\nA rowwise_df: 4 × 2\n\n    CustomerIDAverageIncrease3M_Credet\n    <dbl><dbl>\n\n\n    2154545558.3372521811\n    456415151         NaN\n    7879874560.0003175611\n    985121122         NaN\n\n\n\n\nJoin all the tables\n\nfinal_set <- demographics %>% \n        left_join(avgContractSum3m , by = \"CustomerID\") %>%\n        left_join(credetInc , by = \"CustomerID\") %>%\n        left_join(churn_eval, by = \"CustomerID\")\nfinal_set\n\n\n\nA data.frame: 4 × 7\n\n    CustomerIDGenderEmailVisitsLastYearAverageContractSum3M_CredetAverageIncrease3M_CredetChurn\n    <dbl><chr><chr><dbl><dbl><dbl><dbl>\n\n\n    787987456Male  Yes 12  116.66670.00031756111\n    456415151Male  NULL 0 1271.5930         NaN1\n    215454555FemaleNo  1645541.33358.33725218110\n    985121122FemaleNo   4 4444.4444         NaN1\n\n\n\n\nOn the next stages this data can be transformed with scaling, encoding, binning."
  },
  {
    "objectID": "etl-feature-engineering.html#references",
    "href": "etl-feature-engineering.html#references",
    "title": "25  Feature engineering in R",
    "section": "25.5 References",
    "text": "25.5 References\n\nFeature Engineering in R Programming by (dhruv5819?)\nWhat is Feature Engineering? by Tim Bok\nFeature Scaling-Why it is required? by Rahul Saini\nFeature Scaling for Machine Learning: Understanding the Difference Between Normalization vs. Standardization by ANIRUDDHA BHANDARI\nR Package ‘smbinning’: Optimal Binning for Scoring Modeling by Herman Jopia\nGarcia, S. et al (2013) A Survey of Discretization Techniques: Taxonomy and Empirical Analysis in Supervised Learning. IEEE Transactions on Knowledge and Data Engineering, Vol. 25, No. 4, April 2013.\nAn Overview on the Landscape of R Packages for Credit Scoring by Gero Szepannek"
  },
  {
    "objectID": "train-test-validation-samples.html",
    "href": "train-test-validation-samples.html",
    "title": "26  Data Split: Train, Test and Validation sets",
    "section": "",
    "text": "You need this packages for code execution:"
  },
  {
    "objectID": "train-test-validation-samples.html#whats-train-validation-test-datasets",
    "href": "train-test-validation-samples.html#whats-train-validation-test-datasets",
    "title": "26  Data Split: Train, Test and Validation sets",
    "section": "26.1 What’s Train, Validation, Test datasets",
    "text": "26.1 What’s Train, Validation, Test datasets\nBefore model fitting and some stages of features engeniering we shoudl split out dataset on 2 or 3 parts:\n\nTraining dataset: The sample of data used to fit the model.\n\nThe model sees and learns from this data.\n\nValidation dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n\nThe validation set is used to evaluate a given model, but this is for frequent evaluation. We, as machine learning engineers, use this data to fine-tune the model hyperparameters. Hence the model occasionally sees this data, but never does it “Learn” from this. We use the validation set results, and update higher level hyperparameters. So the validation set affects a model, but only indirectly. The validation set is also known as the Dev set or the Development set. This makes sense since this dataset helps during the “development” stage of the model.\n\nTesting dataset: Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n\nThe Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained(using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner). Many a times the validation set is used as the test set, but it is not good practice. The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world.\n\nYou can also find papers with splitting only for train/test. In this case test means validation."
  },
  {
    "objectID": "train-test-validation-samples.html#splitting-data-in-r",
    "href": "train-test-validation-samples.html#splitting-data-in-r",
    "title": "26  Data Split: Train, Test and Validation sets",
    "section": "26.2 Splitting data in R",
    "text": "26.2 Splitting data in R\nLets describe some conditions before start studiyng splitting data functions in R:\n\nWe will use same seed for all splittings to control results reproduction, for example, let it be 2021.\nWe will use dataset for client churn prediction Telco Customer Churn: https://www.kaggle.com/blastchar/telco-customer-churn\n\nShort dataset description:\n\ncustomerID - Customer ID\ngender Whether the customer is a male or a female\nSeniorCitizen - Whether the customer is a senior citizen or not (1, 0)\nPartner - Whether the customer has a partner or not (Yes, No)\nDependents - Whether the customer has dependents or not (Yes, No)\ntenure - Number of months the customer has stayed with the company\nPhoneService - Whether the customer has a phone service or not (Yes, No)\nMultipleLines - Whether the customer has multiple lines or not (Yes, No, No phone service)\nInternetService - Customer’s internet service provider (DSL, Fiber optic, No)\nOnlineSecurity - Whether the customer has online security or not (Yes, No, No internet service)\nOnlineBackup - Whether the customer has online backup or not (Yes, No, No internet service)\nDeviceProtection - Whether the customer has device protection or not (Yes, No, No internet service)\nTechSupport - Whether the customer has tech support or not (Yes, No, No internet service)\nStreamingTV - Whether the customer has streaming TV or not (Yes, No, No internet service)\nStreamingMovies - Whether the customer has streaming movies or not (Yes, No, No internet service)\nContract - The contract term of the customer (Month-to-month, One year, Two year)\nPaperlessBilling - Whether the customer has paperless billing or not (Yes, No)\nPaymentMethod - The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\nMonthlyCharges - The amount charged to the customer monthly\nTotalCharges - The total amount charged to the customer\nChurn - Whether the customer churned or not (Yes or No)\n\n\n# read data\ntelecom_users <- read.csv(\"data/telecom_users.csv\")\nhead(telecom_users)\n\n\n\nA data.frame: 6 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService⋯DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>⋯<chr><chr><chr><chr><chr><chr><chr><chr><dbl><chr>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ⋯No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No \n    245289688-YGXVRFemale0No No 44YesNo              Fiber optic⋯Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    363449286-DOJGFFemale1YesNo 38YesYes             Fiber optic⋯No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    467396994-KERXLMale  0No No  4YesNo              DSL        ⋯No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No \n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ⋯Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    622154312-GVYNHFemale0YesNo 70No No phone serviceDSL        ⋯Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\nLets check the proportion of column Churn == Yes and Churn == No in dataset with CrossTable() function from gmodels package.\n\n# install.packages(\"gmodels\")\n\n\nlibrary(gmodels)\nCrossTable(telecom_users$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  5986 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      4399 |      1587 | \n          |     0.735 |     0.265 | \n          |-----------|-----------|\n\n\n\n \n\n\nYou can also use CrossTable() to check cross proportions by other fields. Lets check crosstable for TechSupport and Churn:\n\nCrossTable(telecom_users$Churn, telecom_users$TechSupport) # for example\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  5986 \n\n \n                    | telecom_users$TechSupport \ntelecom_users$Churn |                  No | No internet service |                 Yes |           Row Total | \n--------------------|---------------------|---------------------|---------------------|---------------------|\n                 No |                1738 |                1192 |                1469 |                4399 | \n                    |              87.892 |              62.377 |              29.512 |                     | \n                    |               0.395 |               0.271 |               0.334 |               0.735 | \n                    |               0.587 |               0.923 |               0.847 |                     | \n                    |               0.290 |               0.199 |               0.245 |                     | \n--------------------|---------------------|---------------------|---------------------|---------------------|\n                Yes |                1222 |                  99 |                 266 |                1587 | \n                    |             243.627 |             172.904 |              81.805 |                     | \n                    |               0.770 |               0.062 |               0.168 |               0.265 | \n                    |               0.413 |               0.077 |               0.153 |                     | \n                    |               0.204 |               0.017 |               0.044 |                     | \n--------------------|---------------------|---------------------|---------------------|---------------------|\n       Column Total |                2960 |                1291 |                1735 |                5986 | \n                    |               0.494 |               0.216 |               0.290 |                     | \n--------------------|---------------------|---------------------|---------------------|---------------------|\n\n \n\n\nYou can see that most part of Churn 1222 of 1587\nNext, we will check 6 possible ways to split data for train/test sets.\n\n\n26.2.1 Split with sample()\n\nsample_size = round(nrow(telecom_users)*.70) # setting what is 70%\nprint(paste0(\"Size: \", sample_size))\n\nindex <- sample(nrow(telecom_users), size = sample_size)\n \ntrain <- telecom_users[index, ] # index is numbers of selected rows from dataset\ntest <-telecom_users[-index, ] # -index select only rows not in index\n\n[1] \"Size: 4190\"\n\n\n\n# check Churn == Yes/No proportion in train/test\nCrossTable(train$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  4190 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      3063 |      1127 | \n          |     0.731 |     0.269 | \n          |-----------|-----------|\n\n\n\n \n\n\n\n# check Churn == Yes/No proportion in train/test\nCrossTable(test$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  1796 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      1336 |       460 | \n          |     0.744 |     0.256 | \n          |-----------|-----------|\n\n\n\n \n\n\nIts 0.260 for train and 0.276 for test. Diffrence is 1,6%, so, its close.\n\n\n\n26.2.2 Split with sample_frac from dplyr\n\nlibrary(dplyr)\nset.seed(2022)\n\n# Using the above function to create 70 - 30 slipt into test and train\n\ntu <- telecom_users %>% mutate(Id = row_number())\n\ntrain <- tu %>% sample_frac(.70)\ntest <- tu[-train$Id, ]\n\n\nnrow(train)\n\n4190\n\n\n\n# check Churn == Yes/No proportion in train/test\nCrossTable(train$Churn)\nCrossTable(test$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  4190 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      3103 |      1087 | \n          |     0.741 |     0.259 | \n          |-----------|-----------|\n\n\n\n \n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  1796 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      1296 |       500 | \n          |     0.722 |     0.278 | \n          |-----------|-----------|\n\n\n\n \n\n\nsample_n made other proportion of Churn == Yes/No and difference just 0.7%.\n\n\n\n26.2.3 Split with createDataPartition() from caret\n\n#install.packages(\"caret\")\n\nUpdating HTML index of packages in '.Library'\n\nMaking 'packages.html' ...\n done\n\n\n\n\nlibrary(caret)\nset.seed(2021)\n \nindex = createDataPartition(telecom_users$Churn, p = 0.70, list = FALSE)\ntrain = telecom_users[index, ]\ntest = telecom_users[-index, ]\n\n\n# check Churn == Yes/No proportion in train/test\nCrossTable(train$Churn)\nCrossTable(test$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  4191 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      3080 |      1111 | \n          |     0.735 |     0.265 | \n          |-----------|-----------|\n\n\n\n \n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  1795 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      1319 |       476 | \n          |     0.735 |     0.265 | \n          |-----------|-----------|\n\n\n\n \n\n\nCkeck the proportion of target variable. Caret trying to make the same split for both train and test. This is one of the best split methods in R.\n\n\n\n26.2.4 Split with sample.split from caTools\n\n#install.packages(\"caTools\")\n\n\nlibrary(caTools)\n \nset.seed(2022)\nsample = sample.split(telecom_users$Churn, SplitRatio = .70)\n\ntrain = telecom_users[sample, ]\ntest  = telecom_users[!sample, ]\n\n\n# check Churn == Yes/No proportion in train/test\nCrossTable(train$Churn)\nCrossTable(test$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  4190 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      3079 |      1111 | \n          |     0.735 |     0.265 | \n          |-----------|-----------|\n\n\n\n \n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  1796 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      1320 |       476 | \n          |     0.735 |     0.265 | \n          |-----------|-----------|\n\n\n\n \n\n\n\n\nДля нашого курсу це не потрібно поки! Переходимо до наступної теми. Цей матеріал в детелях буде розглянуто під час вивчення крос-валідації."
  },
  {
    "objectID": "train-test-validation-samples.html#splitting-for-n-folds",
    "href": "train-test-validation-samples.html#splitting-for-n-folds",
    "title": "26  Data Split: Train, Test and Validation sets",
    "section": "26.3 Splitting for n-folds",
    "text": "26.3 Splitting for n-folds\n\n# read data again\nlibrary(caret)\ntelecom_users <- read.csv(\"../../data/telecom_users.csv\")\nnrow(telecom_users)\nhead(telecom_users)\n\n5986\n\n\n\n\nA data.frame: 6 × 22\n\n    XcustomerIDgenderSeniorCitizenPartnerDependentstenurePhoneServiceMultipleLinesInternetService...DeviceProtectionTechSupportStreamingTVStreamingMoviesContractPaperlessBillingPaymentMethodMonthlyChargesTotalChargesChurn\n    <int><chr><chr><int><chr><chr><int><chr><chr><chr>...<chr><chr><chr><chr><chr><chr><chr><chr><dbl><chr>\n\n\n    118697010-BRBUUMale  0YesYes72YesYes             No         ...No internet serviceNo internet serviceNo internet serviceNo internet serviceTwo year      No Credit card (automatic)  24.1 1734.65No \n    245289688-YGXVRFemale0No No 44YesNo              Fiber optic...Yes                No                 Yes                No                 Month-to-monthYesCredit card (automatic)  88.153973.20No \n    363449286-DOJGFFemale1YesNo 38YesYes             Fiber optic...No                 No                 No                 No                 Month-to-monthYesBank transfer (automatic)74.952869.85Yes\n    467396994-KERXLMale  0No No  4YesNo              DSL        ...No                 No                 No                 Yes                Month-to-monthYesElectronic check         55.9  238.50No \n    5 4322181-UAESMMale  0No No  2YesNo              DSL        ...Yes                No                 No                 No                 Month-to-monthNo Electronic check         53.45 119.50No \n    622154312-GVYNHFemale0YesNo 70No No phone serviceDSL        ...Yes                Yes                No                 Yes                Two year      YesBank transfer (automatic)49.853370.20No \n\n\n\n\n\nfolds <- createFolds(telecom_users)\nfolds\n\n\n    $Fold01\n        \n110\n\n    $Fold02\n        \n3415\n\n    $Fold03\n        \n217\n\n    $Fold04\n        8\n    $Fold05\n        \n1622\n\n    $Fold06\n        \n918\n\n    $Fold07\n        \n1920\n\n    $Fold08\n        \n51214\n\n    $Fold09\n        11\n    $Fold10\n        \n671321\n\n\n\n\n\n#library(caret)\n#library(mlbench)\n#data(Sonar)\n \n#folds <- createFolds(Sonar$Class)\n#str(folds)"
  },
  {
    "objectID": "train-test-validation-samples.html#references",
    "href": "train-test-validation-samples.html#references",
    "title": "26  Data Split: Train, Test and Validation sets",
    "section": "26.4 References",
    "text": "26.4 References\n\nAbout Train, Validation and Test Sets in Machine Learning by Tarang Shah. Url: https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7"
  }
]