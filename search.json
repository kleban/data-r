[
  {
    "objectID": "22-r-csv.html",
    "href": "22-r-csv.html",
    "title": "3  CSV",
    "section": "",
    "text": "3.1 What is CSV (Comma Separated Values)?\nCSV - comma separated values.\n# lets check current working directory to write correct files path\ngetwd()\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en'\nYou can use / or \\\\ for writing correct path in R. For example:\npath = \"d:/projects/file.csv\"\npath = \"d:\\\\projects\\\\file.csv\"\nTo combine path use paste() or paste0() functions\nwork_dir = getwd()\nwork_dir \n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en'\nfile_name = \"temp_file.csv\"\nfile_path = paste0(work_dir, \"/\", file_name)\nfile_path\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en/temp_file.csv'\nfile_path = paste(work_dir, file_name, sep = \"/\")\nfile_path\n\n'E:/Repos/Season 2022/r-book/_book/docs/data-analysis-en/temp_file.csv'",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CSV</span>"
    ]
  },
  {
    "objectID": "22-r-csv.html#sample-dataset-description",
    "href": "22-r-csv.html#sample-dataset-description",
    "title": "3  CSV",
    "section": "3.2 Sample dataset description",
    "text": "3.2 Sample dataset description\nInformation about dataset from kaggle.com. Original file located at url: https://www.kaggle.com/radmirzosimov/telecom-users-dataset.\nAny business wants to maximize the number of customers. To achieve this goal, it is important not only to try to attract new ones, but also to retain existing ones. Retaining a client will cost the company less than attracting a new one. In addition, a new client may be weakly interested in business services and it will be difficult to work with him, while old clients already have the necessary data on interaction with the service.\nAccordingly, predicting the churn, we can react in time and try to keep the client who wants to leave. Based on the data about the services that the client uses, we can make him a special offer, trying to change his decision to leave the operator. This will make the task of retention easier to implement than the task of attracting new users, about which we do not know anything yet.\nYou are provided with a dataset from a telecommunications company. The data contains information about almost six thousand users, their demographic characteristics, the services they use, the duration of using the operator’s services, the method of payment, and the amount of payment.\nThe task is to analyze the data and predict the churn of users (to identify people who will and will not renew their contract). The work should include the following mandatory items:\n\nDescription of the data (with the calculation of basic statistics);\nResearch of dependencies and formulation of hypotheses;\nBuilding models for predicting the outflow (with justification for the choice of a particular model) 4. based on tested hypotheses and identified relationships;\nComparison of the quality of the obtained models.\n\nFields description:\n\ncustomerID - customer id\ngender - client gender (male / female)\nSeniorCitizen - is the client retired (1, 0)\nPartner - is the client married (Yes, No)\ntenure - how many months a person has been a client of the company\nPhoneService - is the telephone service connected (Yes, No)\nMultipleLines - are multiple phone lines connected (Yes, No, No phone service)\nInternetService - client’s Internet service provider (DSL, Fiber optic, No)\nOnlineSecurity - is the online security service connected (Yes, No, No internet service)\nOnlineBackup - is the online backup service activated (Yes, No, No internet service)\nDeviceProtection - does the client have equipment insurance (Yes, No, No internet service)\nTechSupport - is the technical support service connected (Yes, No, No internet service)\nStreamingTV - is the streaming TV service connected (Yes, No, No internet service)\nStreamingMovies - is the streaming cinema service activated (Yes, No, No internet service)\nContract - type of customer contract (Month-to-month, One year, Two year)\nPaperlessBilling - whether the client uses paperless billing (Yes, No)\nPaymentMethod - payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\nMonthlyCharges - current monthly payment\nTotalCharges - the total amount that the client paid for the services for the entire time\nChurn - whether there was a churn (Yes or No)",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CSV</span>"
    ]
  },
  {
    "objectID": "22-r-csv.html#reading",
    "href": "22-r-csv.html#reading",
    "title": "3  CSV",
    "section": "3.3 Reading",
    "text": "3.3 Reading\nThare are few methods for reading/writing csv in base package:\n\nread.csv(), write.csv - default data separator is ,, decimal is separator ..\nread.csv2(), write.csv2 - default data separator is ;, decimal is separator ,.\n\nBefore using any new function check it usage information with help(function_name) or ?function_name, example: ?read.csv.\nYou can read (current data set has NA values as example, there are no NA in original datase):\n\ndata &lt;- read.csv(\"../../data/telecom_users.csv\") # default reading\nstr(data)\n\n'data.frame':   5986 obs. of  22 variables:\n $ X               : int  1869 4528 6344 6739 432 2215 5260 6001 1480 5137 ...\n $ customerID      : chr  \"7010-BRBUU\" \"9688-YGXVR\" \"9286-DOJGF\" \"6994-KERXL\" ...\n $ gender          : chr  \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ SeniorCitizen   : int  0 0 1 0 0 0 0 0 0 1 ...\n $ Partner         : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Dependents      : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n $ tenure          : int  72 44 38 4 2 70 33 1 39 55 ...\n $ PhoneService    : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ MultipleLines   : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ InternetService : chr  \"No\" \"Fiber optic\" \"Fiber optic\" \"DSL\" ...\n $ OnlineSecurity  : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ OnlineBackup    : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ DeviceProtection: chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ TechSupport     : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ StreamingTV     : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ StreamingMovies : chr  \"No internet service\" \"No\" \"No\" \"Yes\" ...\n $ Contract        : chr  \"Two year\" \"Month-to-month\" \"Month-to-month\" \"Month-to-month\" ...\n $ PaperlessBilling: chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ PaymentMethod   : chr  \"Credit card (automatic)\" \"Credit card (automatic)\" \"Bank transfer (automatic)\" \"Electronic check\" ...\n $ MonthlyCharges  : chr  \"24.1\" \"88.15\" \"74.95\" \"55.9\" ...\n $ TotalCharges    : num  1735 3973 2870 238 120 ...\n $ Churn           : chr  \"No\" \"No\" \"Yes\" \"No\" ...\n\n\n\ndata &lt;- read.csv(\"../../data/telecom_users.csv\",\n                  sep = \",\", # comma not only possibel separator\n                  dec = \".\", # decimal separator can be different\n                  na.strings = c(\"\", \"NA\", \"NULL\")) # you can define NA values\n\n\nstr(data) # chack data structure / types/ values\n\n'data.frame':   5986 obs. of  22 variables:\n $ X               : int  1869 4528 6344 6739 432 2215 5260 6001 1480 5137 ...\n $ customerID      : chr  \"7010-BRBUU\" \"9688-YGXVR\" \"9286-DOJGF\" \"6994-KERXL\" ...\n $ gender          : chr  \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ SeniorCitizen   : int  0 0 1 0 0 0 0 0 0 1 ...\n $ Partner         : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Dependents      : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n $ tenure          : int  72 44 38 4 2 70 33 1 39 55 ...\n $ PhoneService    : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ MultipleLines   : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ InternetService : chr  \"No\" \"Fiber optic\" \"Fiber optic\" \"DSL\" ...\n $ OnlineSecurity  : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ OnlineBackup    : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ DeviceProtection: chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ TechSupport     : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ StreamingTV     : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ StreamingMovies : chr  \"No internet service\" \"No\" \"No\" \"Yes\" ...\n $ Contract        : chr  \"Two year\" \"Month-to-month\" \"Month-to-month\" \"Month-to-month\" ...\n $ PaperlessBilling: chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ PaymentMethod   : chr  \"Credit card (automatic)\" \"Credit card (automatic)\" \"Bank transfer (automatic)\" \"Electronic check\" ...\n $ MonthlyCharges  : num  24.1 88.2 75 55.9 53.5 ...\n $ TotalCharges    : num  1735 3973 2870 238 120 ...\n $ Churn           : chr  \"No\" \"No\" \"Yes\" \"No\" ...\n\n\n\nhead(data, 2) # top 6 rows, use n = X, for viewing top X lines\n\n\nA data.frame: 2 × 22\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.10\n1734.65\nNo\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\n...\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n\n\n\n\nis.data.frame(data) # if data is data.frame\n\nTRUE\n\n\n\nanyNA(data) # if dataframe contains any NA values\n\nTRUE\n\n\n\nlapply(data, anyNA)\n#lapply(, any) #check NA by 2nd dimension - columns\n\n\n    $X\n        FALSE\n    $customerID\n        FALSE\n    $gender\n        FALSE\n    $SeniorCitizen\n        FALSE\n    $Partner\n        FALSE\n    $Dependents\n        FALSE\n    $tenure\n        FALSE\n    $PhoneService\n        FALSE\n    $MultipleLines\n        FALSE\n    $InternetService\n        FALSE\n    $OnlineSecurity\n        FALSE\n    $OnlineBackup\n        FALSE\n    $DeviceProtection\n        FALSE\n    $TechSupport\n        FALSE\n    $StreamingTV\n        FALSE\n    $StreamingMovies\n        FALSE\n    $Contract\n        FALSE\n    $PaperlessBilling\n        FALSE\n    $PaymentMethod\n        FALSE\n    $MonthlyCharges\n        TRUE\n    $TotalCharges\n        TRUE\n    $Churn\n        FALSE\n\n\n\nCheck MonthlyCharges: TRUE and TotalCharges: TRUE. These columns has NA-values.\nLet’s replace them with mean:\n\ndata[is.na(data$TotalCharges), \"TotalCharges\"] &lt;- mean(data$TotalCharges, na.rm = T)\ndata[is.na(data$MonthlyCharges), \"MonthlyCharges\"] &lt;- mean(data$MonthlyCharges, na.rm = T)\n\n\nany(is.na(data)) # check for NA\n\nFALSE\n\n\nYou can write data with write.csv(), write.csv2() from base package.\n\nwrite.csv(data, file = \"../../data/cleaned_data.csv\", row.names = F)\n# by default row.names = TRUE and file will contain first column with row numbers 1,2, ..., N\n\nERROR: Error in as.data.frame.default(x[[i]], optional = TRUE): cannot coerce class '\"function\"' to a data.frame\n\nError in as.data.frame.default(x[[i]], optional = TRUE): cannot coerce class '\"function\"' to a data.frame\nTraceback:\n\n1. write.csv(data, file = \"../../data/cleaned_data.csv\", row.names = F)\n2. eval.parent(Call)\n3. eval(expr, p)\n4. eval(expr, p)\n5. utils::write.table(data, file = \"../../data/cleaned_data.csv\", \n .     row.names = F, col.names = TRUE, sep = \",\", dec = \".\", qmethod = \"double\")\n6. data.frame(x)\n7. as.data.frame(x[[i]], optional = TRUE)\n8. as.data.frame.default(x[[i]], optional = TRUE)\n9. stop(gettextf(\"cannot coerce class %s to a data.frame\", sQuote(deparse(class(x))[1L])), \n .     domain = NA)",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CSV</span>"
    ]
  },
  {
    "objectID": "22-r-csv.html#readr-package",
    "href": "22-r-csv.html#readr-package",
    "title": "3  CSV",
    "section": "3.4 readr package",
    "text": "3.4 readr package\nOne more useful package is readr. Examples of using:\n\n# library(readr)\n# data &lt;- read_csv(file = \"../../data/telecom_users.csv\")\n# data &lt;- read_csv2(file = \"../../data/telecom_users.csv\")`",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CSV</span>"
    ]
  },
  {
    "objectID": "22-r-csv.html#набори-даних",
    "href": "22-r-csv.html#набори-даних",
    "title": "3  CSV",
    "section": "3.5 Набори даних",
    "text": "3.5 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CSV</span>"
    ]
  },
  {
    "objectID": "22-r-csv.html#references",
    "href": "22-r-csv.html#references",
    "title": "3  CSV",
    "section": "3.6 References",
    "text": "3.6 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CSV</span>"
    ]
  },
  {
    "objectID": "23-r-xlsx.html",
    "href": "23-r-xlsx.html",
    "title": "4  MS Excel (xlsx)",
    "section": "",
    "text": "4.1 XLSX-format\nThere are many packages to read/write MS Excel files. xlsx one of the most useful.\n# install.packages(\"xlsx\") #install before use it\nlibrary(xlsx)\nany(grepl(\"xlsx\", installed.packages())) # check if package installed\n\nTRUE\n?read.xlsx - review package functions and params\nLet’s read the data telecom_users.xlsx:\ndata &lt;- read.xlsx(\"../../data/telecom_users.xlsx\", sheetIndex = 1)\n# sheetIndex = 1 - select sheet to read, or use sheetName = \"sheet1\" to read by Name\nhead(data)\n\n\nA data.frame: 6 × 21\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\nNo internet service\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.10\n1734.65\nNo\n\n\n2\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\nNo\n...\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n3\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\nNo\n...\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n\n\n4\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\nNo\n...\nNo\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.90\n238.50\nNo\n\n\n5\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\nYes\n...\nYes\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n\n\n6\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\nYes\n...\nYes\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n# You can also use startRow, endRow and other params to define how much data read\ndata &lt;- read.xlsx(\"../../data/telecom_users.xlsx\", sheetIndex = 1, endRow = 100)\nhead(data)\n\n\nA data.frame: 6 × 21\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\nNo internet service\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.10\n1734.65\nNo\n\n\n2\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\nNo\n...\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n3\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\nNo\n...\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n\n\n4\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\nNo\n...\nNo\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.90\n238.50\nNo\n\n\n5\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\nYes\n...\nYes\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n\n\n6\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\nYes\n...\nYes\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\nLet’s replace Churn values Yes/No by 1/0:\nhead(data$Churn)\n\n\n'No''No''Yes''No''No''No'\ndata$Churn &lt;- ifelse(data$Churn == \"Yes\", 1, 0)\nhead(data$Churn)\n\n\n001000\nWrite final data to excel:\nwrite.xlsx(data, file = \"../../data/final_telecom_data.xlsx\")",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MS Excel (xlsx)</span>"
    ]
  },
  {
    "objectID": "23-r-xlsx.html#task-1",
    "href": "23-r-xlsx.html#task-1",
    "title": "4  MS Excel (xlsx)",
    "section": "4.2 Task 1",
    "text": "4.2 Task 1\nDownload from kaggle.com and read dataset Default_Fin.csv: https://www.kaggle.com/kmldas/loan-default-prediction\nDescription:\nThis is a synthetic dataset created using actual data from a financial institution. The data has been modified to remove identifiable features and the numbers transformed to ensure they do not link to original source (financial institution).\nThis is intended to be used for academic purposes for beginners who want to practice financial analytics from a simple financial dataset\n\nIndex - This is the serial number or unique identifier of the loan taker\nEmployed - This is a Boolean 1= employed 0= unemployed\nBank.Balance - Bank Balance of the loan taker\nAnnual.Salary - Annual salary of the loan taker\n\nDefaulted - This is a Boolean 1= defaulted 0= not defaulted\n\n\nCheck what columns has missing values\nCount default and non-default clients / and parts of total clients in %\nCount Employed clients\nCount Employed Default clients\nAverage salary by Employed clients\nRename columns to “id”, “empl”, “balance”, “salary”, “default”\n\n\nSolution for Task 1\n\ndata &lt;- read.csv(\"../../data/Default_Fin.csv\")\nhead(data)\n\n\nA data.frame: 6 × 5\n\n\n\nIndex\nEmployed\nBank.Balance\nAnnual.Salary\nDefaulted.\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n8754.36\n532339.56\n0\n\n\n2\n2\n0\n9806.16\n145273.56\n0\n\n\n3\n3\n1\n12882.60\n381205.68\n0\n\n\n4\n4\n1\n6351.00\n428453.88\n0\n\n\n5\n5\n1\n9427.92\n461562.00\n0\n\n\n6\n6\n0\n11035.08\n89898.72\n0\n\n\n\n\n\n\n\nCheck what columns has missing values\n\n\n\nanyNA(data)\n\nFALSE\n\n\n\n\nCount default and non-default clients / and parts of total clients in %\n\n\n\ndef_count &lt;- nrow(data[data$Defaulted. == 1, ])\nno_def_count &lt;- nrow(data[data$Defaulted. == 0, ])\ndef_count\nno_def_count \n\n333\n\n\n9667\n\n\n\ndef_count / nrow(data) * 100 # part defaults\nno_def_count / nrow(data) * 100 # part non-defaults\n\n3.33\n\n\n96.67\n\n\n\n\nCount Employed clients\n\n\n\nempl &lt;- data[data$Employed == 1, ]\nnrow(empl)\n\n7056\n\n\n\n\nCount Employed Default clients\n\n\n\nempl &lt;- data[data$Employed == 1 & data$Defaulted. == 1, ]\nnrow(empl)\n\n206\n\n\n\n\nAverage salary by Employed clients\n\n\n\nempl &lt;- data[data$Employed == 1, ]\nmean(empl$Annual.Salary)\n\n480143.43414966\n\n\n\n\nRename columns to “id”, “empl”, “balance”, “salary”, “default”:\n\n\n\ncolnames(data) &lt;- c(\"id\", \"empl\", \"balance\", \"salary\", \"default\")\nhead(data)\n\n\nA data.frame: 6 × 5\n\n\n\nid\nempl\nbalance\nsalary\ndefault\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n8754.36\n532339.56\n0\n\n\n2\n2\n0\n9806.16\n145273.56\n0\n\n\n3\n3\n1\n12882.60\n381205.68\n0\n\n\n4\n4\n1\n6351.00\n428453.88\n0\n\n\n5\n5\n1\n9427.92\n461562.00\n0\n\n\n6\n6\n0\n11035.08\n89898.72\n0",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MS Excel (xlsx)</span>"
    ]
  },
  {
    "objectID": "23-r-xlsx.html#набори-даних",
    "href": "23-r-xlsx.html#набори-даних",
    "title": "4  MS Excel (xlsx)",
    "section": "4.3 Набори даних",
    "text": "4.3 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MS Excel (xlsx)</span>"
    ]
  },
  {
    "objectID": "23-r-xlsx.html#references",
    "href": "23-r-xlsx.html#references",
    "title": "4  MS Excel (xlsx)",
    "section": "4.4 References",
    "text": "4.4 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MS Excel (xlsx)</span>"
    ]
  },
  {
    "objectID": "24-r-xml.html",
    "href": "24-r-xml.html",
    "title": "5  XML",
    "section": "",
    "text": "5.1 XML (eXtensible Markup Language)\nFor our example we will use data from data/employes.xml. File contains records with info:\n#install.packages(\"XML\")\nlibrary(\"XML\")\n#install.packages(\"methods\")\nlibrary(\"methods\")\nresult &lt;- xmlParse(file = \"../../data/employes.xml\")\nprint(result)\n\n&lt;?xml version=\"1.0\"?&gt;\n&lt;RECORDS&gt;\n  &lt;EMPLOYEE&gt;\n    &lt;ID&gt;1&lt;/ID&gt;\n    &lt;NAME&gt;Rick&lt;/NAME&gt;\n    &lt;SALARY&gt;623.3&lt;/SALARY&gt;\n    &lt;STARTDATE&gt;1/1/2012&lt;/STARTDATE&gt;\n    &lt;DEPT&gt;IT&lt;/DEPT&gt;\n  &lt;/EMPLOYEE&gt;\n  &lt;EMPLOYEE&gt;\n    &lt;ID&gt;2&lt;/ID&gt;\n    &lt;NAME&gt;Dan&lt;/NAME&gt;\n    &lt;SALARY&gt;515.2&lt;/SALARY&gt;\n    &lt;STARTDATE&gt;9/23/2013&lt;/STARTDATE&gt;\n    &lt;DEPT&gt;Operations&lt;/DEPT&gt;\n  &lt;/EMPLOYEE&gt;\n  &lt;EMPLOYEE&gt;\n    &lt;ID&gt;3&lt;/ID&gt;\n    &lt;NAME&gt;Michelle&lt;/NAME&gt;\n    &lt;SALARY&gt;611&lt;/SALARY&gt;\n    &lt;STARTDATE&gt;11/15/2014&lt;/STARTDATE&gt;\n    &lt;DEPT&gt;IT&lt;/DEPT&gt;\n  &lt;/EMPLOYEE&gt;\n  &lt;EMPLOYEE&gt;\n    &lt;ID&gt;4&lt;/ID&gt;\n    &lt;NAME&gt;Ryan&lt;/NAME&gt;\n    &lt;SALARY&gt;729&lt;/SALARY&gt;\n    &lt;STARTDATE&gt;5/11/2014&lt;/STARTDATE&gt;\n    &lt;DEPT&gt;HR&lt;/DEPT&gt;\n  &lt;/EMPLOYEE&gt;\n  &lt;EMPLOYEE&gt;\n    &lt;ID&gt;5&lt;/ID&gt;\n    &lt;NAME&gt;Gary&lt;/NAME&gt;\n    &lt;SALARY&gt;843.25&lt;/SALARY&gt;\n    &lt;STARTDATE&gt;3/27/2015&lt;/STARTDATE&gt;\n    &lt;DEPT&gt;Finance&lt;/DEPT&gt;\n  &lt;/EMPLOYEE&gt;\n  &lt;EMPLOYEE&gt;\n    &lt;ID&gt;6&lt;/ID&gt;\n    &lt;NAME&gt;Nina&lt;/NAME&gt;\n    &lt;SALARY&gt;578&lt;/SALARY&gt;\n    &lt;STARTDATE&gt;5/21/2013&lt;/STARTDATE&gt;\n    &lt;DEPT&gt;IT&lt;/DEPT&gt;\n  &lt;/EMPLOYEE&gt;\n  &lt;EMPLOYEE&gt;\n    &lt;ID&gt;7&lt;/ID&gt;\n    &lt;NAME&gt;Simon&lt;/NAME&gt;\n    &lt;SALARY&gt;632.8&lt;/SALARY&gt;\n    &lt;STARTDATE&gt;7/30/2013&lt;/STARTDATE&gt;\n    &lt;DEPT&gt;Operations&lt;/DEPT&gt;\n  &lt;/EMPLOYEE&gt;\n  &lt;EMPLOYEE&gt;\n    &lt;ID&gt;8&lt;/ID&gt;\n    &lt;NAME&gt;Guru&lt;/NAME&gt;\n    &lt;SALARY&gt;722.5&lt;/SALARY&gt;\n    &lt;STARTDATE&gt;6/17/2014&lt;/STARTDATE&gt;\n    &lt;DEPT&gt;Finance&lt;/DEPT&gt;\n  &lt;/EMPLOYEE&gt;\n&lt;/RECORDS&gt;\nrootnode &lt;- xmlRoot(result) # reading rootnode of xml document\nrootnode[[1]] # reading first record\n\n&lt;EMPLOYEE&gt;\n  &lt;ID&gt;1&lt;/ID&gt;\n  &lt;NAME&gt;Rick&lt;/NAME&gt;\n  &lt;SALARY&gt;623.3&lt;/SALARY&gt;\n  &lt;STARTDATE&gt;1/1/2012&lt;/STARTDATE&gt;\n  &lt;DEPT&gt;IT&lt;/DEPT&gt;\n&lt;/EMPLOYEE&gt;\nrootnode[[1]][[2]] # reading first record in root node and second tag, its &lt;NAME&gt;\n\n&lt;NAME&gt;Rick&lt;/NAME&gt;\nFor us the best way is to get dataframe:\nxmldataframe &lt;- xmlToDataFrame(\"../../data/employes.xml\")\nxmldataframe\n\n\nA data.frame: 8 × 5\n\n\nID\nNAME\nSALARY\nSTARTDATE\nDEPT\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\nRick\n623.3\n1/1/2012\nIT\n\n\n2\nDan\n515.2\n9/23/2013\nOperations\n\n\n3\nMichelle\n611\n11/15/2014\nIT\n\n\n4\nRyan\n729\n5/11/2014\nHR\n\n\n5\nGary\n843.25\n3/27/2015\nFinance\n\n\n6\nNina\n578\n5/21/2013\nIT\n\n\n7\nSimon\n632.8\n7/30/2013\nOperations\n\n\n8\nGuru\n722.5\n6/17/2014\nFinance",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>XML</span>"
    ]
  },
  {
    "objectID": "24-r-xml.html#xml-extensible-markup-language",
    "href": "24-r-xml.html#xml-extensible-markup-language",
    "title": "5  XML",
    "section": "",
    "text": "&lt;RECORDS&gt;\n   &lt;EMPLOYEE&gt;\n      &lt;ID&gt;1&lt;/ID&gt;\n      &lt;NAME&gt;Rick&lt;/NAME&gt;\n      &lt;SALARY&gt;623.3&lt;/SALARY&gt;\n      &lt;STARTDATE&gt;1/1/2012&lt;/STARTDATE&gt;\n      &lt;DEPT&gt;IT&lt;/DEPT&gt;\n   &lt;/EMPLOYEE&gt;\n   ...\n&lt;/RECORDS&gt;",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>XML</span>"
    ]
  },
  {
    "objectID": "24-r-xml.html#набори-даних",
    "href": "24-r-xml.html#набори-даних",
    "title": "5  XML",
    "section": "5.2 Набори даних",
    "text": "5.2 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>XML</span>"
    ]
  },
  {
    "objectID": "24-r-xml.html#references",
    "href": "24-r-xml.html#references",
    "title": "5  XML",
    "section": "5.3 References",
    "text": "5.3 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>XML</span>"
    ]
  },
  {
    "objectID": "25-r-api-json.html",
    "href": "25-r-api-json.html",
    "title": "6  JSON and API",
    "section": "",
    "text": "6.1 What is JSON?\nJSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. It is based on a subset of the JavaScript Programming Language Standard.\nAPI is the acronym for Application Programming Interface, which is a software intermediary that allows two applications to talk to each other.\nOne of the most popular packages for json is jsonlite.\n#install.packages(\"jsonlite\")\nlibrary(jsonlite)\nLet’s use readinginformation about BTC and USDT crypro currencies from Binance\nmarket = 'BTCUSDT'\ninterval = '1h'\nlimit = 100\n\nurl &lt;- paste0(url = \"https://api.binance.com/api/v3/klines?symbol=\", market ,\"&interval=\", interval,\"&limit=\", limit)\nprint(url) # complete request URL\n\n[1] \"https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1h&limit=100\"\nOn the next stage you need use fromJSON() function to get data.\nMore details about requests to Binanace at https://github.com/binance/binance-spot-api-docs/blob/master/rest-api.md#klinecandlestick-data\nIf you enter ‘url’ value at browser response is going to be like this:\ndata &lt;- fromJSON(url) # get json and transform it to list()\ndata &lt;- data[, 1:7] # let's left only 1:7 columns (from Open time to Close time)\nhead(data)\n\n\nA matrix: 6 × 7 of type chr\n\n\n1650513600000\n41693.58000000\n41750.00000000\n41525.00000000\n41610.01000000\n1138.64337000\n1650517199999\n\n\n1650517200000\n41610.01000000\n41699.00000000\n41434.44000000\n41462.76000000\n1229.25936000\n1650520799999\n\n\n1650520800000\n41462.75000000\n41600.00000000\n41419.20000000\n41522.38000000\n1049.71244000\n1650524399999\n\n\n1650524400000\n41522.38000000\n41940.00000000\n41451.00000000\n41855.69000000\n1928.48091000\n1650527999999\n\n\n1.650528e+12\n41855.69000000\n42050.30000000\n41741.10000000\n41922.97000000\n2518.04090000\n1650531599999\n\n\n1650531600000\n41922.96000000\n41971.90000000\n41743.96000000\n41803.70000000\n1655.76993000\n1650535199999\ntypeof(data) # check data type\ndata &lt;- as.data.frame(data) # convert to dataframe\nhead(data)\n\n'character'\n\n\n\nA data.frame: 6 × 7\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1650513600000\n41693.58000000\n41750.00000000\n41525.00000000\n41610.01000000\n1138.64337000\n1650517199999\n\n\n2\n1650517200000\n41610.01000000\n41699.00000000\n41434.44000000\n41462.76000000\n1229.25936000\n1650520799999\n\n\n3\n1650520800000\n41462.75000000\n41600.00000000\n41419.20000000\n41522.38000000\n1049.71244000\n1650524399999\n\n\n4\n1650524400000\n41522.38000000\n41940.00000000\n41451.00000000\n41855.69000000\n1928.48091000\n1650527999999\n\n\n5\n1.650528e+12\n41855.69000000\n42050.30000000\n41741.10000000\n41922.97000000\n2518.04090000\n1650531599999\n\n\n6\n1650531600000\n41922.96000000\n41971.90000000\n41743.96000000\n41803.70000000\n1655.76993000\n1650535199999\n# fix columns names\ncolnames(data) &lt;- c(\"Open_time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Close_time\")\nhead(data) # looks better, but columns are characters still\n\n\nA data.frame: 6 × 7\n\n\n\nOpen_time\nOpen\nHigh\nLow\nClose\nVolume\nClose_time\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1650513600000\n41693.58000000\n41750.00000000\n41525.00000000\n41610.01000000\n1138.64337000\n1650517199999\n\n\n2\n1650517200000\n41610.01000000\n41699.00000000\n41434.44000000\n41462.76000000\n1229.25936000\n1650520799999\n\n\n3\n1650520800000\n41462.75000000\n41600.00000000\n41419.20000000\n41522.38000000\n1049.71244000\n1650524399999\n\n\n4\n1650524400000\n41522.38000000\n41940.00000000\n41451.00000000\n41855.69000000\n1928.48091000\n1650527999999\n\n\n5\n1.650528e+12\n41855.69000000\n42050.30000000\n41741.10000000\n41922.97000000\n2518.04090000\n1650531599999\n\n\n6\n1650531600000\n41922.96000000\n41971.90000000\n41743.96000000\n41803.70000000\n1655.76993000\n1650535199999\nis.numeric(data[,1]) # check 1st column type is numeric\nis.numeric(data[,2]) # check 2nd column type is numeric\n\nFALSE\n\n\nFALSE\ndata &lt;- as.data.frame(sapply(data, as.numeric)) # convert all columns to numeric\nhead(data) # good, its double now\n\n\nA data.frame: 6 × 7\n\n\n\nOpen_time\nOpen\nHigh\nLow\nClose\nVolume\nClose_time\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1.650514e+12\n41693.58\n41750.0\n41525.00\n41610.01\n1138.643\n1.650517e+12\n\n\n2\n1.650517e+12\n41610.01\n41699.0\n41434.44\n41462.76\n1229.259\n1.650521e+12\n\n\n3\n1.650521e+12\n41462.75\n41600.0\n41419.20\n41522.38\n1049.712\n1.650524e+12\n\n\n4\n1.650524e+12\n41522.38\n41940.0\n41451.00\n41855.69\n1928.481\n1.650528e+12\n\n\n5\n1.650528e+12\n41855.69\n42050.3\n41741.10\n41922.97\n2518.041\n1.650532e+12\n\n\n6\n1.650532e+12\n41922.96\n41971.9\n41743.96\n41803.70\n1655.770\n1.650535e+12\nFinal stage is to convert Open_time and Close_time to dates.\ndata$Open_time &lt;- as.POSIXct(data$Open_time/1e3, origin = '1970-01-01')\ndata$Close_time &lt;- as.POSIXct(data$Close_time/1e3, origin = '1970-01-01')\n\nhead(data) \n\n\nA data.frame: 6 × 7\n\n\n\nOpen_time\nOpen\nHigh\nLow\nClose\nVolume\nClose_time\n\n\n\n&lt;dttm&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dttm&gt;\n\n\n\n\n1\n2022-04-21 07:00:00\n41693.58\n41750.0\n41525.00\n41610.01\n1138.643\n2022-04-21 07:59:59\n\n\n2\n2022-04-21 08:00:00\n41610.01\n41699.0\n41434.44\n41462.76\n1229.259\n2022-04-21 08:59:59\n\n\n3\n2022-04-21 09:00:00\n41462.75\n41600.0\n41419.20\n41522.38\n1049.712\n2022-04-21 09:59:59\n\n\n4\n2022-04-21 10:00:00\n41522.38\n41940.0\n41451.00\n41855.69\n1928.481\n2022-04-21 10:59:59\n\n\n5\n2022-04-21 11:00:00\n41855.69\n42050.3\n41741.10\n41922.97\n2518.041\n2022-04-21 11:59:59\n\n\n6\n2022-04-21 12:00:00\n41922.96\n41971.9\n41743.96\n41803.70\n1655.770\n2022-04-21 12:59:59\ntail(data) # check last records\n\n\nA data.frame: 6 × 7\n\n\n\nOpen_time\nOpen\nHigh\nLow\nClose\nVolume\nClose_time\n\n\n\n&lt;dttm&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dttm&gt;\n\n\n\n\n95\n2022-04-25 05:00:00\n39095.81\n39153.94\n38961.64\n39091.17\n1205.5158\n2022-04-25 05:59:59\n\n\n96\n2022-04-25 06:00:00\n39091.17\n39294.76\n39086.37\n39253.71\n1443.3318\n2022-04-25 06:59:59\n\n\n97\n2022-04-25 07:00:00\n39253.70\n39256.28\n39055.71\n39139.74\n896.8554\n2022-04-25 07:59:59\n\n\n98\n2022-04-25 08:00:00\n39139.74\n39230.50\n38947.42\n38975.22\n1057.4900\n2022-04-25 08:59:59\n\n\n99\n2022-04-25 09:00:00\n38975.21\n39057.97\n38590.00\n38636.35\n2814.9716\n2022-04-25 09:59:59\n\n\n100\n2022-04-25 10:00:00\n38636.35\n38675.68\n38200.00\n38534.99\n3528.2355\n2022-04-25 10:59:59",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>JSON and API</span>"
    ]
  },
  {
    "objectID": "25-r-api-json.html#what-is-json",
    "href": "25-r-api-json.html#what-is-json",
    "title": "6  JSON and API",
    "section": "",
    "text": "[\n  [\n    1499040000000,      // Open time\n    \"0.01634790\",       // Open\n    \"0.80000000\",       // High\n    \"0.01575800\",       // Low\n    \"0.01577100\",       // Close\n    \"148976.11427815\",  // Volume\n    1499644799999,      // Close time\n    \"2434.19055334\",    // Quote asset volume\n    308,                // Number of trades\n    \"1756.87402397\",    // Taker buy base asset volume\n    \"28.46694368\",      // Taker buy quote asset volume\n    \"17928899.62484339\" // Ignore.\n  ]\n]",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>JSON and API</span>"
    ]
  },
  {
    "objectID": "25-r-api-json.html#набори-даних",
    "href": "25-r-api-json.html#набори-даних",
    "title": "6  JSON and API",
    "section": "6.2 Набори даних",
    "text": "6.2 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>JSON and API</span>"
    ]
  },
  {
    "objectID": "25-r-api-json.html#references",
    "href": "25-r-api-json.html#references",
    "title": "6  JSON and API",
    "section": "6.3 References",
    "text": "6.3 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>JSON and API</span>"
    ]
  },
  {
    "objectID": "26-r-google.html",
    "href": "26-r-google.html",
    "title": "7  Google Services",
    "section": "",
    "text": "7.1 Google Spreadsheets\ngooglesheets4 is a package to work with Google Sheets from R.\n#install.packages(\"googlesheets4\")\nlibrary(googlesheets4)\nYou can read google documents after authentification on google service. There is sample code:\nLet’s read sample dataset gapminder. It detailed described in next paragraph.\n# gs4_example(\"gapminder\")",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Google Services</span>"
    ]
  },
  {
    "objectID": "26-r-google.html#google-spreadsheets",
    "href": "26-r-google.html#google-spreadsheets",
    "title": "7  Google Services",
    "section": "",
    "text": "THIS CHAPTER IS UNDER CONSTRUCTION / Working with Google Spreadsheets need account authorization.\n\n\n\n\nread_sheet(\"https://docs.google.com/spreadsheets/d/1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit#gid=780868077\")\ngs4_deauth()",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Google Services</span>"
    ]
  },
  {
    "objectID": "26-r-google.html#google-search-trends",
    "href": "26-r-google.html#google-search-trends",
    "title": "7  Google Services",
    "section": "7.2 Google Search Trends",
    "text": "7.2 Google Search Trends\nGoogle Trends is a service for analyzing search requests by many filters like region (continent, country, locality), period (year, month), information category (business, education, hobby, healthcare), information type (news, shopping, video, images) https://trends.google.com/trends/\n\n# install.packages('gtrendsR')\n# install.packages('ggplot2')\nlibrary(gtrendsR) # loading package for Google Trends queries\nlibrary(ggplot2)\n\nLet’s configure out google trends query params\n\nkeywords = c(\"Bitcoin\", \"FC Barcelona\") # search keywords\ncountry = c('AT') # search region from https://support.google.com/business/answer/6270107?hl=en\ntime = (\"2021-01-01 2021-06-01\") # period\nchannel = 'web' # search channel: google search ('news' - google news, 'images' - google images)\n\n\n# query\ntrends = gtrends(keywords, gprop = channel, geo = country, time = time, tz = \"UTC\")\n\n\ntime_trend = trends$interest_over_time\nhead(time_trend)\n\n\nA data.frame: 6 × 7\n\n\n\ndate\nhits\nkeyword\ngeo\ntime\ngprop\ncategory\n\n\n\n&lt;dttm&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\n2021-01-01\n36\nBitcoin\nAT\n2021-01-01 2021-06-01\nweb\n0\n\n\n2\n2021-01-02\n67\nBitcoin\nAT\n2021-01-01 2021-06-01\nweb\n0\n\n\n3\n2021-01-03\n74\nBitcoin\nAT\n2021-01-01 2021-06-01\nweb\n0\n\n\n4\n2021-01-04\n57\nBitcoin\nAT\n2021-01-01 2021-06-01\nweb\n0\n\n\n5\n2021-01-05\n53\nBitcoin\nAT\n2021-01-01 2021-06-01\nweb\n0\n\n\n6\n2021-01-06\n66\nBitcoin\nAT\n2021-01-01 2021-06-01\nweb\n0\n\n\n\n\n\n\nplot &lt;- ggplot(data=time_trend, aes(x=date, y=hits, group=keyword, col=keyword)) +\n  geom_line() +\n  xlab('Time') + \n  ylab('Relative Interest') + \n  theme(legend.title = element_blank(), legend.position=\"bottom\", legend.text=element_text(size=15)) + \n  ggtitle(\"Google Search Volume\")  \n\nplot",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Google Services</span>"
    ]
  },
  {
    "objectID": "26-r-google.html#набори-даних",
    "href": "26-r-google.html#набори-даних",
    "title": "7  Google Services",
    "section": "7.3 Набори даних",
    "text": "7.3 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Google Services</span>"
    ]
  },
  {
    "objectID": "26-r-google.html#references",
    "href": "26-r-google.html#references",
    "title": "7  Google Services",
    "section": "7.4 References",
    "text": "7.4 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Google Services</span>"
    ]
  },
  {
    "objectID": "27-r-sql.html",
    "href": "27-r-sql.html",
    "title": "8  SQL (with SQLite sample)",
    "section": "",
    "text": "8.1 Набори даних",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>SQL (with SQLite sample)</span>"
    ]
  },
  {
    "objectID": "27-r-sql.html#набори-даних",
    "href": "27-r-sql.html#набори-даних",
    "title": "8  SQL (with SQLite sample)",
    "section": "",
    "text": "https://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>SQL (with SQLite sample)</span>"
    ]
  },
  {
    "objectID": "27-r-sql.html#references",
    "href": "27-r-sql.html#references",
    "title": "8  SQL (with SQLite sample)",
    "section": "8.2 References",
    "text": "8.2 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>SQL (with SQLite sample)</span>"
    ]
  },
  {
    "objectID": "28-r-html.html",
    "href": "28-r-html.html",
    "title": "9  Web-pages (HTML)",
    "section": "",
    "text": "9.1 Task 3\nFrom a page https://en.wikipedia.org/wiki/List_of_largest_banks read and merge by country named tables:\nSolution\nlibrary(rvest)\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_largest_banks\" # got to url in other tab\n#url &lt;- \"data/List of largest banks - Wikipedia_.html\"\npage_data &lt;- read_html(url) # read html content\n\ntables &lt;- html_nodes(page_data, \"table\")\nhtml_table(tables[1]) #its not needed table\n\n\n    \n\nA tibble: 1  2\n\n\nX1\nX2\n\n\n&lt;lgl&gt;\n&lt;chr&gt;\n\n\n\n\nNA\nThis article is missing information about Revenue and Employment. Please expand the article to include this information. Further details may exist on the talk page. (September 2020)\nhtml_table(tables[3]) # thats solution for \"Number of banks in the top 100 by total assets\"\n#check the end of table. There are NA record\n# lets remove it\n\n\n    \n\nA tibble: 26  3\n\n\nRank\nCountry\nNumber\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\nChina\n19\n\n\n2\nUnited States\n11\n\n\n3\nJapan\n8\n\n\n4\nUnited Kingdom\n6\n\n\n4\nFrance\n6\n\n\n4\nSouth Korea\n6\n\n\n5\nCanada\n5\n\n\n5\nGermany\n5\n\n\n6\nAustralia\n4\n\n\n6\nBrazil\n4\n\n\n6\nSpain\n4\n\n\n7\nNetherlands\n3\n\n\n7\nSingapore\n3\n\n\n7\nSweden\n3\n\n\n7\nSwitzerland\n3\n\n\n8\nItaly\n2\n\n\n9\nIndia\n1\n\n\n9\nAustria\n1\n\n\n9\nBelgium\n1\n\n\n9\nDenmark\n1\n\n\n9\nFinland\n1\n\n\n9\nNorway\n1\n\n\n9\nRussia\n1\n\n\n9\nQatar\n1\n\n\n9\nNA\nNA\n\n\n9\nNA\nNA\ntable1 &lt;- as.data.frame(html_table(tables[3]))\ntable1 &lt;- table1[!is.na(table1$Country), ]\ntable1 # now it OK!\n\n\nA data.frame: 24 × 3\n\n\n\nRank\nCountry\nNumber\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\nChina\n19\n\n\n2\n2\nUnited States\n11\n\n\n3\n3\nJapan\n8\n\n\n4\n4\nUnited Kingdom\n6\n\n\n5\n4\nFrance\n6\n\n\n6\n4\nSouth Korea\n6\n\n\n7\n5\nCanada\n5\n\n\n8\n5\nGermany\n5\n\n\n9\n6\nAustralia\n4\n\n\n10\n6\nBrazil\n4\n\n\n11\n6\nSpain\n4\n\n\n12\n7\nNetherlands\n3\n\n\n13\n7\nSingapore\n3\n\n\n14\n7\nSweden\n3\n\n\n15\n7\nSwitzerland\n3\n\n\n16\n8\nItaly\n2\n\n\n17\n9\nIndia\n1\n\n\n18\n9\nAustria\n1\n\n\n19\n9\nBelgium\n1\n\n\n20\n9\nDenmark\n1\n\n\n21\n9\nFinland\n1\n\n\n22\n9\nNorway\n1\n\n\n23\n9\nRussia\n1\n\n\n24\n9\nQatar\n1\n# SOlution for \"Total market capital (US$ billion) across the top 70 banks by country\"\n# compare this with table on a given page\ntable2 &lt;- as.data.frame(html_table(tables[4]))\ntable2 # now it OK!\n\n\nA data.frame: 50 × 3\n\n\nRank\nBank.name\nMarket.cap.US..billion.\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n1\nJPMorgan Chase\n368.78\n\n\n2\nIndustrial and Commercial Bank of China\n295.65\n\n\n3\nBank of America\n279.73\n\n\n4\nWells Fargo\n214.34\n\n\n5\nChina Construction Bank\n207.98\n\n\n6\nAgricultural Bank of China\n181.49\n\n\n7\nHSBC Holdings PLC\n169.47\n\n\n8\nCitigroup Inc.\n163.58\n\n\n9\nBank of China\n151.15\n\n\n10\nChina Merchants Bank\n133.37\n\n\n11\nRoyal Bank of Canada\n113.80\n\n\n12\nToronto-Dominion Bank\n106.61\n\n\n13\nCommonwealth Bank\n99.77\n\n\n14\nHDFC Bank\n105.90\n\n\n15\nU.S. Bancorp\n84.40\n\n\n16\nGoldman Sachs\n78.70\n\n\n17\nBanco Santander\n75.47\n\n\n18\nBanco Bradesco\n74.67\n\n\n19\nMorgan Stanley\n73.93\n\n\n20\nWestpac\n67.84\n\n\n21\nMitsubishi UFJ Financial Group\n66.20\n\n\n22\nScotiabank\n65.48\n\n\n23\nPNC Financial Services\n63.11\n\n\n24\nBank of Communications\n61.85\n\n\n25\nBNP Paribas\n59.36\n\n\n26\nAustralia and New Zealand Banking Group\n54.88\n\n\n27\nNational Australia Bank\n51.68\n\n\n28\nLloyds Banking Group\n51.19\n\n\n29\nSumitomo Mitsui Financial Group\n49.85\n\n\n30\nBank of Montreal\n48.12\n\n\n31\nUBS\n45.92\n\n\n32\nING Group\n44.97\n\n\n33\nCapital One\n43.22\n\n\n34\nThe Bank of New York Mellon\n42.58\n\n\n35\nChina Minsheng Bank\n39.13\n\n\n36\nChina CITIC Bank\n38.55\n\n\n37\nBanco Bilbao Vizcaya Argentaria\n37.42\n\n\n38\nMizuho Financial Group\n36.95\n\n\n39\nIntesa Sanpaolo\n36.90\n\n\n40\nCredit Agricole\n34.89\n\n\n41\nCanadian Imperial Bank of Commerce\n34.87\n\n\n42\nRoyal Bank of Scotland\n33.95\n\n\n43\nBarclays\n33.26\n\n\n44\nCredit Suisse\n30.75\n\n\n45\nNordea\n29.59\n\n\n46\nStandard Chartered\n29.37\n\n\n47\nKBC Bank\n27.40\n\n\n48\nUniCredit\n26.88\n\n\n49\nSociete Generale\n21.27\n\n\n50\nDeutsche Bank\n15.77",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web-pages (HTML)</span>"
    ]
  },
  {
    "objectID": "28-r-html.html#task-3",
    "href": "28-r-html.html#task-3",
    "title": "9  Web-pages (HTML)",
    "section": "",
    "text": "Number of banks in the top 100 by total assets\nTotal market capital (US$ billion) across the top 70 banks by country",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web-pages (HTML)</span>"
    ]
  },
  {
    "objectID": "28-r-html.html#набори-даних",
    "href": "28-r-html.html#набори-даних",
    "title": "9  Web-pages (HTML)",
    "section": "9.2 Набори даних",
    "text": "9.2 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_users.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/telecom_sers.xlsx\nhttps://github.com/kleban/r-book-published/tree/main/datasets/Default_Fin.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/employes.xml",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web-pages (HTML)</span>"
    ]
  },
  {
    "objectID": "28-r-html.html#references",
    "href": "28-r-html.html#references",
    "title": "9  Web-pages (HTML)",
    "section": "9.3 References",
    "text": "9.3 References\n\nSQLite in R. Datacamp\nTidyverse googlesheets4 0.2.0 \nBinanace spot Api Docs\nWeb Scraping in R: rvest Tutorial by Arvid Kingl",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web-pages (HTML)</span>"
    ]
  },
  {
    "objectID": "30-r-what-is-dplyr.html",
    "href": "30-r-what-is-dplyr.html",
    "title": "10  What’s dplyr package [EN]",
    "section": "",
    "text": "10.1 Refences",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>What's `dplyr` package [EN]</span>"
    ]
  },
  {
    "objectID": "30-r-what-is-dplyr.html#refences",
    "href": "30-r-what-is-dplyr.html#refences",
    "title": "10  What’s dplyr package [EN]",
    "section": "",
    "text": "dplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>What's `dplyr` package [EN]</span>"
    ]
  },
  {
    "objectID": "31-r-data-explore.html",
    "href": "31-r-data-explore.html",
    "title": "11  Exploring data with dplyr",
    "section": "",
    "text": "11.1 Basic funtions and dataset explore\nThere are most popular functions in dplyr is listed in table.\nFor the next sample we are going to use gapminder dataset. Go to gapminder dataset description\nThe gapminder data frame include six variables:\nPer-capita GDP (Gross domestic product) is given in units of international dollars, a hypothetical unit of currency that has the same purchasing power parity that the U.S. dollar had in the United States at a given point in time – 2005, in this case.\nThe gapminder data frame is a special kind of data frame: a tibble.\nlibrary(dplyr) # for demos\n#install.packages(\"gapminder\")\nlibrary(gapminder)  # load package and dataset\nclass(gapminder)\n\n\n'tbl_df''tbl''data.frame'\nLet’s preview it with functions str(), glimpse(), head(), tail(), summary().\nstr(gapminder)\n\ntibble [1,704 x 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", ~\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, ~\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, ~\n$ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8~\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12~\n$ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, ~\nhead(gapminder) #shows first n-rows, 6 by default\n\n\nA tibble: 6 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\ntail(gapminder) #shows last n-rows, 6 by default\n\n\nA tibble: 6 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nZimbabwe\nAfrica\n1982\n60.363\n7636524\n788.8550\n\n\nZimbabwe\nAfrica\n1987\n62.351\n9216418\n706.1573\n\n\nZimbabwe\nAfrica\n1992\n60.377\n10704340\n693.4208\n\n\nZimbabwe\nAfrica\n1997\n46.809\n11404948\n792.4500\n\n\nZimbabwe\nAfrica\n2002\n39.989\n11926563\n672.0386\n\n\nZimbabwe\nAfrica\n2007\n43.487\n12311143\n469.7093\nsummary(gapminder)\n\n        country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exploring data with `dplyr`</span>"
    ]
  },
  {
    "objectID": "31-r-data-explore.html#basic-funtions-and-dataset-explore",
    "href": "31-r-data-explore.html#basic-funtions-and-dataset-explore",
    "title": "11  Exploring data with dplyr",
    "section": "",
    "text": "dplyr Function\nDescription\nEquivalent SQL\n\n\n\n\nselect()\nSelecting columns (variables)\nSELECT\n\n\nfilter()\nFilter (subset) rows.\nWHERE\n\n\ngroup_by()\nGroup the data\nGROUP BY\n\n\nsummarise()\nSummarise (or aggregate) data\n-\n\n\narrange()\nSort the data\nORDER BY\n\n\njoin()\nJoining data frames (tables)\nJOIN\n\n\nmutate()\nCreating New Variables\nCOLUMN ALIAS\n\n\n\n\n\n\n\n\nvariable\nmeaning\n\n\n\n\ncountry\n-\n\n\ncontinent\n-\n\n\nyear\n-\n\n\nlifeExp\nlife expectancy at birth\n\n\npop\ntotal population\n\n\ngdpPercap\nper-capita GDP",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exploring data with `dplyr`</span>"
    ]
  },
  {
    "objectID": "31-r-data-explore.html#filter-function",
    "href": "31-r-data-explore.html#filter-function",
    "title": "11  Exploring data with dplyr",
    "section": "11.2 filter() function",
    "text": "11.2 filter() function\n\naustria &lt;- filter(gapminder, country == \"Austria\")\naustria\n\n\nA tibble: 12 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAustria\nEurope\n1952\n66.800\n6927772\n6137.076\n\n\nAustria\nEurope\n1957\n67.480\n6965860\n8842.598\n\n\nAustria\nEurope\n1962\n69.540\n7129864\n10750.721\n\n\nAustria\nEurope\n1967\n70.140\n7376998\n12834.602\n\n\nAustria\nEurope\n1972\n70.630\n7544201\n16661.626\n\n\nAustria\nEurope\n1977\n72.170\n7568430\n19749.422\n\n\nAustria\nEurope\n1982\n73.180\n7574613\n21597.084\n\n\nAustria\nEurope\n1987\n74.940\n7578903\n23687.826\n\n\nAustria\nEurope\n1992\n76.040\n7914969\n27042.019\n\n\nAustria\nEurope\n1997\n77.510\n8069876\n29095.921\n\n\nAustria\nEurope\n2002\n78.980\n8148312\n32417.608\n\n\nAustria\nEurope\n2007\n79.829\n8199783\n36126.493\n\n\n\n\n\nfilter() takes logical expressions and returns the rows for which all are TRUE.\n\n# task: select rows with lifeExp less than 31\nfilter(gapminder, lifeExp &lt; 31)\n\n\nA tibble: 6 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAngola\nAfrica\n1952\n30.015\n4232095\n3520.6103\n\n\nGambia\nAfrica\n1952\n30.000\n284320\n485.2307\n\n\nRwanda\nAfrica\n1992\n23.599\n7290203\n737.0686\n\n\nSierra Leone\nAfrica\n1952\n30.331\n2143249\n879.7877\n\n\n\n\n\n\n# task: select Austria only and year after 1980\nfilter(gapminder, country == \"Austria\", year &gt; 1980)\n\n\nA tibble: 6 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAustria\nEurope\n1982\n73.180\n7574613\n21597.08\n\n\nAustria\nEurope\n1987\n74.940\n7578903\n23687.83\n\n\nAustria\nEurope\n1992\n76.040\n7914969\n27042.02\n\n\nAustria\nEurope\n1997\n77.510\n8069876\n29095.92\n\n\nAustria\nEurope\n2002\n78.980\n8148312\n32417.61\n\n\nAustria\nEurope\n2007\n79.829\n8199783\n36126.49\n\n\n\n\n\n\n# task: select Austria and Belgium\nfilter(gapminder, country %in% c(\"Austria\", \"Belgium\"))\n\n\nA tibble: 24 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAustria\nEurope\n1952\n66.800\n6927772\n6137.076\n\n\nAustria\nEurope\n1957\n67.480\n6965860\n8842.598\n\n\nAustria\nEurope\n1962\n69.540\n7129864\n10750.721\n\n\nAustria\nEurope\n1967\n70.140\n7376998\n12834.602\n\n\nAustria\nEurope\n1972\n70.630\n7544201\n16661.626\n\n\nAustria\nEurope\n1977\n72.170\n7568430\n19749.422\n\n\nAustria\nEurope\n1982\n73.180\n7574613\n21597.084\n\n\nAustria\nEurope\n1987\n74.940\n7578903\n23687.826\n\n\nAustria\nEurope\n1992\n76.040\n7914969\n27042.019\n\n\nAustria\nEurope\n1997\n77.510\n8069876\n29095.921\n\n\nAustria\nEurope\n2002\n78.980\n8148312\n32417.608\n\n\nAustria\nEurope\n2007\n79.829\n8199783\n36126.493\n\n\nBelgium\nEurope\n1952\n68.000\n8730405\n8343.105\n\n\nBelgium\nEurope\n1957\n69.240\n8989111\n9714.961\n\n\nBelgium\nEurope\n1962\n70.250\n9218400\n10991.207\n\n\nBelgium\nEurope\n1967\n70.940\n9556500\n13149.041\n\n\nBelgium\nEurope\n1972\n71.440\n9709100\n16672.144\n\n\nBelgium\nEurope\n1977\n72.800\n9821800\n19117.974\n\n\nBelgium\nEurope\n1982\n73.930\n9856303\n20979.846\n\n\nBelgium\nEurope\n1987\n75.350\n9870200\n22525.563\n\n\nBelgium\nEurope\n1992\n76.460\n10045622\n25575.571\n\n\nBelgium\nEurope\n1997\n77.530\n10199787\n27561.197\n\n\nBelgium\nEurope\n2002\n78.320\n10311970\n30485.884\n\n\nBelgium\nEurope\n2007\n79.441\n10392226\n33692.605\n\n\n\n\n\nLets rewrite initial code and record it to the variable/data.frame:",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exploring data with `dplyr`</span>"
    ]
  },
  {
    "objectID": "31-r-data-explore.html#pipe-operator",
    "href": "31-r-data-explore.html#pipe-operator",
    "title": "11  Exploring data with dplyr",
    "section": "11.3 Pipe (%>%/|>) operator",
    "text": "11.3 Pipe (%&gt;%/|&gt;) operator\n%&gt;% is pipe operator. The pipe operator takes the thing on the left-hand-side and pipes it into the function call on the right-hand-side – literally, drops it in as the first argument.\nhead() function without pipe and top 4 items:\n\nIn R version before 4.1.0 pipe %&gt;% operator is not a language build-in and you should install magrittr package:\n\n\nPipe opertor in R 4.1+ |&gt;, using this is preferable\n\n\n#install.packages(\"magrittr\") # for pipe %&gt;% operator\nlibrary(magrittr)\n\n\nhead(gapminder, n = 4)\n\n\nA tibble: 4 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\n\n\n\nhead() function with pipe and top 4 items:\n\ngapminder %&gt;% head(4)\n\n\nA tibble: 4 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\n\n\n\nOutput is the same. So, let’s rewrire filtering for Austria with pipe:\n\naustria &lt;- gapminder |&gt; filter(country == \"Austria\")\naustria\n\n\nA tibble: 12 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAustria\nEurope\n1952\n66.800\n6927772\n6137.076\n\n\nAustria\nEurope\n1957\n67.480\n6965860\n8842.598\n\n\nAustria\nEurope\n1962\n69.540\n7129864\n10750.721\n\n\nAustria\nEurope\n1967\n70.140\n7376998\n12834.602\n\n\nAustria\nEurope\n1972\n70.630\n7544201\n16661.626\n\n\nAustria\nEurope\n1977\n72.170\n7568430\n19749.422\n\n\nAustria\nEurope\n1982\n73.180\n7574613\n21597.084\n\n\nAustria\nEurope\n1987\n74.940\n7578903\n23687.826\n\n\nAustria\nEurope\n1992\n76.040\n7914969\n27042.019\n\n\nAustria\nEurope\n1997\n77.510\n8069876\n29095.921\n\n\nAustria\nEurope\n2002\n78.980\n8148312\n32417.608\n\n\nAustria\nEurope\n2007\n79.829\n8199783\n36126.493\n\n\n\n\n\n\n# add more conditions in filter\naustria &lt;- gapminder |&gt; filter(country == \"Austria\", year &gt; 2000)\naustria\n\n\nA tibble: 2 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAustria\nEurope\n2002\n78.980\n8148312\n32417.61\n\n\nAustria\nEurope\n2007\n79.829\n8199783\n36126.49",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exploring data with `dplyr`</span>"
    ]
  },
  {
    "objectID": "31-r-data-explore.html#select-function",
    "href": "31-r-data-explore.html#select-function",
    "title": "11  Exploring data with dplyr",
    "section": "11.4 select() function",
    "text": "11.4 select() function\nUse select() to subset the data on variables/columns by names or index. You also can define order of columns with select().\n\ngapminder |&gt; \nselect(year, country, pop) |&gt;\nslice(1: 10)\n\n\nA tibble: 10 × 3\n\n\nyear\ncountry\npop\n\n\n&lt;int&gt;\n&lt;fct&gt;\n&lt;int&gt;\n\n\n\n\n1952\nAfghanistan\n8425333\n\n\n1957\nAfghanistan\n9240934\n\n\n1962\nAfghanistan\n10267083\n\n\n1967\nAfghanistan\n11537966\n\n\n1972\nAfghanistan\n13079460\n\n\n1977\nAfghanistan\n14880372\n\n\n1982\nAfghanistan\n12881816\n\n\n1987\nAfghanistan\n13867957\n\n\n1992\nAfghanistan\n16317921\n\n\n1997\nAfghanistan\n22227415\n\n\n\n\n\nLets combine few functions with pipe (%&gt;%):\nFinally, lest extend our filtering:\n\n# compare dplyr syntax with base R call\ngapminder[gapminder$country == \"Austria\", c(\"year\", \"pop\", \"lifeExp\")]\n\ngapminder |&gt; \n    filter(country == \"Austria\") |&gt;\n    select(year, pop, lifeExp)\n\n\nA tibble: 12 × 3\n\n\nyear\npop\nlifeExp\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1952\n6927772\n66.800\n\n\n1957\n6965860\n67.480\n\n\n1962\n7129864\n69.540\n\n\n1967\n7376998\n70.140\n\n\n1972\n7544201\n70.630\n\n\n1977\n7568430\n72.170\n\n\n1982\n7574613\n73.180\n\n\n1987\n7578903\n74.940\n\n\n1992\n7914969\n76.040\n\n\n1997\n8069876\n77.510\n\n\n2002\n8148312\n78.980\n\n\n2007\n8199783\n79.829\n\n\n\n\n\n\nA tibble: 12 × 3\n\n\nyear\npop\nlifeExp\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1952\n6927772\n66.800\n\n\n1957\n6965860\n67.480\n\n\n1962\n7129864\n69.540\n\n\n1967\n7376998\n70.140\n\n\n1972\n7544201\n70.630\n\n\n1977\n7568430\n72.170\n\n\n1982\n7574613\n73.180\n\n\n1987\n7578903\n74.940\n\n\n1992\n7914969\n76.040\n\n\n1997\n8069876\n77.510\n\n\n2002\n8148312\n78.980\n\n\n2007\n8199783\n79.829\n\n\n\n\n\nYou can remove some columns using minus(operator) and add few filter conditions:\n\naustria &lt;- gapminder |&gt; \n                filter(country == \"Austria\", year &gt; 2000) |&gt;\n                select(-continent, -gdpPercap) |&gt;\n                head()\naustria\n\n\nA tibble: 2 × 4\n\n\ncountry\nyear\nlifeExp\npop\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\nAustria\n2002\n78.980\n8148312\n\n\nAustria\n2007\n79.829\n8199783\n\n\n\n\n\nYou can insert different conditions about columns you need to select.\n\ngapminder |&gt;\n    select(!where(is.numeric)) |&gt;  # its 1704 records, because of repeating some records\n    slice(1:5)\n\n\nA tibble: 5 × 2\n\n\ncountry\ncontinent\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\nAfghanistan\nAsia\n\n\nAfghanistan\nAsia\n\n\nAfghanistan\nAsia\n\n\nAfghanistan\nAsia\n\n\nAfghanistan\nAsia\n\n\n\n\n\nLet’s output all unique pairs continent -&gt; country with distinct() function:\n\ngapminder |&gt;\n    select(country) |&gt;\n    distinct() # its 142 records now\n\n\nA tibble: 142 × 1\n\n\ncountry\n\n\n&lt;fct&gt;\n\n\n\n\nAfghanistan\n\n\nAlbania\n\n\nAlgeria\n\n\nAngola\n\n\nArgentina\n\n\nAustralia\n\n\nAustria\n\n\nBahrain\n\n\nBangladesh\n\n\nBelgium\n\n\nBenin\n\n\nBolivia\n\n\nBosnia and Herzegovina\n\n\nBotswana\n\n\nBrazil\n\n\nBulgaria\n\n\nBurkina Faso\n\n\nBurundi\n\n\nCambodia\n\n\nCameroon\n\n\nCanada\n\n\nCentral African Republic\n\n\nChad\n\n\nChile\n\n\nChina\n\n\nColombia\n\n\nComoros\n\n\nCongo, Dem. Rep.\n\n\nCongo, Rep.\n\n\nCosta Rica\n\n\n⋮\n\n\nSierra Leone\n\n\nSingapore\n\n\nSlovak Republic\n\n\nSlovenia\n\n\nSomalia\n\n\nSouth Africa\n\n\nSpain\n\n\nSri Lanka\n\n\nSudan\n\n\nSwaziland\n\n\nSweden\n\n\nSwitzerland\n\n\nSyria\n\n\nTaiwan\n\n\nTanzania\n\n\nThailand\n\n\nTogo\n\n\nTrinidad and Tobago\n\n\nTunisia\n\n\nTurkey\n\n\nUganda\n\n\nUnited Kingdom\n\n\nUnited States\n\n\nUruguay\n\n\nVenezuela\n\n\nVietnam\n\n\nWest Bank and Gaza\n\n\nYemen, Rep.\n\n\nZambia\n\n\nZimbabwe",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exploring data with `dplyr`</span>"
    ]
  },
  {
    "objectID": "31-r-data-explore.html#selecting-random-n-rows",
    "href": "31-r-data-explore.html#selecting-random-n-rows",
    "title": "11  Exploring data with dplyr",
    "section": "11.5 Selecting random \\(N\\) rows",
    "text": "11.5 Selecting random \\(N\\) rows\nThe sample_n() function selects random rows from a data frame\n\ngapminder |&gt; sample_n(5)\n\n\nA tibble: 5 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nNorway\nEurope\n1967\n74.080\n3786019\n16361.8765\n\n\nCentral African Republic\nAfrica\n2002\n43.308\n4048013\n738.6906\n\n\nUruguay\nAmericas\n2007\n76.384\n3447496\n10611.4630\n\n\nTogo\nAfrica\n1997\n58.390\n4320890\n982.2869\n\n\nParaguay\nAmericas\n2002\n70.755\n5884491\n3783.6742\n\n\n\n\n\nIf you want make pseudo-random generation reprodusable use set.seed(). Seed is start point of random generation. Different seeds give different output.\n\nset.seed(2023) # example, seed = 2023\n\nThe sample_frac() function selects random fraction rows from a data frame. Let’s select \\(1\\%\\) of data\n\nset.seed(2023) # output not changing, uncomment it \ngapminder %&gt;% sample_frac(0.1)\n\n\nA tibble: 170 × 6\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nSwitzerland\nEurope\n2007\n81.701\n7554661\n37506.4191\n\n\nDjibouti\nAfrica\n2002\n53.373\n447416\n1908.2609\n\n\nSlovenia\nEurope\n1972\n69.820\n1694510\n12383.4862\n\n\nSao Tome and Principe\nAfrica\n1997\n63.306\n145608\n1339.0760\n\n\nTurkey\nEurope\n1987\n63.108\n52881328\n5089.0437\n\n\nLebanon\nAsia\n1957\n59.489\n1647412\n6089.7869\n\n\nEritrea\nAfrica\n1972\n44.142\n2260187\n514.3242\n\n\nPhilippines\nAsia\n1972\n58.065\n40850141\n1989.3741\n\n\nTunisia\nAfrica\n1972\n55.602\n5303507\n2753.2860\n\n\nUganda\nAfrica\n1952\n39.978\n5824797\n734.7535\n\n\nOman\nAsia\n1972\n52.143\n829050\n10618.0385\n\n\nAustralia\nOceania\n2007\n81.235\n20434176\n34435.3674\n\n\nMali\nAfrica\n2002\n51.818\n10580176\n951.4098\n\n\nEquatorial Guinea\nAfrica\n1957\n35.983\n232922\n426.0964\n\n\nSouth Africa\nAfrica\n1982\n58.161\n31140029\n8568.2662\n\n\nBurundi\nAfrica\n1962\n42.045\n2961915\n355.2032\n\n\nAngola\nAfrica\n1992\n40.647\n8735988\n2627.8457\n\n\nYemen, Rep.\nAsia\n1952\n32.548\n4963829\n781.7176\n\n\nCroatia\nEurope\n2007\n75.748\n4493312\n14619.2227\n\n\nOman\nAsia\n1992\n71.197\n1915208\n18616.7069\n\n\nThailand\nAsia\n1962\n56.061\n29263397\n1002.1992\n\n\nComoros\nAfrica\n1952\n40.715\n153936\n1102.9909\n\n\nEritrea\nAfrica\n1957\n38.047\n1542611\n344.1619\n\n\nZambia\nAfrica\n2002\n39.193\n10595811\n1071.6139\n\n\nCote d'Ivoire\nAfrica\n1987\n54.655\n10761098\n2156.9561\n\n\nSouth Africa\nAfrica\n1957\n47.985\n16151549\n5487.1042\n\n\nParaguay\nAmericas\n1957\n63.196\n1770902\n2046.1547\n\n\nKuwait\nAsia\n1952\n55.565\n160000\n108382.3529\n\n\nBrazil\nAmericas\n1952\n50.917\n56602560\n2108.9444\n\n\nCanada\nAmericas\n1957\n69.960\n17010154\n12489.9501\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\nSwaziland\nAfrica\n1997\n54.289\n1054486\n3876.7685\n\n\nMyanmar\nAsia\n2002\n59.908\n45598081\n611.0000\n\n\nSao Tome and Principe\nAfrica\n1987\n61.728\n110812\n1516.5255\n\n\nGhana\nAfrica\n1977\n51.756\n10538093\n993.2240\n\n\nGuinea-Bissau\nAfrica\n1997\n44.873\n1193708\n796.6645\n\n\nGuinea\nAfrica\n1992\n48.576\n6990574\n794.3484\n\n\nHaiti\nAmericas\n1957\n40.696\n3507701\n1726.8879\n\n\nSao Tome and Principe\nAfrica\n2007\n65.528\n199579\n1598.4351\n\n\nComoros\nAfrica\n1997\n60.660\n527982\n1173.6182\n\n\nEquatorial Guinea\nAfrica\n1972\n40.516\n277603\n672.4123\n\n\nOman\nAsia\n1982\n62.728\n1301048\n12954.7910\n\n\nNamibia\nAfrica\n1977\n56.437\n977026\n3876.4860\n\n\nCongo, Dem. Rep.\nAfrica\n1952\n39.143\n14100005\n780.5423\n\n\nHong Kong, China\nAsia\n1977\n73.600\n4583700\n11186.1413\n\n\nBolivia\nAmericas\n1997\n62.050\n7693188\n3326.1432\n\n\nPanama\nAmericas\n2002\n74.712\n2990875\n7356.0319\n\n\nNigeria\nAfrica\n1952\n36.324\n33119096\n1077.2819\n\n\nMalaysia\nAsia\n2007\n74.241\n24821286\n12451.6558\n\n\nJapan\nAsia\n1952\n63.030\n86459025\n3216.9563\n\n\nAlbania\nEurope\n1967\n66.220\n1984060\n2760.1969\n\n\nPortugal\nEurope\n1997\n75.970\n10156415\n17641.0316\n\n\nUruguay\nAmericas\n1952\n66.071\n2252965\n5716.7667\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nSyria\nAsia\n1987\n66.974\n11242847\n3116.7743\n\n\nLibya\nAfrica\n2002\n72.737\n5368585\n9534.6775\n\n\nMauritania\nAfrica\n1962\n44.248\n1146757\n1055.8960\n\n\nTrinidad and Tobago\nAmericas\n1992\n69.862\n1183669\n7370.9909\n\n\nNetherlands\nEurope\n1962\n73.230\n11805689\n12790.8496\n\n\nReunion\nAfrica\n2007\n76.442\n798094\n7670.1226\n\n\nHonduras\nAmericas\n1957\n44.665\n1770390\n2220.4877",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exploring data with `dplyr`</span>"
    ]
  },
  {
    "objectID": "31-r-data-explore.html#refences",
    "href": "31-r-data-explore.html#refences",
    "title": "11  Exploring data with dplyr",
    "section": "11.6 Refences",
    "text": "11.6 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exploring data with `dplyr`</span>"
    ]
  },
  {
    "objectID": "32-r-data-slice.html",
    "href": "32-r-data-slice.html",
    "title": "12  Subset rows with slice()",
    "section": "",
    "text": "12.1 Refences",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Subset rows with `slice()`</span>"
    ]
  },
  {
    "objectID": "32-r-data-slice.html#refences",
    "href": "32-r-data-slice.html#refences",
    "title": "12  Subset rows with slice()",
    "section": "",
    "text": "dplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Subset rows with `slice()`</span>"
    ]
  },
  {
    "objectID": "33-r-data-sorting.html",
    "href": "33-r-data-sorting.html",
    "title": "13  Sorting with arrange()",
    "section": "",
    "text": "13.1 Refences",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sorting with  **`arrange()`**</span>"
    ]
  },
  {
    "objectID": "33-r-data-sorting.html#refences",
    "href": "33-r-data-sorting.html#refences",
    "title": "13  Sorting with arrange()",
    "section": "",
    "text": "dplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sorting with  **`arrange()`**</span>"
    ]
  },
  {
    "objectID": "34-r-data-mutating.html",
    "href": "34-r-data-mutating.html",
    "title": "14  Create new variables with mutate()",
    "section": "",
    "text": "14.1 Refences",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Create new variables with **`mutate()`**</span>"
    ]
  },
  {
    "objectID": "34-r-data-mutating.html#refences",
    "href": "34-r-data-mutating.html#refences",
    "title": "14  Create new variables with mutate()",
    "section": "",
    "text": "dplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Create new variables with **`mutate()`**</span>"
    ]
  },
  {
    "objectID": "35-r-data-rename.html",
    "href": "35-r-data-rename.html",
    "title": "15  Renaming columns with rename()",
    "section": "",
    "text": "15.1 Refences",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Renaming columns with **`rename()`**</span>"
    ]
  },
  {
    "objectID": "35-r-data-rename.html#refences",
    "href": "35-r-data-rename.html#refences",
    "title": "15  Renaming columns with rename()",
    "section": "",
    "text": "dplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Renaming columns with **`rename()`**</span>"
    ]
  },
  {
    "objectID": "36-r-data-grouping.html",
    "href": "36-r-data-grouping.html",
    "title": "16  Grouping columns with dplyr",
    "section": "",
    "text": "16.1 group_by() + summarise()\ngroup_by(.data, ..., add = FALSE) returns copy of table grouped by defined columns.\nLet’s find average by lifeExp for each continent in 2002 (ouput is continent, lifeExpAvg2002, countriesCount, year = 2002):\ngapminder |&gt;\n    filter(year == 2002) |&gt; # year\n    group_by(continent) |&gt; # grouping condition, you ca\n    summarise(\n        lifeExpAvg2002 = mean(lifeExp),\n        countriesCount = n() # n() count of rows in group  \n        ) \n\n\nA tibble: 5 × 3\n\n\ncontinent\nlifeExpAvg2002\ncountriesCount\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\nAfrica\n53.32523\n52\n\n\nAmericas\n72.42204\n25\n\n\nAsia\n69.23388\n33\n\n\nEurope\n76.70060\n30\n\n\nOceania\n79.74000\n2\nLet’s find total population for each continent in 2002 (ouput is continent, totalPop, year):\ngapminder |&gt;\n    filter(year == 2002) |&gt; # year\n    group_by(continent, year) |&gt; # grouping condition\n    summarise(totalPop = sum(pop), .groups = \"keep\") \n\n\nA grouped_df: 5 × 3\n\n\ncontinent\nyear\ntotalPop\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAfrica\n2002\n833723916\n\n\nAmericas\n2002\n849772762\n\n\nAsia\n2002\n3601802203\n\n\nEurope\n2002\n578223869\n\n\nOceania\n2002\n23454829\nThere are additional variations of summarise():",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Grouping columns with **`dplyr`**</span>"
    ]
  },
  {
    "objectID": "36-r-data-grouping.html#group_by-summarise",
    "href": "36-r-data-grouping.html#group_by-summarise",
    "title": "16  Grouping columns with dplyr",
    "section": "",
    "text": "summarise_all() - Apply funs to every column.\nsummarise_at() - Apply funs to specific columns.\n\nsummarise_if() - Apply funs to all cols of one type.\n\n\n\n16.1.1 Task on Credits (rewrite it)\n\nlibrary(ISLR)\n\ngroup_inc &lt;- aggregate(Income ~ Age + Gender, data = Credit, mean)\n\nm_data &lt;- group_inc[group_inc$Gender == \" Male\", ]\nnrow(m_data)\n\nf_data &lt;- group_inc[group_inc$Gender == \"Female\", ]\nnrow(f_data)\nwith(m_data, plot(Age, Income, type = \"l\", col=\"red\"))\nwith(f_data, lines(Age, Income, type = \"l\", col =\"blue\"))\n\n63\n\n\n62\n\n\n\n\n\n\n\n\n\n\ncd &lt;- Credit %&gt;%\nselect(Income, Age, Gender) %&gt;%\ngroup_by(Age, Gender) %&gt;%\nsummarize(Income = mean(Income))\n\nm_data &lt;- cd %&gt;% filter(Gender == \" Male\")\nnrow(m_data)\n\nf_data &lt;- cd %&gt;% filter(Gender == \"Female\")\nnrow(f_data)\n\nwith(m_data, plot(Age, Income, type = \"l\", col=\"red\"))\nwith(f_data, lines(Age, Income, type = \"l\", col =\"blue\"))\n\n`summarise()` has grouped output by 'Age'. You can override using the `.groups`\nargument.\n\n\n63\n\n\n62",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Grouping columns with **`dplyr`**</span>"
    ]
  },
  {
    "objectID": "36-r-data-grouping.html#refences",
    "href": "36-r-data-grouping.html#refences",
    "title": "16  Grouping columns with dplyr",
    "section": "16.2 Refences",
    "text": "16.2 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Grouping columns with **`dplyr`**</span>"
    ]
  },
  {
    "objectID": "37-r-data-bind.html",
    "href": "37-r-data-bind.html",
    "title": "17  Binding rows and columns",
    "section": "",
    "text": "17.1 bind_rows\nbind_rows(.data, …) helps to unite two dataframes with the same columns order and names.\nSo, if we need add one data frame to an other vertically (bind rows) we shoul use bind_rows:\nd2002 &lt;- gapminder %&gt;%\n            filter(year == 2002) %&gt;% # year\n            group_by(continent, year) %&gt;% # grouping condition\n            summarise(\n                lifeExpAvg = mean(lifeExp),\n                countriesCount = n(), # n() count of rows in group \n                .groups = 'drop'\n            )\nhead(d2002)\n\n\nA tibble: 5 × 4\n\n\ncontinent\nyear\nlifeExpAvg\ncountriesCount\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\nAfrica\n2002\n53.32523\n52\n\n\nAmericas\n2002\n72.42204\n25\n\n\nAsia\n2002\n69.23388\n33\n\n\nEurope\n2002\n76.70060\n30\n\n\nOceania\n2002\n79.74000\n2\nd2007 &lt;- gapminder %&gt;%\n            filter(year == 2007) %&gt;% # year\n            group_by(continent, year) %&gt;% # grouping condition\n            summarise(\n                lifeExpAvg = mean(lifeExp),\n                countriesCount = n() # n() count of rows in group                \n            )\nhead(d2007)\n\n`summarise()` has grouped output by 'continent'. You can override using the `.groups` argument.\n\n\n\nA grouped_df: 5 × 4\n\n\ncontinent\nyear\nlifeExpAvg\ncountriesCount\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\nAfrica\n2007\n54.80604\n52\n\n\nAmericas\n2007\n73.60812\n25\n\n\nAsia\n2007\n70.72848\n33\n\n\nEurope\n2007\n77.64860\n30\n\n\nOceania\n2007\n80.71950\n2\nUnite them:\nd2002 %&gt;% bind_rows(d2007) ## bind rows\n\n\nA tibble: 10 × 4\n\n\ncontinent\nyear\nlifeExpAvg\ncountriesCount\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\nAfrica\n2002\n53.32523\n52\n\n\nAmericas\n2002\n72.42204\n25\n\n\nAsia\n2002\n69.23388\n33\n\n\nEurope\n2002\n76.70060\n30\n\n\nOceania\n2002\n79.74000\n2\n\n\nAfrica\n2007\n54.80604\n52\n\n\nAmericas\n2007\n73.60812\n25\n\n\nAsia\n2007\n70.72848\n33\n\n\nEurope\n2007\n77.64860\n30\n\n\nOceania\n2007\n80.71950\n2",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Binding rows and columns</span>"
    ]
  },
  {
    "objectID": "37-r-data-bind.html#bind_cols",
    "href": "37-r-data-bind.html#bind_cols",
    "title": "17  Binding rows and columns",
    "section": "17.2 bind_cols",
    "text": "17.2 bind_cols\nbind_cols(.data, …) helps to unite two dataframes with the same rows count.\n\ngrouped_data2002pop &lt;- gapminder %&gt;%\n    filter(year == 2002) %&gt;% # year\n    group_by(continent) %&gt;% # grouping condition\n    summarise(totalPop = sum(pop)) %&gt;%\n    mutate(year = 2002)\ngrouped_data2002pop\n\n\nA tibble: 5 × 3\n\n\ncontinent\ntotalPop\nyear\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nAfrica\n833723916\n2002\n\n\nAmericas\n849772762\n2002\n\n\nAsia\n3601802203\n2002\n\n\nEurope\n578223869\n2002\n\n\nOceania\n23454829\n2002\n\n\n\n\n\nLet’s combine d2002 and grouped_data2002pop:\n\ngrouped_data &lt;- d2002 %&gt;% \n    bind_cols(grouped_data2002pop)\ngrouped_data\n\n# columns with the same name were renamed!\n\nNew names:\n* `continent` -&gt; `continent...1`\n* `year` -&gt; `year...2`\n* `continent` -&gt; `continent...5`\n* `year` -&gt; `year...7`\n\n\n\nA tibble: 5 × 7\n\n\ncontinent...1\nyear...2\nlifeExpAvg\ncountriesCount\ncontinent...5\ntotalPop\nyear...7\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nAfrica\n2002\n53.32523\n52\nAfrica\n833723916\n2002\n\n\nAmericas\n2002\n72.42204\n25\nAmericas\n849772762\n2002\n\n\nAsia\n2002\n69.23388\n33\nAsia\n3601802203\n2002\n\n\nEurope\n2002\n76.70060\n30\nEurope\n578223869\n2002\n\n\nOceania\n2002\n79.74000\n2\nOceania\n23454829\n2002\n\n\n\n\n\nYou can remove same named variables before binding:\n\ngrouped_data &lt;- d2002 %&gt;% \n    bind_cols(grouped_data2002pop %&gt;%\n              select(-continent, -year))\ngrouped_data\n\n# better, but continents order is not the same in both frames \n# your data is going to be damaged\n\n\nA tibble: 5 × 5\n\n\ncontinent\nyear\nlifeExpAvg\ncountriesCount\ntotalPop\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAfrica\n2002\n53.32523\n52\n833723916\n\n\nAmericas\n2002\n72.42204\n25\n849772762\n\n\nAsia\n2002\n69.23388\n33\n3601802203\n\n\nEurope\n2002\n76.70060\n30\n578223869\n\n\nOceania\n2002\n79.74000\n2\n23454829\n\n\n\n\n\n\ngrouped_data2002pop &lt;- grouped_data2002pop %&gt;% \n    arrange(totalPop)\n\ngrouped_data &lt;- d2002 %&gt;% \n    bind_cols(grouped_data2002pop)\ngrouped_data\n\n# you can see that continent fields different in the same row\n\nNew names:\n* `continent` -&gt; `continent...1`\n* `year` -&gt; `year...2`\n* `continent` -&gt; `continent...5`\n* `year` -&gt; `year...7`\n\n\n\nA tibble: 5 × 7\n\n\ncontinent...1\nyear...2\nlifeExpAvg\ncountriesCount\ncontinent...5\ntotalPop\nyear...7\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nAfrica\n2002\n53.32523\n52\nOceania\n23454829\n2002\n\n\nAmericas\n2002\n72.42204\n25\nEurope\n578223869\n2002\n\n\nAsia\n2002\n69.23388\n33\nAfrica\n833723916\n2002\n\n\nEurope\n2002\n76.70060\n30\nAmericas\n849772762\n2002\n\n\nOceania\n2002\n79.74000\n2\nAsia\n3601802203\n2002\n\n\n\n\n\nHow to solve this? Join functions issolution.",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Binding rows and columns</span>"
    ]
  },
  {
    "objectID": "37-r-data-bind.html#refences",
    "href": "37-r-data-bind.html#refences",
    "title": "17  Binding rows and columns",
    "section": "17.3 Refences",
    "text": "17.3 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Binding rows and columns</span>"
    ]
  },
  {
    "objectID": "38-r-data-join.html",
    "href": "38-r-data-join.html",
    "title": "18  Join()-ing data",
    "section": "",
    "text": "18.1 Join types\nLets check join operations as set opretations\nJoins on table are look like this:\nSource: https://marcus116.blogspot.com/2019/07/cheatsheets-sql-join-cheat-sheets.html",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`Join()`-ing data</span>"
    ]
  },
  {
    "objectID": "38-r-data-join.html#join-functions",
    "href": "38-r-data-join.html#join-functions",
    "title": "18  Join()-ing data",
    "section": "18.2 Join functions",
    "text": "18.2 Join functions\nTo solve previous problem you can use set of join()-functions. left_join() can solve our previous example:\n\nd2002 &lt;- gapminder %&gt;%\n            filter(year == 2002) %&gt;% # year\n            group_by(continent, year) %&gt;% # grouping condition\n            summarise(\n                lifeExpAvg = mean(lifeExp),\n                countriesCount = n(), # n() count of rows in group \n                .groups = 'drop'\n            )\nd2002 |&gt; head()\n\n\nA tibble: 5 × 4\n\n\ncontinent\nyear\nlifeExpAvg\ncountriesCount\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\nAfrica\n2002\n53.32523\n52\n\n\nAmericas\n2002\n72.42204\n25\n\n\nAsia\n2002\n69.23388\n33\n\n\nEurope\n2002\n76.70060\n30\n\n\nOceania\n2002\n79.74000\n2\n\n\n\n\n\n\ngrouped_data2002pop &lt;- gapminder %&gt;%\n    filter(year == 2002) %&gt;% # year\n    group_by(continent) %&gt;% # grouping condition\n    summarise(totalPop = sum(pop),\n             year = min(year))\n\ngrouped_data2002pop |&gt; head()\n\n\nA tibble: 5 × 3\n\n\ncontinent\ntotalPop\nyear\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\nAfrica\n833723916\n2002\n\n\nAmericas\n849772762\n2002\n\n\nAsia\n3601802203\n2002\n\n\nEurope\n578223869\n2002\n\n\nOceania\n23454829\n2002\n\n\n\n\n\n\ngrouped_data2002pop &lt;- grouped_data2002pop %&gt;% \n    arrange(totalPop)\n\ngrouped_data &lt;- d2002 %&gt;% \n    left_join(grouped_data2002pop, by = \"continent\")\ngrouped_data\n\n# but we have duplicated year\n\n\nA tibble: 5 × 6\n\n\ncontinent\nyear.x\nlifeExpAvg\ncountriesCount\ntotalPop\nyear.y\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\nAfrica\n2002\n53.32523\n52\n833723916\n2002\n\n\nAmericas\n2002\n72.42204\n25\n849772762\n2002\n\n\nAsia\n2002\n69.23388\n33\n3601802203\n2002\n\n\nEurope\n2002\n76.70060\n30\n578223869\n2002\n\n\nOceania\n2002\n79.74000\n2\n23454829\n2002\n\n\n\n\n\n\ngrouped_data2002pop &lt;- grouped_data2002pop %&gt;% \n    arrange(totalPop)\n\ngrouped_data &lt;- d2002 %&gt;% \n    left_join(grouped_data2002pop, by = c(\"continent\", \"year\"))\ngrouped_data\n\n#ok\n\n\nA tibble: 5 × 5\n\n\ncontinent\nyear\nlifeExpAvg\ncountriesCount\ntotalPop\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAfrica\n2002\n53.32523\n52\n833723916\n\n\nAmericas\n2002\n72.42204\n25\n849772762\n\n\nAsia\n2002\n69.23388\n33\n3601802203\n\n\nEurope\n2002\n76.70060\n30\n578223869\n\n\nOceania\n2002\n79.74000\n2\n23454829\n\n\n\n\n\nLet’s make a different data sets for testing join() fucntions:\n\nfirst_df &lt;- data.frame(Letter = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                      Value = c(1:5))\n\nsecond_df &lt;- data.frame(Letter = c(\"A\", \"B\", \"C\", \"D\", \"F\"),\n                      Value = c(12, 7, 4, 1, 5))\nfirst_df\nsecond_df \n\n\nA data.frame: 5 × 2\n\n\nLetter\nValue\n\n\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\nA\n1\n\n\nB\n2\n\n\nC\n3\n\n\nD\n4\n\n\nE\n5\n\n\n\n\n\n\nA data.frame: 5 × 2\n\n\nLetter\nValue\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\nA\n12\n\n\nB\n7\n\n\nC\n4\n\n\nD\n1\n\n\nF\n5\n\n\n\n\n\nYou can see that the last row Letter is different in dataframes. left_join() test is next.\n\nfirst_df %&gt;% \n    left_join(second_df, by = \"Letter\")\n# there is no F letter, becouse first_db joined only known first_df Letters.\n\n\nA data.frame: 5 × 3\n\n\nLetter\nValue.x\nValue.y\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nA\n1\n12\n\n\nB\n2\n7\n\n\nC\n3\n4\n\n\nD\n4\n1\n\n\nE\n5\nNA\n\n\n\n\n\n\nfirst_df %&gt;% \n    right_join(second_df, by = \"Letter\")\n# right_join! there is no E letter, becouse first_db joined only known second_df Letters.\n\n\nA data.frame: 5 × 3\n\n\nLetter\nValue.x\nValue.y\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nA\n1\n12\n\n\nB\n2\n7\n\n\nC\n3\n4\n\n\nD\n4\n1\n\n\nF\nNA\n5\n\n\n\n\n\n\nfirst_df %&gt;% \n    inner_join(second_df, by = \"Letter\")\n# inner_join! there is no E and F Letters, \n# only known both first_df and second_df are left here.\n\n\nA data.frame: 4 × 3\n\n\nLetter\nValue.x\nValue.y\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nA\n1\n12\n\n\nB\n2\n7\n\n\nC\n3\n4\n\n\nD\n4\n1\n\n\n\n\n\n\nfirst_df %&gt;% \n    full_join(second_df, by = \"Letter\")\n# all are here, but unknown values replaced by NA, it's ok.\n\n\nA data.frame: 6 × 3\n\n\nLetter\nValue.x\nValue.y\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nA\n1\n12\n\n\nB\n2\n7\n\n\nC\n3\n4\n\n\nD\n4\n1\n\n\nE\n5\nNA\n\n\nF\nNA\n5\n\n\n\n\n\nShort description of reviewed functions:\n\n\n\n\n\n\n\n\n\nFunction\nObjectives\nArguments\nMultiple keys\n\n\n\n\nleft_join()\nMerge two datasets. Keep all observations from the origin table\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)\n\n\nright_join()\nMerge two datasets. Keep all observations from the destination table\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)\n\n\ninner_join()\nMerge two datasets. Excludes all unmatched rows\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)\n\n\nfull_join()\nMerge two datasets. Keeps all observations\ndata, origin, destination, by = “ID”\norigin, destination, by = c(“ID”, “ID2”)",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`Join()`-ing data</span>"
    ]
  },
  {
    "objectID": "38-r-data-join.html#refences",
    "href": "38-r-data-join.html#refences",
    "title": "18  Join()-ing data",
    "section": "18.3 Refences",
    "text": "18.3 Refences\n\ndplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`Join()`-ing data</span>"
    ]
  },
  {
    "objectID": "39-r-wide-to-long.html",
    "href": "39-r-wide-to-long.html",
    "title": "19  Wide-to-long tables",
    "section": "",
    "text": "19.1 Refences",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Wide-to-long tables</span>"
    ]
  },
  {
    "objectID": "39-r-wide-to-long.html#refences",
    "href": "39-r-wide-to-long.html#refences",
    "title": "19  Wide-to-long tables",
    "section": "",
    "text": "dplyr: A Grammar of Data Manipulation on https://cran.r-project.org/.\nData Transformation with splyr::cheat sheet.\nDPLYR TUTORIAL : DATA MANIPULATION (50 EXAMPLES) by Deepanshu Bhalla.\nDplyr Intro by Stat 545. 6.R Dplyr Tutorial: Data Manipulation(Join) & Cleaning(Spread). Introduction to Data Analysis\nLoan Default Prediction. Beginners data set for financial analytics Kaggle",
    "crumbs": [
      "ТЕМА 3. МАНІПУЛЮВАННЯ  ДАНИМИ У `dplyr`",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Wide-to-long tables</span>"
    ]
  },
  {
    "objectID": "40-r-data-quality.html",
    "href": "40-r-data-quality.html",
    "title": "20  Оцінки якості даних",
    "section": "",
    "text": "20.1 Що таке валідація даних?\nВалідація даних відноситься до процесу забезпечення точності та якості даних. Він реалізується шляхом вбудовування кількох перевірок у систему або звітування для забезпечення логічної узгодженості введених і збережених даних.\nЯкість даних залежить від очищення та коригування даних, які відсутні, некоректні, недійсні або нечитабельні. Для забезпечення достовірності даних важливо зрозуміти ключові аспекти якості даних, щоб оцінити, наскільки дані погані/хороші.\nНа перший погляд, очевидно, що перетворення даних до якісних полягає в очищенні поганих даних – даних, які відсутні, неправильні або якимось чином недійсні. Але щоб переконатися, що дані заслуговують довіри, важливо розуміти ключові виміри якості даних, щоб оцінити, наскільки дані є «поганими».\nОкремі компанії мають внутрішні документи, що визначають виміри оцінки якості даних та порядок його проведення - Data Validation Framework або Data Quality Framework.\nКоли говорять про якість даних, то мається на увазі їх оцінка у кількох вимірах. Розглянемо коротко ці виміри:",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Оцінки якості даних</span>"
    ]
  },
  {
    "objectID": "40-r-data-quality.html#що-таке-валідація-даних",
    "href": "40-r-data-quality.html#що-таке-валідація-даних",
    "title": "20  Оцінки якості даних",
    "section": "",
    "text": "Правильність / Accuracy\nПовнота / Completeness\nУзгодженість / Consistency\nВідповідність / Conformity\nЦілісність / Integrity\nСвоєчасність / Timeliness\nУнікальність / Uniqueness",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Оцінки якості даних</span>"
    ]
  },
  {
    "objectID": "40-r-data-quality.html#правильність-accuracy",
    "href": "40-r-data-quality.html#правильність-accuracy",
    "title": "20  Оцінки якості даних",
    "section": "20.2 Правильність / (Accuracy)",
    "text": "20.2 Правильність / (Accuracy)\nПравильність — це ступінь, до якого дані правильно відображають реальний об’єкт АБО описувану подію.\nПриклади: - [x] Реальною вартістю є ціна продажу одиниці товару. - [x] Адреса співробітника в базі даних співробітників є справжньою адресою.\nЗапитання, які ви можете задати собі:\n\nЧи об’єкти даних точно представляють значення «реального світу», які вони повинні моделювати? Наприклад, чи правильно вказувати вік у сотнях тисяч років?\nЧи присутнє неправильне написання назв товарів чи осіб, адрес і навіть несвоєчасних чи неактуальних даних?\n\nЦі проблеми можуть вплинути на результатати аналітичних звітів, наприклад, неправильні середні значення певних показників.",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Оцінки якості даних</span>"
    ]
  },
  {
    "objectID": "40-r-data-quality.html#повнота-completeness",
    "href": "40-r-data-quality.html#повнота-completeness",
    "title": "20  Оцінки якості даних",
    "section": "20.3 Повнота / (Completeness)",
    "text": "20.3 Повнота / (Completeness)\nПовнота визначається як очікувана всебічність. Дані можуть бути повними, навіть якщо додаткові дані відсутні. Поки дані відповідають очікуванням, вони вважаються повними.\nНаприклад, ім’я та прізвище замовника є обов’язковими, але прізвище необов’язково; тому запис можна вважати повним, навіть якщо прізвища не існує.\nПитання, які ви можете задати собі:\n\nЧи доступна вся необхідна інформація?\nЧи мають якісь дані відсутні елементи?\nАбо вони перебувають у непридатному для роботи вигляді?",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Оцінки якості даних</span>"
    ]
  },
  {
    "objectID": "40-r-data-quality.html#узгодженість-consistency",
    "href": "40-r-data-quality.html#узгодженість-consistency",
    "title": "20  Оцінки якості даних",
    "section": "20.4 Узгодженість / Consistency",
    "text": "20.4 Узгодженість / Consistency\nУзгодженість означає, що дані в усіх системах/таблицях відображають однакову інформацію та синхронізовані між собою.\nПриклади: - [x] Статус бізнес-підрозділу “закритий”, але є продажі для цього підрозділу. - [x] Статус працівника “звільнено”, але статус випалати заробіної плати містить суму відмінну від 0 за той самий період. - [x] Зафіксовано, що клієнт має у банку депозити, але у даних про депозити записи по клієнту відсутні.\nЗапитання, які ви можете поставити собі:\n\nЧи однакові значення даних у наборах даних?\nЧи існують якісь різні випадки, коли однакові екземпляри даних надають суперечливу інформацію?",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Оцінки якості даних</span>"
    ]
  },
  {
    "objectID": "40-r-data-quality.html#відповідність-conformity",
    "href": "40-r-data-quality.html#відповідність-conformity",
    "title": "20  Оцінки якості даних",
    "section": "20.5 Відповідність / Conformity",
    "text": "20.5 Відповідність / Conformity\nВідповідність означає, що дані відповідають набору стандартних визначень даних, як-от тип даних, розмір і формат. Наприклад, дата народження клієнта у форматі dd/mm/yyyy або відстань у км числом 100, а не записом 100км.\nЗапитання, які ви можете задати собі: - [x] Чи відповідають значення даних зазначеним форматам? - [x] Якщо так, то чи всі значення даних відповідають цим форматам?\nВажливо підтримувати відповідність конкретним форматам.",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Оцінки якості даних</span>"
    ]
  },
  {
    "objectID": "40-r-data-quality.html#цілісність-integrity",
    "href": "40-r-data-quality.html#цілісність-integrity",
    "title": "20  Оцінки якості даних",
    "section": "20.6 Цілісність / Integrity",
    "text": "20.6 Цілісність / Integrity\nЦілісність означає достовірність даних у взаємозв’язках і гарантує, що всі дані в базі даних можна відстежити та з’єднати з іншими даними.\nНаприклад, у базі даних клієнтів має бути дійсний клієнт, адреси та відношення/зв’язки між ними. Якщо є дані про зв’язок адреси без клієнта, то ці дані недійсні й вважаються загубленим записом.\nЗапитайте себе: - [x] Чи є якісь дані без важливих зв’язків?\nНеможливість пов’язати записи разом може призвести до дублювання у ваших системах.",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Оцінки якості даних</span>"
    ]
  },
  {
    "objectID": "40-r-data-quality.html#своєчасність-timeliness",
    "href": "40-r-data-quality.html#своєчасність-timeliness",
    "title": "20  Оцінки якості даних",
    "section": "20.7 Своєчасність / Timeliness",
    "text": "20.7 Своєчасність / Timeliness\nСвоєчасність показує, чи є інформація доступною, коли вона очікується та потрібна. Своєчасність даних дуже важлива.\nЦе відображається в: - [x] Компанії, які зобов’язані публікувати свої квартальні результати протягом певного періоду часу - [x] Обслуговування клієнтів надає клієнтам актуальну інформацію - [x] Кредитна система перевіряє активність рахунку кредитної картки в режимі реального часу\nСвоєчасність залежить від очікувань користувача. Доступність даних в Інтернеті може знадобитися для системи розподілу номерів у сфері готельного бізнесу.\nЯк бачите, якість даних є важливим питанням, яке слід враховувати, починаючи від етапу визначення цілей проекту, аж до впровадження, обслуговування та використання готово рішення у виробничі процесі підприємства.",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Оцінки якості даних</span>"
    ]
  },
  {
    "objectID": "40-r-data-quality.html#набори-даних",
    "href": "40-r-data-quality.html#набори-даних",
    "title": "20  Оцінки якості даних",
    "section": "20.8 Набори даних",
    "text": "20.8 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Оцінки якості даних</span>"
    ]
  },
  {
    "objectID": "40-r-data-quality.html#використані-та-додаткові-джерела",
    "href": "40-r-data-quality.html#використані-та-додаткові-джерела",
    "title": "20  Оцінки якості даних",
    "section": "20.9 Використані та додаткові джерела",
    "text": "20.9 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Оцінки якості даних</span>"
    ]
  },
  {
    "objectID": "41-r-data-naming.html",
    "href": "41-r-data-naming.html",
    "title": "21  Робота з неіменованими та “поганоіменованими” даними",
    "section": "",
    "text": "21.1 Іменування даних\nПершим прикладом проблем у даних можна розгянути читання неіменованих даних, тобто стопці таблиці не мають заголовків у файлі.\nСтворимо такий файл у блокноті і зчитаємо його:\ndata &lt;- read.csv(\"data/untitled.csv\")\ndata\n\n\nA data.frame: 5 × 4\n\n\nX23\nX185\nX85.7\nMale\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n41\n175\n68.3\nM\n\n\n11\n142*\n55.4\nFemale\n\n\n12\nNA\n48.2\nMan\n\n\n54\n171\nNA\nLooks like a man\n\n\n32\n168\n78.0\nF\nЗверніть увагу, що у якості стовпців взято перший рядок даних у додано X на початку. Зчитаємо дані із параметром, що вказує на відсутність заголовків:\ndata &lt;- read.csv(\"data/untitled.csv\", header = FALSE)\ndata\n\n\nA data.frame: 6 × 4\n\n\nV1\nV2\nV3\nV4\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n85.7\nMale\n\n\n41\n175\n68.3\nM\n\n\n11\n142*\n55.4\nFemale\n\n\n12\nNA\n48.2\nMan\n\n\n54\n171\nNA\nLooks like a man\n\n\n32\n168\n78.0\nF\nПроблема іменування не вирішена, дані ми уже не втратили. Передамо одночасно з читанням інформацію про назви стовпців:\ndata &lt;- read.csv(\"data/untitled.csv\", \n            header = FALSE,\n            col.names = c(\"Age\",\"Height\", \"Weight\", \"Gender\"))\ndata\n\n\nA data.frame: 6 × 4\n\n\nAge\nHeight\nWeight\nGender\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n85.7\nMale\n\n\n41\n175\n68.3\nM\n\n\n11\n142*\n55.4\nFemale\n\n\n12\nNA\n48.2\nMan\n\n\n54\n171\nNA\nLooks like a man\n\n\n32\n168\n78.0\nF\nЩе одним варіантом задання назв стовпців є використання функції colnames() як для усіх різом, так і для окремого:\ncolnames(data) &lt;- c(\"age\", \"height\", \"width\", \"gender\")\ndata\ncolnames(data)[2] &lt;- \"HEIGHT\"\ndata\n\n\nA data.frame: 6 × 4\n\n\nage\nheight\nwidth\ngender\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n85.7\nMale\n\n\n41\n175\n68.3\nM\n\n\n11\n142*\n55.4\nFemale\n\n\n12\nNA\n48.2\nMan\n\n\n54\n171\nNA\nLooks like a man\n\n\n32\n168\n78.0\nF\n\n\n\n\n\n\nA data.frame: 6 × 4\n\n\nage\nHEIGHT\nwidth\ngender\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n85.7\nMale\n\n\n41\n175\n68.3\nM\n\n\n11\n142*\n55.4\nFemale\n\n\n12\nNA\n48.2\nMan\n\n\n54\n171\nNA\nLooks like a man\n\n\n32\n168\n78.0\nF\nТакож змінювати назви стовпців можна за допомогою функції rename() з пакету dplyr:\nlibrary(dplyr)\n\ndata &lt;- data |&gt; rename(AGE = age)\ndata\n\n\nA data.frame: 6 × 4\n\n\nAGE\nHEIGHT\nwidth\ngender\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n85.7\nMale\n\n\n41\n175\n68.3\nM\n\n\n11\n142*\n55.4\nFemale\n\n\n12\nNA\n48.2\nMan\n\n\n54\n171\nNA\nLooks like a man\n\n\n32\n168\n78.0\nF",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Робота з неіменованими та \"поганоіменованими\" даними</span>"
    ]
  },
  {
    "objectID": "41-r-data-naming.html#заміна-назв-стовпців-data.frame",
    "href": "41-r-data-naming.html#заміна-назв-стовпців-data.frame",
    "title": "21  Робота з неіменованими та “поганоіменованими” даними",
    "section": "21.2 Заміна назв стовпців data.frame ",
    "text": "21.2 Заміна назв стовпців data.frame \nЗчитаємо файл, що містить інформацію про осіб, але уже має іменовані стовпці:\n\ndata &lt;- read.csv(\"data/badtitled.csv\")\ndata\n\n\nA data.frame: 13 × 5\n\n\nPerson.Age\nPerson__Height\nperson.Weight\nPerson.Gender\nempty\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\n23\n185\nNA\nMale\nNA\n\n\n41\n175\n68.3\nM\nNA\n\n\n11\n142*\n55.4\nFemale\nNA\n\n\n12\nNA\n48.2\nMan\nNA\n\n\n54\n191\nNA\nfemale\nNA\n\n\n32\n168\n78.0\nF\nNA\n\n\n22\nNA\n54.0\nmale.\nNA\n\n\n21\n165\nNA\nm\nNA\n\n\n14\nNA\n90.2\nMan\nNA\n\n\n51\n250\nNA\nfemale\nNA\n\n\n41\n20\n81.0\nF\nNA\n\n\n66\nNA\n59.0\nmale.\nNA\n\n\n71\n171\nNA\nm\nNA\n\n\n\n\n\nШвидко змінити назви стовпців та привести їх до однакового стилю можна за домогою бібліотеки janitor:\n\n#install.packages(\"janitor\")\n\n\nlibrary(janitor)\nclean &lt;- clean_names(data)\ncolnames(clean)\n\n\n'person_age''person_height''person_weight''person_gender''empty'",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Робота з неіменованими та \"поганоіменованими\" даними</span>"
    ]
  },
  {
    "objectID": "41-r-data-naming.html#набори-даних",
    "href": "41-r-data-naming.html#набори-даних",
    "title": "21  Робота з неіменованими та “поганоіменованими” даними",
    "section": "21.3 Набори даних",
    "text": "21.3 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Робота з неіменованими та \"поганоіменованими\" даними</span>"
    ]
  },
  {
    "objectID": "41-r-data-naming.html#використані-та-додаткові-джерела",
    "href": "41-r-data-naming.html#використані-та-додаткові-джерела",
    "title": "21  Робота з неіменованими та “поганоіменованими” даними",
    "section": "21.4 Використані та додаткові джерела",
    "text": "21.4 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Робота з неіменованими та \"поганоіменованими\" даними</span>"
    ]
  },
  {
    "objectID": "42-r-cleaning-text.html",
    "href": "42-r-cleaning-text.html",
    "title": "22  Підготовка та очистка текстової інформації",
    "section": "",
    "text": "22.1 Набори даних",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Підготовка та очистка текстової інформації</span>"
    ]
  },
  {
    "objectID": "42-r-cleaning-text.html#набори-даних",
    "href": "42-r-cleaning-text.html#набори-даних",
    "title": "22  Підготовка та очистка текстової інформації",
    "section": "",
    "text": "https://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Підготовка та очистка текстової інформації</span>"
    ]
  },
  {
    "objectID": "42-r-cleaning-text.html#використані-та-додаткові-джерела",
    "href": "42-r-cleaning-text.html#використані-та-додаткові-джерела",
    "title": "22  Підготовка та очистка текстової інформації",
    "section": "22.2 Використані та додаткові джерела",
    "text": "22.2 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Підготовка та очистка текстової інформації</span>"
    ]
  },
  {
    "objectID": "43-r-missing-data.html",
    "href": "43-r-missing-data.html",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "",
    "text": "23.1 Перевірка наявності пропусків у даних\nПакет MICE (Multivariate Imputation via Chained Equations)\nlibrary(mice)\nmd.pattern(data)\n\n\nA matrix: 4 × 6 of type dbl\n\n\n\nperson_age\nperson_gender\nperson_height\nperson_weight\nempty\n\n\n\n\n\n4\n1\n1\n1\n1\n0\n1\n\n\n5\n1\n1\n1\n0\n0\n2\n\n\n4\n1\n1\n0\n1\n0\n2\n\n\n\n0\n0\n4\n5\n13\n22\n#install.packages(\"VIM\")\nlibrary(VIM)\nmice_plot &lt;- aggr(data, \n                  col=c('navyblue','yellow'),\n                  numbers=TRUE, \n                  sortVars=TRUE,\n                  labels=names(data), \n                  cex.axis=.7,\n                  gap=3, \n                  ylab=c(\"Missing data\",\"Pattern\"))\nmice_plot\n\n\n Variables sorted by number of missings: \n      Variable     Count\n         empty 1.0000000\n person_weight 0.3846154\n person_height 0.3076923\n    person_age 0.0000000\n person_gender 0.0000000\n\n\n\n Missings in variables:\n      Variable Count\n person_height     4\n person_weight     5\n         empty    13\n#install.packages(\"Amalia\")\nlibrary(Amelia)\nAmelia::missmap(data)\nТакож можна скористатися альтернативними макетами: missForest, mi.",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Заміна пропусків у даних (Missing Value Imputation)</span>"
    ]
  },
  {
    "objectID": "43-r-missing-data.html#видалення-пустих-рядків-та-сповпців-у-data.frame",
    "href": "43-r-missing-data.html#видалення-пустих-рядків-та-сповпців-у-data.frame",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.2 Видалення пустих рядків та сповпців у data.frame",
    "text": "23.2 Видалення пустих рядків та сповпців у data.frame\nПереглянемо стовпці, що містять пропуски:\n\n# Переглянемо список стовпців з пропусками\ncolnames(data)[apply(data, 2, anyNA)]\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\n\n'person_height''person_weight''empty'\n\n\nФункція complete.cases повертає логічні значення\n\ncomplete.cases(data) # бо є стовпець Empty\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\n\nFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSE\n\n\nТакож видаляти стовпці та рядки з data.frame можна за допомогою пакету janitor.\n\nlibrary(janitor)\ndata_cleaned &lt;- remove_empty(data, which = c(\"rows\",\"cols\"), quiet = FALSE)\ndata_cleaned\n# Видаляємо повністю пусті\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windowsNo empty rows to remove.\n\nRemoving 1 empty columns of 5 columns total (Removed: empty).\n\n\n\n\nA data.frame: 13 × 4\n\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n23\n185\nNA\nmale\n\n\n2\n41\n175\n68.3\nmale\n\n\n3\n11\n142\n55.4\nfemale\n\n\n4\n12\nNA\n48.2\nmale\n\n\n5\n54\n191\nNA\nfemale\n\n\n6\n32\n168\n78.0\nfemale\n\n\n7\n22\nNA\n54.0\nmale\n\n\n8\n21\n165\nNA\nmale\n\n\n9\n14\nNA\n90.2\nmale\n\n\n10\n51\n250\nNA\nfemale\n\n\n11\n41\n20\n81.0\nfemale\n\n\n12\n66\nNA\n59.0\nmale\n\n\n13\n71\n171\nNA\nmale\n\n\n\n\n\n\nwrite.csv(data_cleaned, file = \"data/cleaned_titled2.csv\", row.names = F)\n\nЯк бачимо, колонка empty була видалена.\nЩоб переглянути усі записи, що не мають пропусків скористаємося функцією na.omit():\n\nna.omit(data_cleaned)\n\n\nA data.frame: 4 × 4\n\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n2\n41\n175\n68.3\nmale\n\n\n3\n11\n142\n55.4\nfemale\n\n\n6\n32\n168\n78.0\nfemale\n\n\n11\n41\n20\n81.0\nfemale\n\n\n\n\n\nТаким чином пропущені значення будуть видалені з датасети, якщо інформацію переприсвоїти data &lt;- na.omit(data)",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Заміна пропусків у даних (Missing Value Imputation)</span>"
    ]
  },
  {
    "objectID": "43-r-missing-data.html#заміна-пропусків-у-data.frame",
    "href": "43-r-missing-data.html#заміна-пропусків-у-data.frame",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.3 Заміна пропусків у data.frame",
    "text": "23.3 Заміна пропусків у data.frame\nІснує ряд підходів, що використовуються для заміни пропущених значень у датасеті:\nЗаміна на 0 * Вставте пропущені значення нулем\nЗаміна на медіану/середнє значення * Для числових змінних - середнє або медіана, мінімум, максимум * Для категоріальних змінних - мода (бувають випадки, коли моду доцільно використовувати і для числових)\nСегментна заміна * Визначення сегментів * Обчислення середнього/медіани/моди для сегментів * Замінити значення по сегментах * Наприклад, ми можемо сказати, що кількість опадів майже не змінюється для міст у певній області України, у такому випадку ми можемо для усіх міст з пропусками записати значення середнє по регіону.\nІнтелектуальна заміна (Частковий випадок сегментної заміни) * Заміна значень з використанням методів машинного навчання\n\n23.3.1 Заміна пропусків на нуль (0)\n\ndata &lt;- read.csv(\"data/cleaned_titled2.csv\")\ndata\n\n\nA data.frame: 13 × 4\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\nNA\nmale\n\n\n41\n175\n68.3\nmale\n\n\n11\n142\n55.4\nfemale\n\n\n12\nNA\n48.2\nmale\n\n\n54\n191\nNA\nfemale\n\n\n32\n168\n78.0\nfemale\n\n\n22\nNA\n54.0\nmale\n\n\n21\n165\nNA\nmale\n\n\n14\nNA\n90.2\nmale\n\n\n51\n250\nNA\nfemale\n\n\n41\n20\n81.0\nfemale\n\n\n66\nNA\n59.0\nmale\n\n\n71\n171\nNA\nmale\n\n\n\n\n\nЗамінимо інформацію про вагу з пропусками на 0:\n\ndata_w0 &lt;- data |&gt; \n    mutate(person_weight = ifelse(is.na(person_weight), 0, person_weight))\ndata_w0\n\n\nA data.frame: 13 × 4\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n0.0\nmale\n\n\n41\n175\n68.3\nmale\n\n\n11\n142\n55.4\nfemale\n\n\n12\nNA\n48.2\nmale\n\n\n54\n191\n0.0\nfemale\n\n\n32\n168\n78.0\nfemale\n\n\n22\nNA\n54.0\nmale\n\n\n21\n165\n0.0\nmale\n\n\n14\nNA\n90.2\nmale\n\n\n51\n250\n0.0\nfemale\n\n\n41\n20\n81.0\nfemale\n\n\n66\nNA\n59.0\nmale\n\n\n71\n171\n0.0\nmale\n\n\n\n\n\n\n# Без dplyr\ndata_w0 &lt;- data\ndata_w0[is.na(data_w0$person_weight), \"person_weight\"] &lt;- 0\ndata_w0\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\n\nA data.frame: 13 × 4\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n0.0\nmale\n\n\n41\n175\n68.3\nmale\n\n\n11\n142\n55.4\nfemale\n\n\n12\nNA\n48.2\nmale\n\n\n54\n191\n0.0\nfemale\n\n\n32\n168\n78.0\nfemale\n\n\n22\nNA\n54.0\nmale\n\n\n21\n165\n0.0\nmale\n\n\n14\nNA\n90.2\nmale\n\n\n51\n250\n0.0\nfemale\n\n\n41\n20\n81.0\nfemale\n\n\n66\nNA\n59.0\nmale\n\n\n71\n171\n0.0\nmale\n\n\n\n\n\nЗробити заміну для усіх числових стовпців:\n\nlibrary(tidyr) # for replace_na()\ndata_all &lt;- data |&gt; \n    mutate_if(is.numeric , replace_na, replace = 0)\ndata_all\n\n\nA data.frame: 13 × 4\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n0.0\nmale\n\n\n41\n175\n68.3\nmale\n\n\n11\n142\n55.4\nfemale\n\n\n12\n0\n48.2\nmale\n\n\n54\n191\n0.0\nfemale\n\n\n32\n168\n78.0\nfemale\n\n\n22\n0\n54.0\nmale\n\n\n21\n165\n0.0\nmale\n\n\n14\n0\n90.2\nmale\n\n\n51\n250\n0.0\nfemale\n\n\n41\n20\n81.0\nfemale\n\n\n66\n0\n59.0\nmale\n\n\n71\n171\n0.0\nmale",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Заміна пропусків у даних (Missing Value Imputation)</span>"
    ]
  },
  {
    "objectID": "43-r-missing-data.html#числова-заміна-пропусків",
    "href": "43-r-missing-data.html#числова-заміна-пропусків",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.4 Числова заміна пропусків",
    "text": "23.4 Числова заміна пропусків\nЗаміна на константи або обчислені значення є стандарним підходом. Так, наприклад, заміна певного значення на середнє матиме вигляд:\n\ndata_m &lt;- data |&gt; \n    mutate(person_weight = ifelse(is.na(person_weight), mean(data$person_weight, na.rm = T), person_weight))\ndata_m\n\n\nA data.frame: 13 × 4\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n66.7625\nmale\n\n\n41\n175\n68.3000\nmale\n\n\n11\n142\n55.4000\nfemale\n\n\n12\nNA\n48.2000\nmale\n\n\n54\n191\n66.7625\nfemale\n\n\n32\n168\n78.0000\nfemale\n\n\n22\nNA\n54.0000\nmale\n\n\n21\n165\n66.7625\nmale\n\n\n14\nNA\n90.2000\nmale\n\n\n51\n250\n66.7625\nfemale\n\n\n41\n20\n81.0000\nfemale\n\n\n66\nNA\n59.0000\nmale\n\n\n71\n171\n66.7625\nmale\n\n\n\n\n\nЗаміна на min, max, median не відрізняється.\nЯкщо виникає потреба замінити, наприклад, усі значення на медіану у всіх стовпцях за один прохід можна скористатися функцією mutate_if():\n\ndata_all &lt;- data |&gt; \n    mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm = T), x))\ndata_all\n\n\nA data.frame: 13 × 4\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n63.65\nmale\n\n\n41\n175\n68.30\nmale\n\n\n11\n142\n55.40\nfemale\n\n\n12\n171\n48.20\nmale\n\n\n54\n191\n63.65\nfemale\n\n\n32\n168\n78.00\nfemale\n\n\n22\n171\n54.00\nmale\n\n\n21\n165\n63.65\nmale\n\n\n14\n171\n90.20\nmale\n\n\n51\n250\n63.65\nfemale\n\n\n41\n20\n81.00\nfemale\n\n\n66\n171\n59.00\nmale\n\n\n71\n171\n63.65\nmale\n\n\n\n\n\nРозглянемо кілька бібліотек для перевірки даних на наявність пропусків…\nЩе одним із варіантів заміни значень може бути використання бібліотеки Hmisc:\n\n#install.packages(\"Hmisc\")\n\n\nlibrary(Hmisc)\ndata_wm &lt;- data |&gt; \n    mutate(person_weight = impute(data$person_weight, fun = mean)) # mean imputation\n# Аналогічно можна замінити на min,max, median чи інші функції\ndata_wm \n# * Значення із * - замінені\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\n\nA data.frame: 13 × 4\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;impute&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n66.7625\nmale\n\n\n41\n175\n68.3000\nmale\n\n\n11\n142\n55.4000\nfemale\n\n\n12\nNA\n48.2000\nmale\n\n\n54\n191\n66.7625\nfemale\n\n\n32\n168\n78.0000\nfemale\n\n\n22\nNA\n54.0000\nmale\n\n\n21\n165\n66.7625\nmale\n\n\n14\nNA\n90.2000\nmale\n\n\n51\n250\n66.7625\nfemale\n\n\n41\n20\n81.0000\nfemale\n\n\n66\nNA\n59.0000\nmale\n\n\n71\n171\n66.7625\nmale\n\n\n\n\n\n\n\n23.4.0.1 Hot deck imputation (як перекласти???)\nМетод Hot deck imputation передбачає, що пропущені значення обчислюються шляхом копіювання значень із подібних записів у тому ж наборі даних.\nОсновне питання при Hot deck imputation полягає в тому, як вибрати значення заміни. Одним із поширених підходів є випадковий відбір:\n\n# set.seed(1)\ndata_hot &lt;- data |&gt; \n    mutate(person_weight = impute(data$person_weight, \"random\")) \ndata_hot \n\n\nA data.frame: 13 × 4\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;impute&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n59.0\nmale\n\n\n41\n175\n68.3\nmale\n\n\n11\n142\n55.4\nfemale\n\n\n12\nNA\n48.2\nmale\n\n\n54\n191\n55.4\nfemale\n\n\n32\n168\n78.0\nfemale\n\n\n22\nNA\n54.0\nmale\n\n\n21\n165\n54.0\nmale\n\n\n14\nNA\n90.2\nmale\n\n\n51\n250\n90.2\nfemale\n\n\n41\n20\n81.0\nfemale\n\n\n66\nNA\n59.0\nmale\n\n\n71\n171\n54.0\nmale\n\n\n\n\n\nВихідне значення залежить від значення seed.",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Заміна пропусків у даних (Missing Value Imputation)</span>"
    ]
  },
  {
    "objectID": "43-r-missing-data.html#сегментна-заміна-пропусків",
    "href": "43-r-missing-data.html#сегментна-заміна-пропусків",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.5 Сегментна заміна пропусків",
    "text": "23.5 Сегментна заміна пропусків\nЗаміна по сегментах часто дозволяє будувати точніші математичні моделі, адже групові середні краще описують явища і процеси, ніж загальні для всієї вибірки.\nЗнайдемо середні значення ваги за статтю та використаємо ці значення для заміни пропусків у даних.\n\ndata_sgm &lt;- data |&gt; \n                group_by(person_gender) |&gt;\n                mutate(person_weight = replace_na(person_weight, mean(person_weight, na.rm = TRUE)))\ndata_sgm\n\n\nA grouped_df: 13 × 4\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n63.94000\nmale\n\n\n41\n175\n68.30000\nmale\n\n\n11\n142\n55.40000\nfemale\n\n\n12\nNA\n48.20000\nmale\n\n\n54\n191\n71.46667\nfemale\n\n\n32\n168\n78.00000\nfemale\n\n\n22\nNA\n54.00000\nmale\n\n\n21\n165\n63.94000\nmale\n\n\n14\nNA\n90.20000\nmale\n\n\n51\n250\n71.46667\nfemale\n\n\n41\n20\n81.00000\nfemale\n\n\n66\nNA\n59.00000\nmale\n\n\n71\n171\n63.94000\nmale\n\n\n\n\n\nТакож можна здійснити заміну значень по усіх стовпцях датасету за один раз. Проте не варто такий підхід використовувати постійно, а враховувати бізнес-логіку процесів, що вивчаються.\n\ndata_sgm2 &lt;- data %&gt;% \n  group_by(person_gender) %&gt;% \n    mutate(\n      across(everything(), ~replace_na(.x, min(.x, na.rm = TRUE)))\n    )\ndata_sgm2\n\n\nA grouped_df: 13 × 4\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n23\n185\n48.2\nmale\n\n\n41\n175\n68.3\nmale\n\n\n11\n142\n55.4\nfemale\n\n\n12\n165\n48.2\nmale\n\n\n54\n191\n55.4\nfemale\n\n\n32\n168\n78.0\nfemale\n\n\n22\n165\n54.0\nmale\n\n\n21\n165\n48.2\nmale\n\n\n14\n165\n90.2\nmale\n\n\n51\n250\n55.4\nfemale\n\n\n41\n20\n81.0\nfemale\n\n\n66\n165\n59.0\nmale\n\n\n71\n171\n48.2\nmale\n\n\n\n\n\nЯкщо ж є потреба замінювати по окремих стовпцях, то їх можна вказати замість everything(): across(c(\"person_height\", \"person_weight\"), ~replace_na(.x, min(.x, na.rm = TRUE))).\nІншим варіантом може бути вказання номерів колонок: across(c(1,3), ~replace_na(.x, min(.x, na.rm = TRUE)))",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Заміна пропусків у даних (Missing Value Imputation)</span>"
    ]
  },
  {
    "objectID": "43-r-missing-data.html#інтелектуальні-методи-заміни",
    "href": "43-r-missing-data.html#інтелектуальні-методи-заміни",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.6 Інтелектуальні методи заміни",
    "text": "23.6 Інтелектуальні методи заміни\nТеоретично інтелектуальні методи заміни пропусків є найкращими, адже враховують математичні залежності у даних.\nhttps://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4\n\nlibrary(VIM)\ndata_knn &lt;- kNN(data)\ndata_knn\n\n\nA data.frame: 13 × 8\n\n\nperson_age\nperson_height\nperson_weight\nperson_gender\nperson_age_imp\nperson_height_imp\nperson_weight_imp\nperson_gender_imp\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n\n\n\n\n23\n185\n59.0\nmale\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n41\n175\n68.3\nmale\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n11\n142\n55.4\nfemale\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n12\n168\n48.2\nmale\nFALSE\nTRUE\nFALSE\nFALSE\n\n\n54\n191\n68.3\nfemale\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n32\n168\n78.0\nfemale\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n22\n168\n54.0\nmale\nFALSE\nTRUE\nFALSE\nFALSE\n\n\n21\n165\n59.0\nmale\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n14\n168\n90.2\nmale\nFALSE\nTRUE\nFALSE\nFALSE\n\n\n51\n250\n68.3\nfemale\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n41\n20\n81.0\nfemale\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n66\n168\n59.0\nmale\nFALSE\nTRUE\nFALSE\nFALSE\n\n\n71\n171\n59.0\nmale\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n\n\n\nЩе одним схожим методом заміни пропусків може бути здійснення прогнозів на основі регресії чи складніших математичних методів пропусків.",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Заміна пропусків у даних (Missing Value Imputation)</span>"
    ]
  },
  {
    "objectID": "43-r-missing-data.html#набори-даних",
    "href": "43-r-missing-data.html#набори-даних",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.7 Набори даних",
    "text": "23.7 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Заміна пропусків у даних (Missing Value Imputation)</span>"
    ]
  },
  {
    "objectID": "43-r-missing-data.html#використані-та-додаткові-джерела",
    "href": "43-r-missing-data.html#використані-та-додаткові-джерела",
    "title": "23  Заміна пропусків у даних (Missing Value Imputation)",
    "section": "23.8 Використані та додаткові джерела",
    "text": "23.8 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Заміна пропусків у даних (Missing Value Imputation)</span>"
    ]
  },
  {
    "objectID": "44-r-outliers.html",
    "href": "44-r-outliers.html",
    "title": "24  Аналіз та обробка статистичних викидів у даних",
    "section": "",
    "text": "24.1 Додаткові прийоми очистки даних",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Аналіз та обробка статистичних викидів у даних</span>"
    ]
  },
  {
    "objectID": "44-r-outliers.html#додаткові-прийоми-очистки-даних",
    "href": "44-r-outliers.html#додаткові-прийоми-очистки-даних",
    "title": "24  Аналіз та обробка статистичних викидів у даних",
    "section": "",
    "text": "24.1.1 Видалення дублікатів\n\ndf &lt;- data.frame(X = c(1,1,2,1,3,2,1), Y = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\"))\ndf\n\n\nA data.frame: 7 × 2\n\n\nX\nY\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\nA\n\n\n1\nB\n\n\n2\nC\n\n\n1\nA\n\n\n3\nB\n\n\n2\nC\n\n\n1\nA\n\n\n\n\n\n\ndf |&gt; distinct()\n\n\nA data.frame: 4 × 2\n\n\nX\nY\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\nA\n\n\n1\nB\n\n\n2\nC\n\n\n3\nB",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Аналіз та обробка статистичних викидів у даних</span>"
    ]
  },
  {
    "objectID": "44-r-outliers.html#набори-даних",
    "href": "44-r-outliers.html#набори-даних",
    "title": "24  Аналіз та обробка статистичних викидів у даних",
    "section": "24.2 Набори даних",
    "text": "24.2 Набори даних\n\nhttps://github.com/kleban/r-book-published/tree/main/datasets/untitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/badtitled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/cleaned_titled2.csv\nhttps://github.com/kleban/r-book-published/tree/main/datasets/river_eco.csv",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Аналіз та обробка статистичних викидів у даних</span>"
    ]
  },
  {
    "objectID": "44-r-outliers.html#використані-та-додаткові-джерела",
    "href": "44-r-outliers.html#використані-та-додаткові-джерела",
    "title": "24  Аналіз та обробка статистичних викидів у даних",
    "section": "24.3 Використані та додаткові джерела",
    "text": "24.3 Використані та додаткові джерела\n\nKPMG Virtual Internship\nAn introduction to data cleaning with R / Edwin de Jonge, Mark van der Loo, 2013\nAnomaly Detection in R\nK-nearest Neighbor: The maths behind it, how it works and an example\nQuantile. Wikipedia",
    "crumbs": [
      "ТЕМА 4. СПОСОБИ ОЧИСТКИ ДАНИХ",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Аналіз та обробка статистичних викидів у даних</span>"
    ]
  },
  {
    "objectID": "etl-feature-engineering.html",
    "href": "etl-feature-engineering.html",
    "title": "25  Feature engineering in R",
    "section": "",
    "text": "25.1 What’s Feature Engineering?\nFeature engineering is the most important technique used in creating machine learning models.\nFeature Engineering is a basic term used to cover many operations that are performed on the variables(features) to fit them into the algorithm. It helps in increasing the accuracy of the model thereby enhances the results of the predictions. Feature Engineered machine learning models perform better on data than basic machine learning models. The following aspects of feature engineering are as follows [1]:\nA \"feature\" in the context of predictive modeling is just another name for a predictor variable. Feature engineering is the general term for creating and manipulating predictors so that a good predictive model can be created.",
    "crumbs": [
      "ТЕМА 5. КОНСТРУЮВАННЯ ОЗНАК",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Feature engineering in R</span>"
    ]
  },
  {
    "objectID": "etl-feature-engineering.html#whats-feature-engineering",
    "href": "etl-feature-engineering.html#whats-feature-engineering",
    "title": "25  Feature engineering in R",
    "section": "",
    "text": "Feature Scaling: It is done to get the features on the same scale( for eg. Euclidean distance).\nFeature Transformation: It is done to normalize the data(feature) by a function.\nFeature Construction: It is done to create new features based on original descriptors to improve the accuracy of the predictive model.",
    "crumbs": [
      "ТЕМА 5. КОНСТРУЮВАННЯ ОЗНАК",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Feature engineering in R</span>"
    ]
  },
  {
    "objectID": "etl-feature-engineering.html#feature-scaling",
    "href": "etl-feature-engineering.html#feature-scaling",
    "title": "25  Feature engineering in R",
    "section": "25.2 Feature Scaling",
    "text": "25.2 Feature Scaling\nFeature Scaling refers to putting the values in the same range or same scale so that no variable is dominated by the other.\nMost of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Euclidean distance between two data points in their computations, this is a problem.\nIf left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. To suppress this effect, we need to bring all features to the same level of magnitudes. This can be achieved by scaling.\nHere’s the curious thing about feature scaling – it improves (significantly) the performance of some machine learning algorithms and does not work at all for others.\nAlso, what’s the difference between normalization and standardization? These are two of the most commonly used feature scaling techniques in machine learning but a level of ambiguity exists in their understanding.\n\n\n25.2.1 Normalization\n\n25.2.1.1 Theory\nNormalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\nHere’s the formula for normalization:\n\n\\(X' = \\frac{X-X_{min}}{X_{max} - X_{min}}\\)\n\nHere, \\(X_{max}\\) and \\(X_{min}\\) are the maximum and the minimum values of the feature respectively.\nWhen the value of \\(X\\) is the minimum value in the column, the numerator will be \\(0\\), and hence \\(X'\\) is \\(0\\).\nOn the other hand, when the value of \\(X\\) is the maximum value in the column, the numerator is equal to the denominator and thus the value of \\(X'\\) is \\(1\\).\nIf the value of \\(X\\) is between the minimum and the maximum value, then the value of \\(X'\\) is between \\(0\\) and \\(1\\).\n\n\n\n25.2.1.2 Practice\nSo, let’s implement own normalization function.\n\n# Lets use client churn dataset from telco: https://www.kaggle.com/blastchar/telco-customer-churn\nchurn_data &lt;- read.csv(\"data/telecom_users.csv\")\nhead(churn_data)\n\n\nA data.frame: 6 × 22\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n⋯\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n⋯\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\n⋯\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\n⋯\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n\n\n4\n6739\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\n⋯\nNo\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n\n\n5\n432\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\n⋯\nYes\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\n⋯\nYes\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n\n\n\n\n\n\nstr(churn_data)\n\n'data.frame':   5986 obs. of  22 variables:\n $ X               : int  1869 4528 6344 6739 432 2215 5260 6001 1480 5137 ...\n $ customerID      : chr  \"7010-BRBUU\" \"9688-YGXVR\" \"9286-DOJGF\" \"6994-KERXL\" ...\n $ gender          : chr  \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ SeniorCitizen   : int  0 0 1 0 0 0 0 0 0 1 ...\n $ Partner         : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Dependents      : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n $ tenure          : int  72 44 38 4 2 70 33 1 39 55 ...\n $ PhoneService    : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ MultipleLines   : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ InternetService : chr  \"No\" \"Fiber optic\" \"Fiber optic\" \"DSL\" ...\n $ OnlineSecurity  : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ OnlineBackup    : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ DeviceProtection: chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ TechSupport     : chr  \"No internet service\" \"No\" \"No\" \"No\" ...\n $ StreamingTV     : chr  \"No internet service\" \"Yes\" \"No\" \"No\" ...\n $ StreamingMovies : chr  \"No internet service\" \"No\" \"No\" \"Yes\" ...\n $ Contract        : chr  \"Two year\" \"Month-to-month\" \"Month-to-month\" \"Month-to-month\" ...\n $ PaperlessBilling: chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ PaymentMethod   : chr  \"Credit card (automatic)\" \"Credit card (automatic)\" \"Bank transfer (automatic)\" \"Electronic check\" ...\n $ MonthlyCharges  : chr  \"24.1\" \"88.15\" \"74.95\" \"55.9\" ...\n $ TotalCharges    : num  1735 3973 2870 238 120 ...\n $ Churn           : chr  \"No\" \"No\" \"Yes\" \"No\" ...\n\n\n\n# next check summary of values\nsummary(churn_data)\n\n# check TotalCharges field\n\n       X         customerID           gender          SeniorCitizen   \n Min.   :   0   Length:5986        Length:5986        Min.   :0.0000  \n 1st Qu.:1777   Class :character   Class :character   1st Qu.:0.0000  \n Median :3546   Mode  :character   Mode  :character   Median :0.0000  \n Mean   :3534                                         Mean   :0.1614  \n 3rd Qu.:5292                                         3rd Qu.:0.0000  \n Max.   :7042                                         Max.   :1.0000  \n                                                                      \n   Partner           Dependents            tenure      PhoneService      \n Length:5986        Length:5986        Min.   : 0.00   Length:5986       \n Class :character   Class :character   1st Qu.: 9.00   Class :character  \n Mode  :character   Mode  :character   Median :29.00   Mode  :character  \n                                       Mean   :32.47                     \n                                       3rd Qu.:56.00                     \n                                       Max.   :72.00                     \n                                                                         \n MultipleLines      InternetService    OnlineSecurity     OnlineBackup      \n Length:5986        Length:5986        Length:5986        Length:5986       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n DeviceProtection   TechSupport        StreamingTV        StreamingMovies   \n Length:5986        Length:5986        Length:5986        Length:5986       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Contract         PaperlessBilling   PaymentMethod      MonthlyCharges    \n Length:5986        Length:5986        Length:5986        Length:5986       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  TotalCharges       Churn          \n Min.   :  18.8   Length:5986       \n 1st Qu.: 404.3   Class :character  \n Median :1412.2   Mode  :character  \n Mean   :2298.1                     \n 3rd Qu.:3847.0                     \n Max.   :8684.8                     \n NA's   :10                         \n\n\nNext, lets build histogram of Income and check how it splited with ggplot2:\n\n#install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nggplot() - function for building charts\ndata - first parameter - dataset\naes() - authetics - visualition axis / Construct aesthetic mappings\ngeom_CHART_TYPE() - set the chart type\ngeom_histogram() - Histograms and frequency polygons\n//theme_set() - theme configutation\n\nlibrary(ggplot2)\nggplot(data = churn_data, aes(x=TotalCharges)) + geom_histogram(bins = 15)\n\n# try theme\n\nWarning message:\n\"Removed 10 rows containing non-finite values (stat_bin).\"\n\n\n\n\n\n\n\n\n\n\n# Lets replace missing with 0 zero for TotalCharges \nlibrary(magrittr) # if pipe not loaded\nlibrary(dplyr) # for mutate function\nchurn_data &lt;- churn_data %&gt;%\n            mutate(TotalCharges = ifelse(is.na(TotalCharges), 0 , TotalCharges))\n\nggplot(churn_data, aes(x=TotalCharges)) + geom_histogram(bins = 15)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\n\n\n\n\n\n\n# Lets implement own normalization function by fomula explained earlie\nnormalizeData &lt;- function(x) {\n    return ((x - min(x)) / (max(x) - min(x)))\n}\n\n\nlibrary(dplyr)\n# Normalize TotalCharges\nchurn_data &lt;- churn_data |&gt;\n    filter(!is.na(TotalCharges)) |&gt;\n    mutate(TotalChargesNorm = normalizeData(TotalCharges))\n\nchurn_data %&gt;% head() # check the last columns\n\n\nA data.frame: 6 × 23\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n⋯\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\nTotalChargesNorm\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n⋯\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n0.19799792\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\n⋯\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n0.45631202\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\n⋯\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n0.32899261\n\n\n4\n6739\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\n⋯\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n0.02535195\n\n\n5\n432\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\n⋯\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n0.01162012\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\n⋯\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n0.38672975\n\n\n\n\n\n\n#summary for the last field\nsummary(churn_data$TotalChargesNorm)\n\n#its from 1 to zero\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.04449 0.16078 0.26301 0.44174 1.00000 \n\n\n\n# And lets make a histogram\nggplot(churn_data, aes(x=TotalChargesNorm)) + geom_histogram(bins = 15)\n\n\n\n\n\n\n\n\nWe observe identical histograms even though the TotalCharges / TotalChargesNorm axis is rescaled.\nTherefore we show that normalization didn’t affect the distribution properties of the rescaled data.\n\n\n\n\n25.2.2 Standardization\n\n25.2.2.1 Theory\nStandardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\nHere’s the formula for standardization:\n\n\\(X' = \\frac{X-\\mu}{\\sigma}\\)\n\nFeature scaling: \\(\\mu\\) is the mean of the feature values and Feature scaling: \\(\\sigma\\) is the standard deviation of the feature values. Note that in this case, the values are not restricted to a particular range.\nNow, the big question in your mind must be when should we use normalization and when should we use standardization?\nNormalization vs. standardization is an eternal question among machine learning newcomers. Let me elaborate on the answer in this section.\nNormalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\nStandardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization. However, at the end of the day, the choice of using normalization or standardization will depend on your problem and the machine learning algorithm you are using. There is no hard and fast rule to tell you when to normalize or standardize your data. You can always start by fitting your model to raw, normalized and standardized data and compare the performance for best results.\nIt is a good practice to fit the scaler on the training data and then use it to transform the testing data. This would avoid any data leakage during the model testing process. Also, the scaling of target values is generally not required.\n\n\n\n25.2.2.2 Practice\nLets write own function for standartization:\n\n# if sdev is NA - calculate start deviation from data\n\nstandartize &lt;- function(data) {    \n    sdev = sd(data, na.rm = TRUE)  \n    data &lt;- (data - mean(data, na.rm = T)) / sdev\n    return (data)    \n}\n\n\n# Normalize TotalCharges\nchurn_data &lt;- churn_data %&gt;%\n    mutate(TotalChargesStand = standartize(TotalCharges))\n\nchurn_data %&gt;% head() # check the last columns\n\n\nA data.frame: 6 × 24\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n⋯\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\nTotalChargesNorm\nTotalChargesStand\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n⋯\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n0.19799792\n-0.2477481\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\n⋯\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n0.45631202\n0.7366076\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\n⋯\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n0.32899261\n0.2514325\n\n\n4\n6739\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\n⋯\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n0.02535195\n-0.9056488\n\n\n5\n432\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\n⋯\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n0.01162012\n-0.9579766\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\n⋯\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n0.38672975\n0.4714509\n\n\n\n\n\nLets compare data distribution fot normalization and standartization.\n\n#install.packages(\"gridExtra\") # to view 2+ ggplots \nlibrary(gridExtra)\nn_plot &lt;- ggplot(churn_data, aes(x=TotalChargesNorm)) + geom_histogram(bins = 15)\ns_plot &lt;- ggplot(churn_data, aes(x=TotalChargesStand)) + geom_histogram(bins = 15)\ninit_plot &lt;- ggplot(churn_data, aes(x=TotalCharges)) + geom_histogram(bins = 15)\ngrid.arrange(n_plot, init_plot, s_plot, ncol=3) # from gridExtra\n\n# data distribution changed after standartisation scaling\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\n\n\n\n\n\n\n\n\n\nSo, lets use stardart R function for scaling and compare results:\n\n# the next \nchurn_data &lt;- churn_data %&gt;%\n        mutate(TotalChargesScaled = as.numeric(scale(TotalCharges))) \n\ns1_plot &lt;- ggplot(churn_data, aes(x=TotalChargesStand)) + geom_histogram(bins = 15)\ns2_plot &lt;- ggplot(churn_data, aes(x=TotalChargesScaled)) + geom_histogram(bins = 15)\ngrid.arrange(s1_plot, s2_plot, ncol=2) # it looks like we created the same function \n\n\n\n\n\n\n\n\n\nmean(churn_data$TotalCharges)\n\n2298.06061746988\n\n\nLook like default function has the same result as our.\n\nhead(churn_data) # check last two columns, its the same\n\n\nA data.frame: 6 × 25\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n...\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\nTotalChargesNorm\nTotalChargesStand\nTotalChargesScaled\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n...\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n0.19973402\n-0.2460559\n-0.2460559\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\n...\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n0.45748895\n0.7382838\n0.7382838\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\n...\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n0.33044515\n0.2531165\n0.2531165\n\n\n4\n6739\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\n...\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n0.02746177\n-0.9039460\n-0.9039460\n\n\n5\n432\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\n...\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n0.01375967\n-0.9562729\n-0.9562729\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\n...\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n0.38805730\n0.4731314\n0.4731314\n\n\n\n\n\n\n\n\n\n25.2.3 Scaling for train/test/validation/prediction\nIf you use scaling initial parameters should be remembered somewhere for future prediction data and reimplemented for new/test/validation/prediction dataset\nFor experiment lets split our dataset for train and test:\n\nlibrary(caret)\nset.seed(2021)\n \nindex = createDataPartition(churn_data$Churn, p = 0.70, list = FALSE)\ntrain = churn_data[index, ]\ntest = churn_data[-index, ]\n\nnrow(churn_data)\nnrow(train)\nnrow(test)\n\nLoading required package: lattice\n\n\n\n5976\n\n\n4184\n\n\n1792\n\n\nLets rescale TotalCharges data for training set:\n\ntrain &lt;- train %&gt;% mutate(TotalChargesScaled = scale(TotalCharges))\nhead(train) # you can see that TotalChangesStand and TotalChangesScaled are different becouse of changed mean and standart deviation of data\n\n\nA data.frame: 6 × 25\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n⋯\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\nTotalChargesNorm\nTotalChargesStand\nTotalChargesScaled\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl[,1]&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n⋯\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n0.19799792\n-0.2477481\n-0.2500210\n\n\n5\n432\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\n⋯\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n0.01162012\n-0.9579766\n-0.9563465\n\n\n9\n1480\n8898-KASCD\nMale\n0\nNo\nNo\n39\nNo\nNo phone service\nDSL\n⋯\nNo\nOne year\nNo\nMailed check\n35.55\n1309.15\nNo\n0.14889799\n-0.4348528\n-0.4360975\n\n\n10\n5137\n8016-NCFVO\nMale\n1\nNo\nNo\n55\nYes\nYes\nFiber optic\n⋯\nYes\nMonth-to-month\nYes\nElectronic check\n116.5\n6382.55\nNo\n0.73433533\n1.7960690\n1.7825644\n\n\n11\n3169\n4578-PHJYZ\nMale\n0\nYes\nYes\n52\nYes\nNo\nDSL\n⋯\nNo\nOne year\nYes\nElectronic check\n68.75\n3482.85\nNo\n0.39972883\n0.5209864\n0.5144889\n\n\n12\n4653\n2091-MJTFX\nFemale\n0\nYes\nYes\n30\nNo\nNo phone service\nDSL\n⋯\nYes\nMonth-to-month\nNo\nCredit card (automatic)\n51.2\n1561.50\nYes\n0.17801754\n-0.3238872\n-0.3257417\n\n\n\n\n\nSo, for train, test and prediction data we should use the same scaling base, in this case mean and standart deviation.\nCorrect data scaling code should be like this:\n\n# fix mean and sd\nmeanTotalCharges = mean(train$TotalCharges, na.rm = T)\nsdTotalCharges = sd(train$TotalCharges, na.rm = T)\n\ntrain &lt;- train %&gt;% mutate(TotalChargesScaled = scale(TotalCharges, center = meanTotalCharges, scale = sdTotalCharges)) # default\ntest &lt;- test %&gt;% mutate(TotalChargesScaled = scale(TotalCharges, center = meanTotalCharges, scale = sdTotalCharges)) # use parameters of train set\n\nsd(train$TotalChargesScaled)\nsd(test$TotalChargesScaled)\n\n#check the same value TotalCharges == 0 in train and set\nhead(train %&gt;% filter(TotalCharges == 0))\nhead(test %&gt;% filter(TotalCharges == 0))\n\n1\n\n\n0.981778490083862\n\n\nERROR while rich displaying an object: Error in apply(apply(col, 2L, format), 1L, paste, collapse = \", \"): dim(X) must have a positive length\n\nTraceback:\n1. tryCatch(withCallingHandlers({\n .     if (!mime %in% names(repr::mime2repr)) \n .         stop(\"No repr_* for mimetype \", mime, \" in repr::mime2repr\")\n .     rpr &lt;- repr::mime2repr[[mime]](obj)\n .     if (is.null(rpr)) \n .         return(NULL)\n .     prepare_content(is.raw(rpr), rpr)\n . }, error = error_handler), error = outer_handler)\n2. tryCatchList(expr, classes, parentenv, handlers)\n3. tryCatchOne(expr, names, parentenv, handlers[[1L]])\n4. doTryCatch(return(expr), name, parentenv, handler)\n5. withCallingHandlers({\n .     if (!mime %in% names(repr::mime2repr)) \n .         stop(\"No repr_* for mimetype \", mime, \" in repr::mime2repr\")\n .     rpr &lt;- repr::mime2repr[[mime]](obj)\n .     if (is.null(rpr)) \n .         return(NULL)\n .     prepare_content(is.raw(rpr), rpr)\n . }, error = error_handler)\n6. repr::mime2repr[[mime]](obj)\n7. repr_text.data.frame(obj)\n8. ellip_limit_arr(obj, rows, cols)\n9. arr_parts_format(parts)\n10. structure(lapply(parts, arr_part_format), omit = attr(parts, \n  .     \"omit\"))\n11. lapply(parts, arr_part_format)\n12. FUN(X[[i]], ...)\n13. vapply(part, function(col) {\n  .     if (is.matrix(col)) \n  .         apply(apply(col, 2L, format), 1L, paste, collapse = \", \")\n  .     else format(col)\n  . }, character(nrow(part)))\n14. FUN(X[[i]], ...)\n15. apply(apply(col, 2L, format), 1L, paste, collapse = \", \")\n16. stop(\"dim(X) must have a positive length\")\nERROR while rich displaying an object: Error in apply(apply(col, 2L, format), 1L, paste, collapse = \", \"): dim(X) must have a positive length\n\nTraceback:\n1. tryCatch(withCallingHandlers({\n .     if (!mime %in% names(repr::mime2repr)) \n .         stop(\"No repr_* for mimetype \", mime, \" in repr::mime2repr\")\n .     rpr &lt;- repr::mime2repr[[mime]](obj)\n .     if (is.null(rpr)) \n .         return(NULL)\n .     prepare_content(is.raw(rpr), rpr)\n . }, error = error_handler), error = outer_handler)\n2. tryCatchList(expr, classes, parentenv, handlers)\n3. tryCatchOne(expr, names, parentenv, handlers[[1L]])\n4. doTryCatch(return(expr), name, parentenv, handler)\n5. withCallingHandlers({\n .     if (!mime %in% names(repr::mime2repr)) \n .         stop(\"No repr_* for mimetype \", mime, \" in repr::mime2repr\")\n .     rpr &lt;- repr::mime2repr[[mime]](obj)\n .     if (is.null(rpr)) \n .         return(NULL)\n .     prepare_content(is.raw(rpr), rpr)\n . }, error = error_handler)\n6. repr::mime2repr[[mime]](obj)\n7. repr_text.data.frame(obj)\n8. ellip_limit_arr(obj, rows, cols)\n9. arr_parts_format(parts)\n10. structure(lapply(parts, arr_part_format), omit = attr(parts, \n  .     \"omit\"))\n11. lapply(parts, arr_part_format)\n12. FUN(X[[i]], ...)\n13. vapply(part, function(col) {\n  .     if (is.matrix(col)) \n  .         apply(apply(col, 2L, format), 1L, paste, collapse = \", \")\n  .     else format(col)\n  . }, character(nrow(part)))\n14. FUN(X[[i]], ...)\n15. apply(apply(col, 2L, format), 1L, paste, collapse = \", \")\n16. stop(\"dim(X) must have a positive length\")\n\n\n\n#compare it with all dataset TotalCharges == 0\n\n\nfilter(churn_data, TotalCharges == 0)\n# for now TotalChargesScaled in train/test th same, but in churn data its different, because of diffrent scaling bases\n\n\nA data.frame: 10 × 25\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n...\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\nTotalChargesNorm\nTotalChargesStand\nTotalChargesScaled\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n6754\n2775-SEFEE\nMale\n0\nNo\nYes\n0\nYes\nYes\nDSL\n...\nNo\nTwo year\nYes\nBank transfer (automatic)\n61.9\n0\nNo\n0\n-1.00882\n-1.00882\n\n\n1340\n1371-DWPAZ\nFemale\n0\nYes\nYes\n0\nNo\nNo phone service\nDSL\n...\nNo\nTwo year\nNo\nCredit card (automatic)\n56.05\n0\nNo\n0\n-1.00882\n-1.00882\n\n\n3826\n3213-VVOLG\nMale\n0\nYes\nYes\n0\nYes\nYes\nNo\n...\nNo internet service\nTwo year\nNo\nMailed check\n25.35\n0\nNo\n0\n-1.00882\n-1.00882\n\n\n5218\n2923-ARZLG\nMale\n0\nYes\nYes\n0\nYes\nNo\nNo\n...\nNo internet service\nOne year\nYes\nMailed check\n19.7\n0\nNo\n0\n-1.00882\n-1.00882\n\n\n3331\n7644-OMVMY\nMale\n0\nYes\nYes\n0\nYes\nNo\nNo\n...\nNo internet service\nTwo year\nNo\nMailed check\n19.85\n0\nNo\n0\n-1.00882\n-1.00882\n\n\n936\n5709-LVOEQ\nFemale\n0\nYes\nYes\n0\nYes\nNo\nDSL\n...\nYes\nTwo year\nNo\nMailed check\n80.85\n0\nNo\n0\n-1.00882\n-1.00882\n\n\n753\n3115-CZMZD\nMale\n0\nNo\nYes\n0\nYes\nNo\nNo\n...\nNo internet service\nTwo year\nNo\nMailed check\n20.25\n0\nNo\n0\n-1.00882\n-1.00882\n\n\n4380\n2520-SGTTA\nFemale\n0\nYes\nYes\n0\nYes\nNo\nNo\n...\nNo internet service\nTwo year\nNo\nMailed check\n20.0\n0\nNo\n0\n-1.00882\n-1.00882\n\n\n488\n4472-LVYGI\nFemale\n0\nYes\nYes\n0\nNo\nNo phone service\nDSL\n...\nNo\nTwo year\nYes\nBank transfer (automatic)\n52.55\n0\nNo\n0\n-1.00882\n-1.00882\n\n\n1082\n4367-NUYAO\nMale\n0\nYes\nYes\n0\nYes\nYes\nNo\n...\nNo internet service\nTwo year\nNo\nMailed check\n25.75\n0\nNo\n0\n-1.00882\n-1.00882",
    "crumbs": [
      "ТЕМА 5. КОНСТРУЮВАННЯ ОЗНАК",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Feature engineering in R</span>"
    ]
  },
  {
    "objectID": "etl-feature-engineering.html#feature-transformation",
    "href": "etl-feature-engineering.html#feature-transformation",
    "title": "25  Feature engineering in R",
    "section": "25.3 Feature Transformation",
    "text": "25.3 Feature Transformation\nFeature transformation involves manipulating a predictor variable in some way so as to improve its performance in the predictive model. A variety of considerations come into play when transforming models, including:\n\nThe flexibility of machine learning and statistical models in dealing with different types of data. For example, some techniques require that the input data be in numeric format, whereas others can deal with other formats, such as categorical, text, or dates.\nEase of interpretation. A predictive model where all the predictors are on the same scale (e.g., have a mean of 0 and a standard deviation of 1), can make interpretation easier.\nPredictive accuracy. Some transformations of variables can improve the accuracy of prediction (e.g., rather than including a numeric variable as a predictor, instead include both it and a second variable that is its square).\nTheory. For example, economic theory dictates that in many situations the natural logarithm of data representing prices and quantities should be used.\nComputational error. Many algorithms are written in such a way that “large” numbers cause them to give the wrong result, where “large” may not be so large (e.g., more than 10 or less than -10).\n\n\n25.3.1 Scaling based on calculations\nSometimes for changing data distribution before using in modeling or change correlation between input and output variables scientist changes data type with standart mathematical functions. Lets try transform TotalCharges with logarithm, sqrt and power up 2.\n\nlibrary(gridExtra)\nchurn_data_tmp &lt;- churn_data %&gt;%\n        mutate(TotalChargesLog = log(TotalCharges),\n              TotalChargesSqrt = sqrt(TotalCharges),\n              TotalChargesPow2 = TotalCharges^2)\n\nplot1 &lt;- ggplot(churn_data_tmp, aes(x=TotalChargesLog)) + geom_histogram(bins = 15)\nplot2 &lt;- ggplot(churn_data_tmp, aes(x=TotalChargesSqrt)) + geom_histogram(bins = 15)\nplot3 &lt;- ggplot(churn_data_tmp, aes(x=TotalChargesPow2)) + geom_histogram(bins = 15)\ngrid.arrange(plot1, plot2, plot3, ncol=3) \n\n\n\n\n\n\n\n\nLets try\n\n#install.packages(\"moments\")\n\npackage 'moments' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    D:\\Temp\\Rtmpum9OKD\\downloaded_packages\n\n\n\nlibrary(moments)\n\nskewness(churn_data_tmp$TotalCharges)\nskewness(churn_data_tmp$TotalChargesLog)\nskewness(churn_data_tmp$TotalChargesSqrt)\nskewness(churn_data_tmp$TotalChargesPow2)\n\n0.949325175892764\n\n\n-0.75144252264496\n\n\n0.302580803403444\n\n\n1.80376407354267\n\n\nConclusion: different scaling gives different data distribution and may improve model perfomance if you have found the correct form of dependence of input/ouput parameters.",
    "crumbs": [
      "ТЕМА 5. КОНСТРУЮВАННЯ ОЗНАК",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Feature engineering in R</span>"
    ]
  },
  {
    "objectID": "etl-feature-engineering.html#feature-construction",
    "href": "etl-feature-engineering.html#feature-construction",
    "title": "25  Feature engineering in R",
    "section": "25.4 Feature Construction",
    "text": "25.4 Feature Construction\nThe feature Construction method helps in creating new features in the data thereby increasing model accuracy and overall predictions. It is of two types:\n\nBinning: Bins are created for continuous variables.\nEncoding: Numerical variables or features are formed from categorical variables.\n\nДодано мною, бо не знайшов ніде такого прийому) Всі одразу моделюють)) - [x] Evaluation - construction new features on raw data from datasource.\n\n25.4.1 Binning\nBinning is done to create bins for continuous variables where they are converted to categorical variables.\nBinning is the term used in scoring modeling for what is also known in Machine Learning as Discretization, the process of transforming a continuous characteristic into a finite number of intervals (the bins), which allows for a better understanding of its distribution and its relationship with a binary variable. The bins generated by the this process will eventually become the attributes of a predictive characteristic, the key component of a Scorecard.\nWhy Binning?\n\nIt allows missing data and other special calculations (e.g. divided by zero) to be included in the model.\nIt controls or mitigates the impact of outliers over the model.\nIt solves the issue of having different scales among the characteristics, making the weights of the coefficients in the final model comparable.\n\nThere are two types of binning: Unsupervised and Supervised.\nUnsupervised Binning involves Automatic and Manual binning. In Automatic Binning, bins are created without human interference and are created automatically. In Manual Binning, bins are created with human interference and we specify where the bins to be created.\nSupervised Binning involves creating bins for the continuous variable while taking the target variable into the consideration also. Supervised Discretization or Binning divides a continuous feature into groups (bins) mapped to a target variable. The central idea is to find those cutpoints that maximize the difference between the groups.\nIn the past, analysts used to iteratively move from Fine Binning to Coarse Binning, a very time consuming process of finding manually and visually the right cutpoints (if ever). Nowadays with algorithms like ChiMerge or Recursive Partitioning, two out of several techniques available, analysts can quickly find the optimal cutpoints in seconds and evaluate the relationship with the target variable using metrics such as Weight of Evidence and Information Value.\nThere are many packages for creating new variables: smbinning, scorecard, rbin, InformationValue and other.\n\n25.4.1.1 WOE binning: theory\nWeight of evidence (WOE)\nThis is basically a technique that can be applied if we have a binary response variable and any kind of predictor variable. First we perform a reasonable binning on the response variable and then decide which form of the binary response we count as positive and which as negative. Then we calculate the percentage positive cases in each bin of the total of all positive cases. For example 20 positive cases in bin A out of 100 total positive cases in all bins equals 20 %. Next we calculate the percentage of negative cases in each bin of the total of all negative cases, for example 5 negative cases in bin A out of a total of 50 negative cases in all bins equals 10%. Then we calculate the WOE by dividing the bin percentages of positive cases by the bin percentage of negative cases, and take the logarithm. For the described example log(20/10).\nRule of thump: If WOE values are negative, negative cases supersede the positive cases. If WOE values are positive, positive cases supersede the negative cases.\nThis serves the following purposes:\n\nWe eliminate any none-linear relationships\nWe automatically scale all variables too some extend\nWe convert categorical variables to contineous variables\nMissing Data can be handled as just another factor value\nWe can built a stand alone score card, that could be manually applied by a person with a pen and a printout of all relevant variables.\n\nIt has the following disadvantages:\n\nWe always loose information via binning\nScore development along single variables is not contineous and occurs in steps\nBinning requires manual revision\nCalculating Variable importance is not as straight forward as with classical logistic regression with regularly scaled variables\n\nInformation Value (IV)\nBy doing another sequence of calculations similar to the WOE calculation we can calculate the IV. Classically this serves as variable ranking method and allows us to perform feature selection, which is less compuationally demanding as other methods.\n\n\n\nInformation Value\nPredictive Power\n\n\n\n\n&lt; 0.02\nuseless for prediction\n\n\n0.02 - 0.1\nweak predictor\n\n\n0.1 - 0.3\nmedium predictor\n\n\n0.3 - 0.5\nstrong predictor\n\n\n&gt; 0.5\nsuspicious too good to be true\n\n\n\nAfter calculating WOE value it replaces the original values in dataset.\n\n\n25.4.1.2 scorecard package and woebin()-function\nwoebin generates optimal binning for numerical, factor and categorical variables using methods including tree-like segmentation or chi-square merge. woebin can also customizing breakpoints if the breaks_list was provided. The default woe is defined as ln(Pos_i/Neg_i). If you prefer ln(Neg_i/Pos_i), please set the argument positive as negative value, such as ‘0’ or ‘good’. If there is a zero frequency class when calculating woe, the zero will replaced by 0.99 to make the woe calculable.\n\n# lets try to bin InternetService, TotalCharges from churn_data_tmp\nchurn_data_tmp &lt;- churn_data %&gt;%\n        mutate(Churn = ifelse(Churn == \"Yes\", 1, 0))\n\nbin_data &lt;- churn_data_tmp %&gt;% select(customerID, InternetService, TotalCharges, Churn)\nhead(bin_data)\n\n#churn_data_tmp%&gt;%select(Churn) %&gt;% distinct()\n\n\nA data.frame: 6 × 4\n\n\n\ncustomerID\nInternetService\nTotalCharges\nChurn\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n7010-BRBUU\nNo\n1734.65\n0\n\n\n2\n9688-YGXVR\nFiber optic\n3973.20\n0\n\n\n3\n9286-DOJGF\nFiber optic\n2869.85\n1\n\n\n4\n6994-KERXL\nDSL\n238.50\n0\n\n\n5\n2181-UAESM\nDSL\n119.50\n0\n\n\n6\n4312-GVYNH\nDSL\n3370.20\n0\n\n\n\n\n\n\n# install.packages(\"scorecard\")\nlibrary(scorecard)\n\nbins = woebin(bin_data, # dataset\n              y = 'Churn', # target variable\n              x = c(\"InternetService\", \"TotalCharges\")) # variables for binning\n\n[INFO] creating woe binning ... \n\n\n\n# lets view our bins\nbins\n\n\n    $InternetService\n        \n\nA data.table: 3 × 12\n\n\nvariable\nbin\ncount\ncount_distr\nneg\npos\nposprob\nwoe\nbin_iv\ntotal_iv\nbreaks\nis_special_values\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\nInternetService\nNo\n1291\n0.2156699\n1192\n99\n0.07668474\n-1.4687362\n0.30636195\n0.5897126\nNo\nFALSE\n\n\nInternetService\nDSL\n2068\n0.3454728\n1671\n397\n0.19197292\n-0.4177094\n0.05417755\n0.5897126\nDSL\nFALSE\n\n\nInternetService\nFiber optic\n2627\n0.4388573\n1536\n1091\n0.41530263\n0.6774449\n0.22917306\n0.5897126\nFiber optic\nFALSE\n\n\n\n\n\n    $TotalCharges\n        \n\nA data.table: 4 × 12\n\n\nvariable\nbin\ncount\ncount_distr\nneg\npos\nposprob\nwoe\nbin_iv\ntotal_iv\nbreaks\nis_special_values\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\nTotalCharges\n[-Inf,200)\n1004\n0.16772469\n517\n487\n0.4850598\n0.9597530\n0.181721173\n0.3097399\n200\nFALSE\n\n\nTotalCharges\n[200,400)\n488\n0.08152355\n327\n161\n0.3299180\n0.3109760\n0.008431865\n0.3097399\n400\nFALSE\n\n\nTotalCharges\n[400,3800)\n2981\n0.49799532\n2266\n715\n0.2398524\n-0.1339571\n0.008651146\n0.3097399\n3800\nFALSE\n\n\nTotalCharges\n[3800, Inf)\n1513\n0.25275643\n1289\n224\n0.1480502\n-0.7304442\n0.110935711\n0.3097399\nInf\nFALSE\n\n\n\n\n\n\n\n\n\nchurn_data_tmp %&gt;% filter(TotalCharges &gt; 3800) %&gt;% select(Churn) %&gt;% group_by(Churn) %&gt;% summarize(n())\n\n\nA tibble: 2 × 2\n\n\nChurn\nn()\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n0\n1289\n\n\n1\n224\n\n\n\n\n\n\nbins$TotalCharges %&gt;% knitr::kable() # better view for RStudio, need knitr to be installed\n\n\n\n|variable     |bin         | count| count_distr|  neg| pos|   posprob|        woe|    bin_iv|  total_iv|breaks |is_special_values |\n|:------------|:-----------|-----:|-----------:|----:|---:|---------:|----------:|---------:|---------:|:------|:-----------------|\n|TotalCharges |[-Inf,200)  |  1004|   0.1677247|  517| 487| 0.4850598|  0.9597530| 0.1817212| 0.3097399|200    |FALSE             |\n|TotalCharges |[200,400)   |   488|   0.0815236|  327| 161| 0.3299180|  0.3109760| 0.0084319| 0.3097399|400    |FALSE             |\n|TotalCharges |[400,3800)  |  2981|   0.4979953| 2266| 715| 0.2398524| -0.1339571| 0.0086511| 0.3097399|3800   |FALSE             |\n|TotalCharges |[3800, Inf) |  1513|   0.2527564| 1289| 224| 0.1480502| -0.7304442| 0.1109357| 0.3097399|Inf    |FALSE             |\n\n\n\n# preview the plot\nwoebin_plot(bins$TotalCharges)\n\n$TotalCharges\n\n\n\n\n\n\n\n\n\nIf TotalCharges less than 200 its most risky group of customers, 48.5% of them are potential churn.\n\nbins$InternetService %&gt;% knitr::kable() \nwoebin_plot(bins$InternetService)\n\n\n\n|variable        |bin         | count| count_distr|  neg|  pos|   posprob|        woe|    bin_iv|  total_iv|breaks      |is_special_values |\n|:---------------|:-----------|-----:|-----------:|----:|----:|---------:|----------:|---------:|---------:|:-----------|:-----------------|\n|InternetService |No          |  1291|   0.2156699| 1192|   99| 0.0766847| -1.4687362| 0.3063620| 0.5897126|No          |FALSE             |\n|InternetService |DSL         |  2068|   0.3454728| 1671|  397| 0.1919729| -0.4177094| 0.0541776| 0.5897126|DSL         |FALSE             |\n|InternetService |Fiber optic |  2627|   0.4388573| 1536| 1091| 0.4153026|  0.6774449| 0.2291731| 0.5897126|Fiber optic |FALSE             |\n\n\n$InternetService\n\n\n\n\n\n\n\n\n\nFiber optic InternetService customers are the most risky. 41.5% of them are potentiona churn.\nThe next stage is applying bins to th the variables:\n\nbin_data_woe = woebin_ply(bin_data, bins) \nhead(bin_data)\nhead(bin_data_woe) # compare how it changed\n\n[INFO] converting into woe values ... \n\n\n\nA data.frame: 6 × 4\n\n\n\ncustomerID\nInternetService\nTotalCharges\nChurn\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n7010-BRBUU\nNo\n1734.65\n0\n\n\n2\n9688-YGXVR\nFiber optic\n3973.20\n0\n\n\n3\n9286-DOJGF\nFiber optic\n2869.85\n1\n\n\n4\n6994-KERXL\nDSL\n238.50\n0\n\n\n5\n2181-UAESM\nDSL\n119.50\n0\n\n\n6\n4312-GVYNH\nDSL\n3370.20\n0\n\n\n\n\n\n\nA data.table: 6 × 4\n\n\ncustomerID\nChurn\nInternetService_woe\nTotalCharges_woe\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n7010-BRBUU\n0\n-1.4687362\n-0.1339571\n\n\n9688-YGXVR\n0\n0.6774449\n-0.7304442\n\n\n9286-DOJGF\n1\n0.6774449\n-0.1339571\n\n\n6994-KERXL\n0\n-0.4177094\n0.3109760\n\n\n2181-UAESM\n0\n-0.4177094\n0.9597530\n\n\n4312-GVYNH\n0\n-0.4177094\n-0.1339571\n\n\n\n\n\n\nbins$InternetService #No is replaced by WOE -1.4687362 (line 1)\n\n\nA data.table: 3 × 12\n\n\nvariable\nbin\ncount\ncount_distr\nneg\npos\nposprob\nwoe\nbin_iv\ntotal_iv\nbreaks\nis_special_values\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\nInternetService\nNo\n1291\n0.2156699\n1192\n99\n0.07668474\n-1.4687362\n0.30636195\n0.5897126\nNo\nFALSE\n\n\nInternetService\nDSL\n2068\n0.3454728\n1671\n397\n0.19197292\n-0.4177094\n0.05417755\n0.5897126\nDSL\nFALSE\n\n\nInternetService\nFiber optic\n2627\n0.4388573\n1536\n1091\n0.41530263\n0.6774449\n0.22917306\n0.5897126\nFiber optic\nFALSE\n\n\n\n\n\nSo, both numerical and categorical variables are binned by woebin function.\nIn real-life project you should dou binning with the next steps:\n\nClean data\nSplit data into train/test (+prediction)\nCreate bins on train set (and save them if your model not one-time used)\nApply bins to all sets you have\n\nLest see how to make it with our dataset churn_data.\n\nlibrary(caret)\nlibrary(gmodels)\nset.seed(2021) # to fix split options\n\nchurn_data &lt;- read.csv(\"../../data/telecom_users.csv\") # read data\nchurn_data &lt;- churn_data %&gt;% \n    select(customerID, gender, PaymentMethod, TotalCharges, Churn) %&gt;% # select some columns for test + target\n    mutate(Churn = ifelse(Churn == \"Yes\", 1, 0)) # replace Churn Yes/No with 1/0 - Event/NonEvent\nhead(churn_data)\n\n\nA data.frame: 6 × 5\n\n\n\ncustomerID\ngender\nPaymentMethod\nTotalCharges\nChurn\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n7010-BRBUU\nMale\nCredit card (automatic)\n1734.65\n0\n\n\n2\n9688-YGXVR\nFemale\nCredit card (automatic)\n3973.20\n0\n\n\n3\n9286-DOJGF\nFemale\nBank transfer (automatic)\n2869.85\n1\n\n\n4\n6994-KERXL\nMale\nElectronic check\n238.50\n0\n\n\n5\n2181-UAESM\nMale\nElectronic check\n119.50\n0\n\n\n6\n4312-GVYNH\nFemale\nBank transfer (automatic)\n3370.20\n0\n\n\n\n\n\n\nindex = createDataPartition(churn_data$Churn, p = 0.60, list = FALSE) # select randomly indexes of the rows for train\ntrain = churn_data[index, ]\ntest = churn_data[-index, ]\n\n\ndata_bins = woebin(train, # dataset\n                   y = \"Churn\", # target\n                   var_skip = \"customerID\" # skip ID\n                  # x = c(\"gender\", \"PaymentMethod\", \"TotalCharges\") # select some varables\n                   #var_skip = \"customerID\" # target variable - not working in jupyter\n             )\n\n[INFO] creating woe binning ... \n\n\n\ndata_bins\n\n\n    $gender\n        \n\nA data.table: 2 × 12\n\n\nvariable\nbin\ncount\ncount_distr\nneg\npos\nposprob\nwoe\nbin_iv\ntotal_iv\nbreaks\nis_special_values\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\ngender\nFemale\n1770\n0.4927617\n1314\n456\n0.2576271\n-0.02546718\n0.0003176558\n0.0006226087\nFemale\nFALSE\n\n\ngender\nMale\n1822\n0.5072383\n1335\n487\n0.2672887\n0.02444876\n0.0003049529\n0.0006226087\nMale\nFALSE\n\n\n\n\n\n    $PaymentMethod\n        \n\nA data.table: 3 × 12\n\n\nvariable\nbin\ncount\ncount_distr\nneg\npos\nposprob\nwoe\nbin_iv\ntotal_iv\nbreaks\nis_special_values\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\nPaymentMethod\nCredit card (automatic)%,%Bank transfer (automatic)\n1562\n0.4348552\n1312\n250\n0.1600512\n-0.6249758\n0.14385062\n0.4324699\nCredit card (automatic)%,%Bank transfer (automatic)\nFALSE\n\n\nPaymentMethod\nMailed check\n822\n0.2288419\n665\n157\n0.1909976\n-0.4106700\n0.03472141\n0.4324699\nMailed check\nFALSE\n\n\nPaymentMethod\nElectronic check\n1208\n0.3363029\n672\n536\n0.4437086\n0.8067470\n0.25389789\n0.4324699\nElectronic check\nFALSE\n\n\n\n\n\n    $TotalCharges\n        \n\nA data.table: 5 × 12\n\n\nvariable\nbin\ncount\ncount_distr\nneg\npos\nposprob\nwoe\nbin_iv\ntotal_iv\nbreaks\nis_special_values\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\nTotalCharges\nmissing\n6\n0.001670379\n6\n0\n0.00000000\n-0.7699879\n0.0009365099\n0.3222676\nmissing\nTRUE\n\n\nTotalCharges\n[-Inf,400)\n893\n0.248608018\n505\n388\n0.43449048\n0.7682688\n0.1693136398\n0.3222676\n400\nFALSE\n\n\nTotalCharges\n[400,3200)\n1601\n0.445712695\n1207\n394\n0.24609619\n-0.0877204\n0.0033567382\n0.3222676\n3200\nFALSE\n\n\nTotalCharges\n[3200,6600)\n850\n0.236636971\n711\n139\n0.16352941\n-0.6003766\n0.0727391340\n0.3222676\n6600\nFALSE\n\n\nTotalCharges\n[6600, Inf)\n242\n0.067371938\n220\n22\n0.09090909\n-1.2707632\n0.0759215884\n0.3222676\nInf\nFALSE\n\n\n\n\n\n\n\n\n\ntrain_woe = woebin_ply(train, data_bins) \nhead(train_woe)\ntest_woe = woebin_ply(test, data_bins) \nhead(test_woe)\n\n[INFO] converting into woe values ... \n\n\n\nA data.table: 6 × 5\n\n\ncustomerID\nChurn\ngender_woe\nPaymentMethod_woe\nTotalCharges_woe\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n7010-BRBUU\n0\n0.02444876\n-0.6249758\n-0.0877204\n\n\n4367-NHWMM\n0\n-0.02546718\n-0.4106700\n0.7682688\n\n\n8016-NCFVO\n0\n0.02444876\n0.8067470\n-0.6003766\n\n\n4578-PHJYZ\n0\n0.02444876\n0.8067470\n-0.6003766\n\n\n2091-MJTFX\n1\n-0.02546718\n-0.6249758\n-0.0877204\n\n\n2277-DJJDL\n0\n0.02444876\n0.8067470\n-0.6003766\n\n\n\n\n\n[INFO] converting into woe values ... \n\n\n\nA data.table: 6 × 5\n\n\ncustomerID\nChurn\ngender_woe\nPaymentMethod_woe\nTotalCharges_woe\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n9688-YGXVR\n0\n-0.02546718\n-0.6249758\n-0.6003766\n\n\n9286-DOJGF\n1\n-0.02546718\n-0.6249758\n-0.0877204\n\n\n6994-KERXL\n0\n0.02444876\n0.8067470\n0.7682688\n\n\n2181-UAESM\n0\n0.02444876\n0.8067470\n0.7682688\n\n\n4312-GVYNH\n0\n-0.02546718\n-0.6249758\n-0.6003766\n\n\n2495-KZNFB\n0\n-0.02546718\n0.8067470\n-0.0877204\n\n\n\n\n\nFor now our data is ready for modeling.\n\n\n\n25.4.1.3 Encoding\nEncoding is the process in which numerical variables or features are created from categorical variables. It is a widely used method in the industry and in every model building process. It is of two types: Label Encoding and One-hot Encoding.\nLabel Encoding involves assigning each label a unique integer or value based on alphabetical ordering. It is the most popular and widely used encoding.\nOne-hot Encoding involves creating additional features or variables on the basis of unique values in categorical variables i.e. every unique value in the category will be added as a new feature.\n\n25.4.1.3.1 Label encoding\n\n# Lets use client churn dataset from telco: https://www.kaggle.com/blastchar/telco-customer-churn\nchurn_data &lt;- read.csv(\"data/telecom_users.csv\")\nhead(churn_data)\n\n\nA data.frame: 6 × 22\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n⋯\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n⋯\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\n⋯\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\n⋯\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n\n\n4\n6739\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\n⋯\nNo\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n\n\n5\n432\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\n⋯\nYes\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\n⋯\nYes\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n\n\n\n\n\n\n\n\n25.4.1.3.2 Encoding with factors\nIts good way to encode labels as factors in R. This approach used for categorical data encoding. Easiest way is convert character values to factor and later convert factor to numeric:\n\nchurn_data &lt;- churn_data %&gt;% mutate(Partner = as.factor(Partner),\n                                   Dependents = as.factor(Dependents))\nhead(churn_data) # data types changed for selected fields\n\n\nA data.frame: 6 × 22\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n⋯\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n⋯\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\n⋯\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\n⋯\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n\n\n4\n6739\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\n⋯\nNo\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n\n\n5\n432\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\n⋯\nYes\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\n⋯\nYes\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n\n\n\n\n\n\nstr(churn_data$Partner) # check levels\n\n Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 2 1 1 1 1 ...\n\n\n\n# convert to integers\nchurn_data &lt;- churn_data %&gt;% mutate(Partner = as.integer(Partner),\n                                   Dependents = as.integer(Dependents))\nhead(churn_data) # data types changed for selected fields\n\n\nA data.frame: 6 × 22\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n⋯\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\n2\n2\n72\nYes\nYes\nNo\n⋯\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\n1\n1\n44\nYes\nNo\nFiber optic\n⋯\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\n2\n1\n38\nYes\nYes\nFiber optic\n⋯\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n\n\n4\n6739\n6994-KERXL\nMale\n0\n1\n1\n4\nYes\nNo\nDSL\n⋯\nNo\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n\n\n5\n432\n2181-UAESM\nMale\n0\n1\n1\n2\nYes\nNo\nDSL\n⋯\nYes\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\n2\n1\n70\nNo\nNo phone service\nDSL\n⋯\nYes\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n\n\n\n\n\nBut you should check factor to numbers converting to synchronize train/test/prediction correct numbering. In this case you should save somewhere levels of factor. Better way is to use encoders with fitting.\n\n\n\n25.4.1.3.3 Encoding with LabelEncoder from superml package\n\n# install.packages(\"superml\") #you olso need R6 package\n\nalso installing the dependency 'BH'\n\n\n\n\npackage 'BH' successfully unpacked and MD5 sums checked\npackage 'superml' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    D:\\Temp\\Rtmpum9OKD\\downloaded_packages\n\n\n\nlibrary(superml)\n\nlabel &lt;- LabelEncoder$new() # create new encoder\nprint(label$fit(churn_data$InternetService)) # fir the encoder\n\nLoading required package: R6\n\n\n\n[1] TRUE\n\n\n\n# encode data\nchurn_data$InternetService &lt;- label$fit_transform(churn_data$InternetService) \n# do not re-run it, because InternetService ineteger for now\nhead(churn_data)\n\n\nA data.frame: 6 × 22\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n...\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\n2\n2\n72\nYes\nYes\n0\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\n1\n1\n44\nYes\nNo\n1\n...\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\n2\n1\n38\nYes\nYes\n1\n...\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n\n\n4\n6739\n6994-KERXL\nMale\n0\n1\n1\n4\nYes\nNo\n2\n...\nNo\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n\n\n5\n432\n2181-UAESM\nMale\n0\n1\n1\n2\nYes\nNo\n2\n...\nYes\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\n2\n1\n70\nNo\nNo phone service\n2\n...\nYes\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n\n\n\n\n\nNow you can save your encoding configuration and use it for test/prediction sets later.\n\n\n\n\n25.4.1.4 One-hot encoding\n\n# Lets use client churn dataset from telco: https://www.kaggle.com/blastchar/telco-customer-churn\nchurn_data &lt;- read.csv(\"data/telecom_users.csv\")\nhead(churn_data)\n\n\nA data.frame: 6 × 22\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n⋯\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n⋯\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\n⋯\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\n⋯\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n\n\n4\n6739\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\n⋯\nNo\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n\n\n5\n432\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\n⋯\nYes\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\n⋯\nYes\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n\n\n\n\n\n\n# check possible Gender values\ngmodels::CrossTable(churn_data$gender)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  5986 \n\n \n          |    Female |      Male | \n          |-----------|-----------|\n          |      2936 |      3050 | \n          |     0.490 |     0.510 | \n          |-----------|-----------|\n\n\n\n \n\n\nLets create two additional variables Male and Female encoded by 1 / 0:\n\nCreate a Male column that encodes each row corresponding to male as 1 and set everything else to 0.\nCreate a Female column that encodes each row corresponding to female as 1 and set everything else to 0.\n\n\n# if gender is factor\n#churn_data &lt;- churn_data %&gt;% \n   #     mutate(gender = as.character(gender)) # convert Gender to character from Factor\n\nchurn_data &lt;- churn_data %&gt;% \n        mutate(\n            # create Male column\n            Male = ifelse(gender == \"Male\", 1, 0),\n            # create Female column\n            Female = ifelse(gender == \"Female\", 1, 0))\n\nchurn_data |&gt; select(gender, Male, Female) |&gt; head()\n\n\nA data.frame: 6 × 3\n\n\n\ngender\nMale\nFemale\n\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\nMale\n1\n0\n\n\n2\nFemale\n0\n1\n\n\n3\nFemale\n0\n1\n\n\n4\nMale\n1\n0\n\n\n5\nMale\n1\n0\n\n\n6\nFemale\n0\n1\n\n\n\n\n\nLets create a dummy variables for InternetService column. So, what data it has for now?\n\ngmodels::CrossTable(churn_data$InternetService)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  5986 \n\n \n            |         DSL | Fiber optic |          No | \n            |-------------|-------------|-------------|\n            |        2068 |        2627 |        1291 | \n            |       0.345 |       0.439 |       0.216 | \n            |-------------|-------------|-------------|\n\n\n\n \n\n\nThe function dummyVars() from caret package can be used to generate a complete (less than full rank parameterized) set of dummy variables from one or more factors. The function takes a formula and a data set and outputs an object that can be used to create the dummy variables using the predict method.\n\nlibrary(caret)\n\nchurn_data &lt;- read.csv(\"data/telecom_users.csv\")\n\ndummy &lt;- dummyVars(\" ~ InternetService\", data = churn_data)\nnew_df &lt;- data.frame(predict(dummy, newdata = churn_data)) # precit dummy variables and\nnew_df %&gt;% head()\n\n\nA data.frame: 6 × 3\n\n\n\nInternetServiceDSL\nInternetServiceFiber.optic\nInternetServiceNo\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0\n0\n1\n\n\n2\n0\n1\n0\n\n\n3\n0\n1\n0\n\n\n4\n1\n0\n0\n\n\n5\n1\n0\n0\n\n\n6\n1\n0\n0\n\n\n\n\n\nIf you want create dummy variables for all categoriacal columns just use \" ~ .\" formula.\n\ndummy &lt;- dummyVars(\" ~ .\", data = churn_data)\nnew_df &lt;- data.frame(predict(dummy, newdata = churn_data)) # precit dummy variables and\nnew_df %&gt;% head()\n\n\nA data.frame: 6 × 7560\n\n\n\nX\ncustomerID0002.ORFBO\ncustomerID0003.MKNFE\ncustomerID0004.TLHLJ\ncustomerID0011.IGKFF\ncustomerID0013.EXCHZ\ncustomerID0013.MHZWF\ncustomerID0013.SMEOE\ncustomerID0014.BMAQU\ncustomerID0015.UOCOJ\n⋯\nMonthlyCharges99.7\nMonthlyCharges99.75\nMonthlyCharges99.8\nMonthlyCharges99.85\nMonthlyCharges99.9\nMonthlyCharges99.95\nMonthlyChargesNULL\nTotalCharges\nChurnNo\nChurnYes\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n⋯\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1869\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n0\n0\n0\n0\n0\n0\n0\n1734.65\n1\n0\n\n\n2\n4528\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n0\n0\n0\n0\n0\n0\n0\n3973.20\n1\n0\n\n\n3\n6344\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n0\n0\n0\n0\n0\n0\n0\n2869.85\n0\n1\n\n\n4\n6739\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n0\n0\n0\n0\n0\n0\n0\n238.50\n1\n0\n\n\n5\n432\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n0\n0\n0\n0\n0\n0\n0\n119.50\n1\n0\n\n\n6\n2215\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n0\n0\n0\n0\n0\n0\n0\n3370.20\n1\n0\n\n\n\n\n\nYou can remove some variables, for example Churn, becouse its target variable\n\ndummy &lt;- dummyVars(\" ~. -Churn\", data = churn_data)\nnew_df &lt;- data.frame(predict(dummy, newdata = churn_data)) # precit dummy variables and\nnew_df %&gt;% head()\n\n\nA data.frame: 6 × 7558\n\n\n\nX\ncustomerID0002.ORFBO\ncustomerID0003.MKNFE\ncustomerID0004.TLHLJ\ncustomerID0011.IGKFF\ncustomerID0013.EXCHZ\ncustomerID0013.MHZWF\ncustomerID0013.SMEOE\ncustomerID0014.BMAQU\ncustomerID0015.UOCOJ\n...\nMonthlyCharges99.6\nMonthlyCharges99.65\nMonthlyCharges99.7\nMonthlyCharges99.75\nMonthlyCharges99.8\nMonthlyCharges99.85\nMonthlyCharges99.9\nMonthlyCharges99.95\nMonthlyChargesNULL\nTotalCharges\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1869\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1734.65\n\n\n2\n4528\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3973.20\n\n\n3\n6344\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2869.85\n\n\n4\n6739\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n238.50\n\n\n5\n432\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n119.50\n\n\n6\n2215\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3370.20\n\n\n\n\n\nIf you need some few do this with + operator:\n\ndummy &lt;- dummyVars(\" ~ InternetService + PhoneService\", data = churn_data)\nnew_df &lt;- data.frame(predict(dummy, newdata = churn_data)) # precit dummy variables and\nnew_df %&gt;% head()\n\n\nA data.frame: 6 × 5\n\n\n\nInternetServiceDSL\nInternetServiceFiber.optic\nInternetServiceNo\nPhoneServiceNo\nPhoneServiceYes\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0\n0\n1\n0\n1\n\n\n2\n0\n1\n0\n0\n1\n\n\n3\n0\n1\n0\n0\n1\n\n\n4\n1\n0\n0\n0\n1\n\n\n5\n1\n0\n0\n0\n1\n\n\n6\n1\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n25.4.2 Evaluate data on raw dataset\nPrevious data transformation approaches use prepared datasets. But sometimes you should create new variables from raw data than cannot be attached to you existing dataset and implemented as-is for modeling.\nLest check an example with customer transactions data.\n\nPreview data file in excel before read it to R!\n\n\n# read transaction data from dataset\n# explore file in Excel before reading to check sheet numbers and tables structure\nlibrary(xlsx)\ndemographics &lt;- read.xlsx(\"data/transactions.xlsx\", sheetIndex = 1)\nhead(demographics)\n\njava.home option: \n\nJAVA_HOME environment variable: C:\\Program Files\\Java\\jdk-19\\bin;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath\\java.exe;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath\\;\n\nWarning message in fun(libname, pkgname):\n\"Java home setting is INVALID, it will be ignored.\nPlease do NOT set it unless you want to override system settings.\"\n\n\n\nA data.frame: 4 × 4\n\n\n\nCustomerID\nGender\nEmail\nVisitsLastYear\n\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n787987456\nMale\nYes\n12\n\n\n2\n456415151\nMale\nNULL\n0\n\n\n3\n215454555\nFemale\nNo\n16\n\n\n4\n985121122\nFemale\nNo\n4\n\n\n\n\n\n\n# read transactions\ntransactions &lt;- read.xlsx(\"data/transactions.xlsx\", sheetIndex = 2)\nhead(transactions)\n\n\nA data.frame: 6 × 7\n\n\n\nCustomerID\nContractID\nDate\nTime\nContractorID\nUsdEquiv_sum\nType\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;date&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n215454555\n19065798\n2019-03-15\n114537\nNULL\n6293.7111\ncredet\n\n\n2\n215454555\n19065798\n2019-04-05\n102525\nNULL\n914.9459\ncredet\n\n\n3\n215454555\n19065798\n2019-05-11\n80833\nNULL\n-4655.9111\ndebet\n\n\n4\n215454555\n19065798\n2019-05-16\n74606\nNULL\n-13900.6889\ndebet\n\n\n5\n215454555\n19065798\n2019-05-30\n104506\nNULL\n2102.0570\ncredet\n\n\n6\n215454555\n19065798\n2019-08-23\n122656\nNULL\n-16244.2333\ndebet\n\n\n\n\n\nOur nex task is to construct new features based on transactions history:\n\nChurn - target parameter if client has more than 60 days absent transactions between 2021-04-30 and last transaction date.\nAverageContractSum3M_Credet - average contract sum by credet last 3 month before event Churn == 1/0\nAverageIncrease3M_Credet - average sum increase from month to month, for last 3 month, by credet\n\n\n# lets find a maximum transaction date for each client\nlibrary(tidyverse) # includes magrittr, dplyr\nmax_dates &lt;- transactions %&gt;%\n        group_by(CustomerID) %&gt;%\n        summarise(MaxDate = max(Date))\nmax_dates\n\n-- Attaching packages ------------------------------------------------------------------------------- tidyverse 1.3.2 --\nv tibble  3.1.8     v purrr   0.3.5\nv tidyr   1.2.1     v stringr 1.4.1\nv readr   2.1.3     v forcats 0.5.2\n-- Conflicts ---------------------------------------------------------------------------------- tidyverse_conflicts() --\nx gridExtra::combine() masks dplyr::combine()\nx dplyr::filter()      masks stats::filter()\nx dplyr::lag()         masks stats::lag()\nx purrr::lift()        masks caret::lift()\n\n\n\nA tibble: 4 × 2\n\n\nCustomerID\nMaxDate\n\n\n&lt;dbl&gt;\n&lt;date&gt;\n\n\n\n\n215454555\n2021-04-30\n\n\n456415151\n2021-01-22\n\n\n787987456\n2019-12-06\n\n\n985121122\n2020-09-09\n\n\n\n\n\n\nlibrary(lubridate) #for datetime manipulation\ncurrent_date &lt;- ymd(\"2021-04-30\")\nmax_dates &lt;- max_dates %&gt;%\n        mutate(DaysDiff = as.period(current_date - MaxDate) %&gt;% day()) # find period and convert it to days\nmax_dates\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n\n\n\nA tibble: 4 × 3\n\n\nCustomerID\nMaxDate\nDaysDiff\n\n\n&lt;dbl&gt;\n&lt;date&gt;\n&lt;dbl&gt;\n\n\n\n\n215454555\n2021-04-30\n0\n\n\n456415151\n2021-01-22\n98\n\n\n787987456\n2019-12-06\n511\n\n\n985121122\n2020-09-09\n233\n\n\n\n\n\n\n# lests calculate Churn feature\nmax_dates &lt;- max_dates %&gt;%\n        mutate(Churn = ifelse(DaysDiff &gt; 60, 1, 0))\nmax_dates\n\n\nA tibble: 4 × 4\n\n\nCustomerID\nMaxDate\nDaysDiff\nChurn\n\n\n&lt;dbl&gt;\n&lt;date&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n215454555\n2021-04-30\n0\n0\n\n\n456415151\n2021-01-22\n98\n1\n\n\n787987456\n2019-12-06\n511\n1\n\n\n985121122\n2020-09-09\n233\n1\n\n\n\n\n\nYou can finally merge code into one query:\n\nchurn_eval &lt;- transactions %&gt;%\n        group_by(CustomerID) %&gt;%\n        summarise(MaxDate = max(Date)) %&gt;%\n        mutate(DaysDiff = as.period(current_date - MaxDate) %&gt;% day(),\n               Churn = ifelse(DaysDiff &gt; 60, 1, 0)) %&gt;%\n        select(CustomerID, Churn) # Select only CustomerID and new feature / target\nchurn_eval # we will merge it with demographics later\n\n\nA tibble: 4 × 2\n\n\nCustomerID\nChurn\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n215454555\n0\n\n\n456415151\n1\n\n\n787987456\n1\n\n\n985121122\n1\n\n\n\n\n\n\n25.4.2.1 Task. AverageContractSum3M_Credet\nTASK: calculate average sum of contract for each customer by credet for last 3 month before Churn/NotChurn event\nFor calculating this data wee should do this steps:\n\nFind max transaction date\nFind transaction date 3 month ago (max transaction date - 3 month)\nFilter only records in range (-3 month, max_transaction)\nGroup data by contracts and find every contract sum\nGroup data by customers and find every customer average\n\n\navgContractSum3m &lt;- transactions %&gt;%\n        filter(Type == \"credet\") %&gt;% # only credet\n        group_by(CustomerID) %&gt;% \n        mutate(MaxDate = max(Date), # get max transaction date\n               Month3Date = MaxDate %m+% months(-3)) %&gt;% # get date 3 month before max transaction date\n       filter(Date &gt;= Month3Date) %&gt;% # only transaction more than 3 month age left\n       group_by(CustomerID, ContractID) %&gt;% # group by customer and contract\n       summarize(ContractSum = sum(UsdEquiv_sum), .groups = 'drop') %&gt;% # find sum by each contract\n       group_by(CustomerID) %&gt;% # group by customer\n       summarise(AverageContractSum3M_Credet = mean(ContractSum)) # find mean by each customer\n\navgContractSum3m\n\n\nA tibble: 4 × 2\n\n\nCustomerID\nAverageContractSum3M_Credet\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n215454555\n45541.3335\n\n\n456415151\n1271.5930\n\n\n787987456\n116.6667\n\n\n985121122\n4444.4444\n\n\n\n\n\n\n\n25.4.2.2 Task. Find last 3 average credet sum increase\n\nGet dates range for -1, -2, -3 monthes by credet:\n\n\nmonthByMonth &lt;- transactions %&gt;%\n        filter(Type == \"credet\") %&gt;%\n        select(CustomerID, Date, UsdEquiv_sum) %&gt;%\n        group_by(CustomerID) %&gt;%\n        mutate(MaxDate = max(Date),\n               Month1Before = MaxDate %m+% months(-1),\n               Month2Before = MaxDate %m+% months(-2),\n               Month3Before = MaxDate %m+% months(-3)) \nhead(monthByMonth)\n\n\nA grouped_df: 6 × 7\n\n\nCustomerID\nDate\nUsdEquiv_sum\nMaxDate\nMonth1Before\nMonth2Before\nMonth3Before\n\n\n&lt;dbl&gt;\n&lt;date&gt;\n&lt;dbl&gt;\n&lt;date&gt;\n&lt;date&gt;\n&lt;date&gt;\n&lt;date&gt;\n\n\n\n\n215454555\n2019-03-15\n6293.7111\n2021-04-30\n2021-03-30\n2021-02-28\n2021-01-30\n\n\n215454555\n2019-04-05\n914.9459\n2021-04-30\n2021-03-30\n2021-02-28\n2021-01-30\n\n\n215454555\n2019-05-30\n2102.0570\n2021-04-30\n2021-03-30\n2021-02-28\n2021-01-30\n\n\n215454555\n2019-10-09\n10854.7630\n2021-04-30\n2021-03-30\n2021-02-28\n2021-01-30\n\n\n215454555\n2019-11-27\n639.5389\n2021-04-30\n2021-03-30\n2021-02-28\n2021-01-30\n\n\n215454555\n2019-12-20\n2654.9459\n2021-04-30\n2021-03-30\n2021-02-28\n2021-01-30\n\n\n\n\n\n\nBig guery calculate sums for each month and find increases\n\n\ncredetInc &lt;- monthByMonth %&gt;%\n    filter(Date &gt;= Month1Before & Date &lt;= MaxDate) %&gt;% # data in range from Month1Before to MaxDate\n    group_by(CustomerID) %&gt;%\n    summarise(M1Sum = sum(UsdEquiv_sum)) %&gt;% # sum for month -1\n    left_join(monthByMonth %&gt;% # join with data about month -2\n              filter(Date &gt;= Month2Before & Date &lt;= Month1Before) %&gt;% # data in range from Month2Before to Month1Before\n              group_by(CustomerID) %&gt;%\n              summarise(M2Sum = sum(UsdEquiv_sum)), by = \"CustomerID\") %&gt;%\n            left_join(monthByMonth %&gt;% # join with data about month -3\n              filter(Date &gt;= Month3Before & Date &lt;= Month2Before) %&gt;% # data in range from Month3Before to Month2Before\n              group_by(CustomerID) %&gt;%\n              summarise(M3Sum = sum(UsdEquiv_sum)), by = \"CustomerID\") %&gt;%\n    rowwise %&gt;% # calculate separately for each row, not for columns\n    mutate(AverageIncrease3M_Credet = mean(c(M1Sum/M2Sum, M2Sum/M3Sum), na.rm = T)) %&gt;% # increase is an average of sum changinf from month to month\n    select(CustomerID, AverageIncrease3M_Credet)\n        \nhead(credetInc)\n\n\nA rowwise_df: 4 × 2\n\n\nCustomerID\nAverageIncrease3M_Credet\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n215454555\n8.3372521811\n\n\n456415151\nNaN\n\n\n787987456\n0.0003175611\n\n\n985121122\nNaN\n\n\n\n\n\nJoin all the tables\n\nfinal_set &lt;- demographics %&gt;% \n        left_join(avgContractSum3m , by = \"CustomerID\") %&gt;%\n        left_join(credetInc , by = \"CustomerID\") %&gt;%\n        left_join(churn_eval, by = \"CustomerID\")\nfinal_set\n\n\nA data.frame: 4 × 7\n\n\nCustomerID\nGender\nEmail\nVisitsLastYear\nAverageContractSum3M_Credet\nAverageIncrease3M_Credet\nChurn\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n787987456\nMale\nYes\n12\n116.6667\n0.0003175611\n1\n\n\n456415151\nMale\nNULL\n0\n1271.5930\nNaN\n1\n\n\n215454555\nFemale\nNo\n16\n45541.3335\n8.3372521811\n0\n\n\n985121122\nFemale\nNo\n4\n4444.4444\nNaN\n1\n\n\n\n\n\nOn the next stages this data can be transformed with scaling, encoding, binning.",
    "crumbs": [
      "ТЕМА 5. КОНСТРУЮВАННЯ ОЗНАК",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Feature engineering in R</span>"
    ]
  },
  {
    "objectID": "etl-feature-engineering.html#references",
    "href": "etl-feature-engineering.html#references",
    "title": "25  Feature engineering in R",
    "section": "25.5 References",
    "text": "25.5 References\n\nFeature Engineering in R Programming by (dhruv5819?)\nWhat is Feature Engineering? by Tim Bok\nFeature Scaling-Why it is required? by Rahul Saini\nFeature Scaling for Machine Learning: Understanding the Difference Between Normalization vs. Standardization by ANIRUDDHA BHANDARI\nR Package ‘smbinning’: Optimal Binning for Scoring Modeling by Herman Jopia\nGarcia, S. et al (2013) A Survey of Discretization Techniques: Taxonomy and Empirical Analysis in Supervised Learning. IEEE Transactions on Knowledge and Data Engineering, Vol. 25, No. 4, April 2013.\nAn Overview on the Landscape of R Packages for Credit Scoring by Gero Szepannek",
    "crumbs": [
      "ТЕМА 5. КОНСТРУЮВАННЯ ОЗНАК",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Feature engineering in R</span>"
    ]
  },
  {
    "objectID": "train-test-validation-samples.html",
    "href": "train-test-validation-samples.html",
    "title": "26  Data Split: Train, Test and Validation sets",
    "section": "",
    "text": "26.1 What’s Train, Validation, Test datasets\nBefore model fitting and some stages of features engeniering we shoudl split out dataset on 2 or 3 parts:\nThe model sees and learns from this data.\nThe validation set is used to evaluate a given model, but this is for frequent evaluation. We, as machine learning engineers, use this data to fine-tune the model hyperparameters. Hence the model occasionally sees this data, but never does it “Learn” from this. We use the validation set results, and update higher level hyperparameters. So the validation set affects a model, but only indirectly. The validation set is also known as the Dev set or the Development set. This makes sense since this dataset helps during the “development” stage of the model.\nThe Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained(using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner). Many a times the validation set is used as the test set, but it is not good practice. The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world.\nYou can also find papers with splitting only for train/test. In this case test means validation.",
    "crumbs": [
      "ТЕМА 6. ПОДІЛ ВИБІРКИ",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Split: Train, Test and Validation sets</span>"
    ]
  },
  {
    "objectID": "train-test-validation-samples.html#whats-train-validation-test-datasets",
    "href": "train-test-validation-samples.html#whats-train-validation-test-datasets",
    "title": "26  Data Split: Train, Test and Validation sets",
    "section": "",
    "text": "Training dataset: The sample of data used to fit the model.\n\n\n\nValidation dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n\n\n\nTesting dataset: Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.",
    "crumbs": [
      "ТЕМА 6. ПОДІЛ ВИБІРКИ",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Split: Train, Test and Validation sets</span>"
    ]
  },
  {
    "objectID": "train-test-validation-samples.html#splitting-data-in-r",
    "href": "train-test-validation-samples.html#splitting-data-in-r",
    "title": "26  Data Split: Train, Test and Validation sets",
    "section": "26.2 Splitting data in R",
    "text": "26.2 Splitting data in R\nLets describe some conditions before start studiyng splitting data functions in R:\n\nWe will use same seed for all splittings to control results reproduction, for example, let it be 2021.\nWe will use dataset for client churn prediction Telco Customer Churn: https://www.kaggle.com/blastchar/telco-customer-churn\n\nShort dataset description:\n\ncustomerID - Customer ID\ngender Whether the customer is a male or a female\nSeniorCitizen - Whether the customer is a senior citizen or not (1, 0)\nPartner - Whether the customer has a partner or not (Yes, No)\nDependents - Whether the customer has dependents or not (Yes, No)\ntenure - Number of months the customer has stayed with the company\nPhoneService - Whether the customer has a phone service or not (Yes, No)\nMultipleLines - Whether the customer has multiple lines or not (Yes, No, No phone service)\nInternetService - Customer’s internet service provider (DSL, Fiber optic, No)\nOnlineSecurity - Whether the customer has online security or not (Yes, No, No internet service)\nOnlineBackup - Whether the customer has online backup or not (Yes, No, No internet service)\nDeviceProtection - Whether the customer has device protection or not (Yes, No, No internet service)\nTechSupport - Whether the customer has tech support or not (Yes, No, No internet service)\nStreamingTV - Whether the customer has streaming TV or not (Yes, No, No internet service)\nStreamingMovies - Whether the customer has streaming movies or not (Yes, No, No internet service)\nContract - The contract term of the customer (Month-to-month, One year, Two year)\nPaperlessBilling - Whether the customer has paperless billing or not (Yes, No)\nPaymentMethod - The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\nMonthlyCharges - The amount charged to the customer monthly\nTotalCharges - The total amount charged to the customer\nChurn - Whether the customer churned or not (Yes or No)\n\n\n# read data\ntelecom_users &lt;- read.csv(\"data/telecom_users.csv\")\nhead(telecom_users)\n\n\nA data.frame: 6 × 22\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n⋯\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n⋯\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\n⋯\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\n⋯\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n\n\n4\n6739\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\n⋯\nNo\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n\n\n5\n432\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\n⋯\nYes\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\n⋯\nYes\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n\n\n\n\n\nLets check the proportion of column Churn == Yes and Churn == No in dataset with CrossTable() function from gmodels package.\n\n# install.packages(\"gmodels\")\n\n\nlibrary(gmodels)\nCrossTable(telecom_users$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  5986 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      4399 |      1587 | \n          |     0.735 |     0.265 | \n          |-----------|-----------|\n\n\n\n \n\n\nYou can also use CrossTable() to check cross proportions by other fields. Lets check crosstable for TechSupport and Churn:\n\nCrossTable(telecom_users$Churn, telecom_users$TechSupport) # for example\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  5986 \n\n \n                    | telecom_users$TechSupport \ntelecom_users$Churn |                  No | No internet service |                 Yes |           Row Total | \n--------------------|---------------------|---------------------|---------------------|---------------------|\n                 No |                1738 |                1192 |                1469 |                4399 | \n                    |              87.892 |              62.377 |              29.512 |                     | \n                    |               0.395 |               0.271 |               0.334 |               0.735 | \n                    |               0.587 |               0.923 |               0.847 |                     | \n                    |               0.290 |               0.199 |               0.245 |                     | \n--------------------|---------------------|---------------------|---------------------|---------------------|\n                Yes |                1222 |                  99 |                 266 |                1587 | \n                    |             243.627 |             172.904 |              81.805 |                     | \n                    |               0.770 |               0.062 |               0.168 |               0.265 | \n                    |               0.413 |               0.077 |               0.153 |                     | \n                    |               0.204 |               0.017 |               0.044 |                     | \n--------------------|---------------------|---------------------|---------------------|---------------------|\n       Column Total |                2960 |                1291 |                1735 |                5986 | \n                    |               0.494 |               0.216 |               0.290 |                     | \n--------------------|---------------------|---------------------|---------------------|---------------------|\n\n \n\n\nYou can see that most part of Churn 1222 of 1587\nNext, we will check 6 possible ways to split data for train/test sets.\n\n\n26.2.1 Split with sample()\n\nsample_size = round(nrow(telecom_users)*.70) # setting what is 70%\nprint(paste0(\"Size: \", sample_size))\n\nindex &lt;- sample(nrow(telecom_users), size = sample_size)\n \ntrain &lt;- telecom_users[index, ] # index is numbers of selected rows from dataset\ntest &lt;-telecom_users[-index, ] # -index select only rows not in index\n\n[1] \"Size: 4190\"\n\n\n\n# check Churn == Yes/No proportion in train/test\nCrossTable(train$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  4190 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      3074 |      1116 | \n          |     0.734 |     0.266 | \n          |-----------|-----------|\n\n\n\n \n\n\n\n# check Churn == Yes/No proportion in train/test\nCrossTable(test$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  1796 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      1325 |       471 | \n          |     0.738 |     0.262 | \n          |-----------|-----------|\n\n\n\n \n\n\nIts 0.260 for train and 0.276 for test. Diffrence is 1,6%, so, its close.\n\n\n\n26.2.2 Split with sample_frac from dplyr\n\nlibrary(dplyr)\nset.seed(2023)\n\n# Using the above function to create 70 - 30 slipt into test and train\n\ntu &lt;- telecom_users %&gt;% mutate(Id = row_number())\n\ntrain &lt;- tu %&gt;% sample_frac(.70)\ntest &lt;- tu[-train$Id, ]\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\nnrow(train)\n\n4190\n\n\n\n# check Churn == Yes/No proportion in train/test\nCrossTable(train$Churn)\nCrossTable(test$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  4190 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      3099 |      1091 | \n          |     0.740 |     0.260 | \n          |-----------|-----------|\n\n\n\n \n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  1796 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      1300 |       496 | \n          |     0.724 |     0.276 | \n          |-----------|-----------|\n\n\n\n \n\n\nsample_n made other proportion of Churn == Yes/No and difference just 0.7%.\n\n\n\n26.2.3 Split with createDataPartition() from caret\n\n#install.packages(\"caret\")\n\nUpdating HTML index of packages in '.Library'\n\nMaking 'packages.html' ...\n done\n\n\n\n\nlibrary(caret)\nset.seed(2023)\n \nindex = createDataPartition(telecom_users$Churn, p = 0.70, list = FALSE)\ntrain = telecom_users[index, ]\ntest = telecom_users[-index, ]\n\nLoading required package: ggplot2\n\nLoading required package: lattice\n\n\n\n\n# check Churn == Yes/No proportion in train/test\nCrossTable(train$Churn)\nCrossTable(test$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  4191 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      3080 |      1111 | \n          |     0.735 |     0.265 | \n          |-----------|-----------|\n\n\n\n \n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  1795 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      1319 |       476 | \n          |     0.735 |     0.265 | \n          |-----------|-----------|\n\n\n\n \n\n\nCkeck the proportion of target variable. Caret trying to make the same split for both train and test. This is one of the best split methods in R.\n\n\n\n26.2.4 Split with sample.split from caTools\n\n#install.packages(\"caTools\")\n\n\nlibrary(caTools)\n \nset.seed(2023)\nsample = sample.split(telecom_users$Churn, SplitRatio = .70)\n\ntrain = telecom_users[sample, ]\ntest  = telecom_users[!sample, ]\n\n\n# check Churn == Yes/No proportion in train/test\nCrossTable(train$Churn)\nCrossTable(test$Churn)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  4190 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      3079 |      1111 | \n          |     0.735 |     0.265 | \n          |-----------|-----------|\n\n\n\n \n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  1796 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |      1320 |       476 | \n          |     0.735 |     0.265 | \n          |-----------|-----------|\n\n\n\n \n\n\n\n\nДля нашого курсу це не потрібно поки! Переходимо до наступної теми. Цей матеріал в детелях буде розглянуто під час вивчення крос-валідації.",
    "crumbs": [
      "ТЕМА 6. ПОДІЛ ВИБІРКИ",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Split: Train, Test and Validation sets</span>"
    ]
  },
  {
    "objectID": "train-test-validation-samples.html#splitting-for-n-folds",
    "href": "train-test-validation-samples.html#splitting-for-n-folds",
    "title": "26  Data Split: Train, Test and Validation sets",
    "section": "26.3 Splitting for n-folds",
    "text": "26.3 Splitting for n-folds\n\n# read data again\nlibrary(caret)\ntelecom_users &lt;- read.csv(\"../../data/telecom_users.csv\")\nnrow(telecom_users)\nhead(telecom_users)\n\n5986\n\n\n\nA data.frame: 6 × 22\n\n\n\nX\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n1869\n7010-BRBUU\nMale\n0\nYes\nYes\n72\nYes\nYes\nNo\n...\nNo internet service\nNo internet service\nNo internet service\nNo internet service\nTwo year\nNo\nCredit card (automatic)\n24.1\n1734.65\nNo\n\n\n2\n4528\n9688-YGXVR\nFemale\n0\nNo\nNo\n44\nYes\nNo\nFiber optic\n...\nYes\nNo\nYes\nNo\nMonth-to-month\nYes\nCredit card (automatic)\n88.15\n3973.20\nNo\n\n\n3\n6344\n9286-DOJGF\nFemale\n1\nYes\nNo\n38\nYes\nYes\nFiber optic\n...\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nBank transfer (automatic)\n74.95\n2869.85\nYes\n\n\n4\n6739\n6994-KERXL\nMale\n0\nNo\nNo\n4\nYes\nNo\nDSL\n...\nNo\nNo\nNo\nYes\nMonth-to-month\nYes\nElectronic check\n55.9\n238.50\nNo\n\n\n5\n432\n2181-UAESM\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\n...\nYes\nNo\nNo\nNo\nMonth-to-month\nNo\nElectronic check\n53.45\n119.50\nNo\n\n\n6\n2215\n4312-GVYNH\nFemale\n0\nYes\nNo\n70\nNo\nNo phone service\nDSL\n...\nYes\nYes\nNo\nYes\nTwo year\nYes\nBank transfer (automatic)\n49.85\n3370.20\nNo\n\n\n\n\n\n\nfolds &lt;- createFolds(telecom_users)\nfolds\n\n\n    $Fold01\n        \n110\n\n    $Fold02\n        \n3415\n\n    $Fold03\n        \n217\n\n    $Fold04\n        8\n    $Fold05\n        \n1622\n\n    $Fold06\n        \n918\n\n    $Fold07\n        \n1920\n\n    $Fold08\n        \n51214\n\n    $Fold09\n        11\n    $Fold10\n        \n671321\n\n\n\n\n\n#library(caret)\n#library(mlbench)\n#data(Sonar)\n \n#folds &lt;- createFolds(Sonar$Class)\n#str(folds)",
    "crumbs": [
      "ТЕМА 6. ПОДІЛ ВИБІРКИ",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Split: Train, Test and Validation sets</span>"
    ]
  },
  {
    "objectID": "train-test-validation-samples.html#references",
    "href": "train-test-validation-samples.html#references",
    "title": "26  Data Split: Train, Test and Validation sets",
    "section": "26.4 References",
    "text": "26.4 References\n\nAbout Train, Validation and Test Sets in Machine Learning by Tarang Shah. Url: https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7",
    "crumbs": [
      "ТЕМА 6. ПОДІЛ ВИБІРКИ",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Split: Train, Test and Validation sets</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "27  EDA на прикладі вина",
    "section": "",
    "text": "27.1 Етапи дослідницького аналізу даних (Exploratory Data Analysis)\nОволодіння дослідницьким аналізом даних (EDA) є важливим для розуміння ваших даних, виявлення закономірностей та отримання інсайтів, які можуть інформувати подальший аналіз або прийняття рішень. Дані є життєво важливими для передових груп, і здатність витягувати інсайти з даних стала важливим навиком у сучасному світі, орієнтованому на статистику. Дослідницький аналіз даних (EDA) є потужним методом, який дозволяє аналітикам, вченим та дослідникам отримати повне уявлення про свої дані до початку формального моделювання або тестування гіпотез.\nЦе ітеративний процес, який включає узагальнення, візуалізацію та дослідження інформації для виявлення закономірностей, аномалій та взаємозв’язків, які можуть бути неочевидними на перший погляд. У цій статті ми зрозуміємо та реалізуємо критичні кроки для виконання дослідницького аналізу даних. Ось кроки, які допоможуть вам оволодіти EDA:",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>EDA на прикладі вина</span>"
    ]
  },
  {
    "objectID": "eda.html#етапи-дослідницького-аналізу-даних-exploratory-data-analysis",
    "href": "eda.html#етапи-дослідницького-аналізу-даних-exploratory-data-analysis",
    "title": "27  EDA на прикладі вина",
    "section": "",
    "text": "Збір данихз Збір та об’єднання всіх необхідних даних для аналіз у2. **Очищення даних*в: Видалення або виправлення помилок, пропущених значень та аномалій у дан и\n\n\n**Описова статистикао*: Обчислення основних статистичних показників, таких як середнє, медіана, мода, стандартне відхилення т о\nВізуалізація данис: Створення графіків та діаграм для виявлення закономірностей, трендів та взаємозв’язків у д а.\nВиявлення взаємозв’язкав: Аналіз кореляцій та інших статистичних взаємозв’язків між змі ни.\nФормулювання гіповез: Висування гіпотез на основі виявлених закономірностей та взаємозв ’ів.\nПеревірка гіпвтез: Використання статистичних методів для перевірки висунутих гтез.\nІнтерпретація резульаатів: Аналіз отриманих результатів та формулювання в и вків.\nДокументдвання: Документування всіх етапів аналізу, включаючи проблеми, рішення та висновки.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>EDA на прикладі вина</span>"
    ]
  },
  {
    "objectID": "eda.html#кроки-для-оволодіння-дослідницьким-аналізом-даних",
    "href": "eda.html#кроки-для-оволодіння-дослідницьким-аналізом-даних",
    "title": "27  EDA на прикладі вина",
    "section": "27.2 Кроки для оволодіння дослідницьким аналізом даних",
    "text": "27.2 Кроки для оволодіння дослідницьким аналізом даних\nКрок 1: Зрозумійте проблему та дані\nКрок 2: Імпортуйте та перевірте дані\nКрок 3: Обробка пропущених значень\nКрок 4: Дослідження характеристик даних\nКрок 5: Виконання трансформації даних\nКрок 6: Візуалізація взаємозв’язків даних\nКрок 7: Обробка викидів\nКрок 8: Комунікація висновків та інсайтів\n\n27.2.1 Мета цього проекту\nУ цьому проекті ми будемо аналізувати дані про біле вино і намагатися зрозуміти, які змінні відповідають за якість вина. Спочатку ми спробуємо отримати уявлення про змінні самі по собі, а потім спробуємо знайти кореляцію між ними та якістю вина з урахуванням інших факторів.\n\n#install.packages(\"GGally\")\n\n\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(GGally)\n\n\nwhite_wines &lt;- read.csv('https://raw.githubusercontent.com/Ashish25/EDA_Rmd/refs/heads/master/wineQualityWhites.csv', \n                        header = TRUE, \n                        sep = ',', \n                        row.names = 1)\n\n\n\n27.2.2 Контекст\nДва набори даних стосуються червоних і білих варіантів португальського вина “Vinho Verde”. Для отримання додаткової інформації зверніться до посилання [Cortez et al., 2009]. Через питання конфіденційності та логістики доступні лише фізико-хімічні (вхідні) та сенсорні (вихідні) змінні (наприклад, немає даних про типи винограду, бренд вина, ціну продажу вина тощо).\nЦі набори даних можна розглядати як завдання класифікації або регресії. Класи впорядковані і не збалансовані (наприклад, набагато більше звичайних вин, ніж відмінних або поганих).\nЦей набір даних також доступний у репозиторії машинного навчання UCI: https://archive.ics.uci.edu/ml/datasets/wine+quality.\n\n\n27.2.3 Опис\nДля отримання додаткової інформації читайте [Cortez et al., 2009].\nВхідні змінні (на основі фізико-хімічних тестів): 1. fixed acidity - фіксована кислотність 2. volatile acidity - летка кислотність 3. citric acid - лимонна кислота 4. residual sugar - залишковий цукор 5. chlorides - хлориди 6. free sulfur dioxide - вільний діоксид сірки 7. total sulfur dioxide - загальний діоксид сірки 8. density - густина 9. pH - рівень pH 10. sulphates - сульфати 11. alcohol - вміст алкоголю\nВихідна змінна (на основі сенсорних даних): 12. quality - якість (оцінка від 0 до 10)\n\n\n27.2.4 Перший погляд\nМи маємо наступні змінні:\n\nnames(white_wines)\n\n\n'fixed.acidity''volatile.acidity''citric.acid''residual.sugar''chlorides''free.sulfur.dioxide''total.sulfur.dioxide''density''pH''sulphates''alcohol''quality'\n\n\n\nstr(white_wines)\n\n'data.frame':   4898 obs. of  12 variables:\n $ fixed.acidity       : num  7 6.3 8.1 7.2 7.2 8.1 6.2 7 6.3 8.1 ...\n $ volatile.acidity    : num  0.27 0.3 0.28 0.23 0.23 0.28 0.32 0.27 0.3 0.22 ...\n $ citric.acid         : num  0.36 0.34 0.4 0.32 0.32 0.4 0.16 0.36 0.34 0.43 ...\n $ residual.sugar      : num  20.7 1.6 6.9 8.5 8.5 6.9 7 20.7 1.6 1.5 ...\n $ chlorides           : num  0.045 0.049 0.05 0.058 0.058 0.05 0.045 0.045 0.049 0.044 ...\n $ free.sulfur.dioxide : num  45 14 30 47 47 30 30 45 14 28 ...\n $ total.sulfur.dioxide: num  170 132 97 186 186 97 136 170 132 129 ...\n $ density             : num  1.001 0.994 0.995 0.996 0.996 ...\n $ pH                  : num  3 3.3 3.26 3.19 3.19 3.26 3.18 3 3.3 3.22 ...\n $ sulphates           : num  0.45 0.49 0.44 0.4 0.4 0.44 0.47 0.45 0.49 0.45 ...\n $ alcohol             : num  8.8 9.5 10.1 9.9 9.9 10.1 9.6 8.8 9.5 11 ...\n $ quality             : int  6 6 6 6 6 6 6 6 6 6 ...\n\n\n\nsummary(white_wines)\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 3.800   Min.   :0.0800   Min.   :0.0000   Min.   : 0.600  \n 1st Qu.: 6.300   1st Qu.:0.2100   1st Qu.:0.2700   1st Qu.: 1.700  \n Median : 6.800   Median :0.2600   Median :0.3200   Median : 5.200  \n Mean   : 6.855   Mean   :0.2782   Mean   :0.3342   Mean   : 6.391  \n 3rd Qu.: 7.300   3rd Qu.:0.3200   3rd Qu.:0.3900   3rd Qu.: 9.900  \n Max.   :14.200   Max.   :1.1000   Max.   :1.6600   Max.   :65.800  \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      \n Min.   :0.00900   Min.   :  2.00      Min.   :  9.0        Min.   :0.9871  \n 1st Qu.:0.03600   1st Qu.: 23.00      1st Qu.:108.0        1st Qu.:0.9917  \n Median :0.04300   Median : 34.00      Median :134.0        Median :0.9937  \n Mean   :0.04577   Mean   : 35.31      Mean   :138.4        Mean   :0.9940  \n 3rd Qu.:0.05000   3rd Qu.: 46.00      3rd Qu.:167.0        3rd Qu.:0.9961  \n Max.   :0.34600   Max.   :289.00      Max.   :440.0        Max.   :1.0390  \n       pH          sulphates         alcohol         quality     \n Min.   :2.720   Min.   :0.2200   Min.   : 8.00   Min.   :3.000  \n 1st Qu.:3.090   1st Qu.:0.4100   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.180   Median :0.4700   Median :10.40   Median :6.000  \n Mean   :3.188   Mean   :0.4898   Mean   :10.51   Mean   :5.878  \n 3rd Qu.:3.280   3rd Qu.:0.5500   3rd Qu.:11.40   3rd Qu.:6.000  \n Max.   :3.820   Max.   :1.0800   Max.   :14.20   Max.   :9.000  \n\n\n\n\n27.2.5 Підсумок\nПідсумок показує, що максимальні значення для змінних residual sugar (залишковий цукор), chlorides (хлориди) та free sulfur dioxide (вільний діоксид сірки) значно відрізняються від інших значень. Це можуть бути викиди або проблеми з введенням даних, або ж це можуть бути дійсно особливі вина.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>EDA на прикладі вина</span>"
    ]
  },
  {
    "objectID": "eda.html#одновимірні-графіки",
    "href": "eda.html#одновимірні-графіки",
    "title": "27  EDA на прикладі вина",
    "section": "27.3 Одновимірні графіки",
    "text": "27.3 Одновимірні графіки\nДавайте побудуємо розподіл кожної змінної, оскільки я хочу спочатку отримати уявлення про змінні. Виходячи з форми розподілу, тобто нормального, позитивного або негативного перекосу, і кількості викидів, це також допоможе нам зрозуміти, чого очікувати, коли я буду будувати графіки різних змінних одна проти одної.\n\n27.3.1 Якість (Quality)\nОскільки якість є числовою змінною, ми додамо її як фактор.\n\nwhite_wines$quality.lvl=cut(white_wines$quality, \n                          c(0,1,2,3,4,5,6,7,8,9,10), \n                          labels = c(1,2,3,4,5,6,7,8,9,10))\n\nggplot(aes(x = quality.lvl, fill = after_stat(count)), data = white_wines) +\n  geom_bar() +\n  scale_x_discrete()\n\n\n\n\n\n\n\n\nСхоже на нормальний розподіл. Більшість вин мають якість близько 5/6/7. Оскільки вина хорошої та поганої якості майже як викиди, може бути важко отримати точну модель якості вина. Давайте подивимося на інші графіки.\n\n\n\n27.3.2 Алкоголь (Alcohol)\nДавайте перевіримо розподіл алкоголю.\n\nggplot(aes(x = alcohol, fill = ..count..), data = white_wines) +\n  geom_histogram(binwidth = 0.1) +\n  scale_fill_gradient(low = \"#fcf4d9\", high = \"#f9ae00\")\n\n\n\n\n\n\n\n\nНабагато більш різнорідний, але ми маємо гарний пік близько 9,5% за об’ємом. Загалом, це все ще виглядає як нормальний розподіл, з невеликим перекосом вліво.\n\n\n27.3.3 Залишковий цукор (Residual Sugar)\nЗалишковий цукор - це кількість цукру, що залишилася після ферментації.\n\ngridExtra:: grid.arrange(ggplot(white_wines, aes( x = 1, y = residual.sugar ) ) + \n               geom_jitter(color = 'blue', size = 0.2, alpha = 0.5 ) +\n               geom_boxplot(alpha = 0.4, color = 'red' ) ,\n             ggplot(white_wines, aes( x   = residual.sugar  ) ) + scale_x_log10(breaks = seq(1,30,8)) +\n                   geom_histogram(binwidth = 0.04),ncol=2)\n\n\n\n\n\n\n\n\nШкала X в логарифмічному масштабі (log10). Ми маємо той самий тип розподілу, але з довгим хвостом. Пік знаходиться близько 1,5 г/дм³, а дані, здається, доходять до 25 г/дм³. Давайте подивимося на підсумок.\n\nsummary(white_wines$residual.sugar)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.600   1.700   5.200   6.391   9.900  65.800 \n\n\nМаксимальне значення становить 65 (що досить дивно), тоді як третій квартиль знаходиться на рівні 9,9, а медіана - на рівні 5,2. Отже, це досить особливі вина або є якась помилка в даних.\n\n\n27.3.4 Хлориди (Chlorides)\nХлориди - це сіль, і я вважаю, що вони не додають гарного смаку вину.\n\ngridExtra:: grid.arrange(ggplot(white_wines, aes( x = 1, y = chlorides ) ) + \n               geom_jitter(color = 'violet', size = 0.2, alpha = 0.5 ) +\n               geom_boxplot(outlier.colour = 'red', alpha = 0.2, outlier.size = 0.5 ) ,\n             ggplot(white_wines, aes( x   = chlorides, fill = ..count..) ) +\n               scale_x_continuous(trans = 'log10') +\n               scale_fill_gradient2(low = \"grey\", high = \"steelblue\", midpoint = 100, mid = \"lightblue\") +\n                   geom_histogram(binwidth = 0.01),ncol=2)\n\n\n\n\n\n\n\n\nМи маємо нормальний розподіл після перетворення осі X за допомогою функції log10. Як і у випадку з залишковим цукром, ми маємо правий довгий хвіст даних.\n\nsummary(white_wines$chlorides)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00900 0.03600 0.04300 0.04577 0.05000 0.34600 \n\n\nОдиниця виміру така ж, як і для залишкового цукру, але значення набагато нижчі. Медіана становить 0,043 г/дм³. Максимальне значення знову більше ніж у 80 разів вище, що не є нормальним.\n\n\n\n27.3.5 Лимонна кислота (Citric acid)\nУ невеликих кількостях лимонна кислота може додати свіжості та яскравості вину.\n\ngridExtra:: grid.arrange(ggplot(white_wines, aes( x = 1, y = citric.acid ) ) + \n               geom_jitter(color = 'blue', size = 0.2, alpha = 0.5 ) +\n               geom_boxplot(outlier.colour = 'red', alpha = 0.2, outlier.size = 0.5 ) ,\n             ggplot(white_wines, aes( x   = citric.acid, fill = ..count..) ) +\n               scale_x_continuous(trans = 'log10') +\n               scale_fill_gradient2(mid = \"lightgrey\") +\n                   geom_histogram(binwidth = 0.03),ncol=2)\n\nWarning message in scale_x_continuous(trans = \"log10\"):\n\"log-10 transformation introduced infinite values.\"\nWarning message:\n\"Removed 19 rows containing non-finite outside the scale range (`stat_bin()`).\"\n\n\n\n\n\n\n\n\n\nРозглядаючи лимонну кислоту в логарифмічному масштабі (log10), ми знову маємо нормальний розподіл, звісно, з кількома помітними викидами.\n\nsummary(white_wines$citric.acid)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.2700  0.3200  0.3342  0.3900  1.6600 \n\n\nЗнову ж таки, медіана становить 0,32 г/дм³, тоді як максимальне значення - 1,6, а мінімальне - 0.\n\n\n27.3.6 pH\nШкала pH від 0 (дуже кислий) до 14 (дуже лужний). Більшість вин мають pH між 3 і 3,5.\n\nggplot(aes(x = pH), data = white_wines) +\n  geom_bar(binwidth = 0.01, position = 'identity', fill = '#ffc532', colour = 'grey')\n\nWarning message in geom_bar(binwidth = 0.01, position = \"identity\", fill = \"#ffc532\", :\n\"Ignoring unknown parameters: `binwidth`\"\n\n\n\n\n\n\n\n\n\npH має нормальний розподіл з піком без необхідності використання логарифмічної шкали. Ми можемо побачити, що дані досить розсіяні, але, як зазначено в описі, більшість точок даних знаходяться між 3 і 3,5.\n\nsummary(white_wines$pH)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.720   3.090   3.180   3.188   3.280   3.820 \n\n\nМінімальне та максимальне значення знову досить далеко від медіани та відповідно першого і третього квартилів, але не так багато варіацій.\n\n\n27.3.7 Летюча кислотність (Volatile acidity)\nЛетюча кислотність - це кількість оцтової кислоти у вині. При високій концентрації вона надає неприємного смаку оцту, що, на мою думку, характерно для вина низької якості.\n\nggplot(aes(x = volatile.acidity, fill = ..count..), data = white_wines) +\n  geom_bar(binwidth = 0.01, position = 'identity') +\n  scale_fill_gradient(low = \"#56B1F7\", high = \"#132B43\")\n\nWarning message in geom_bar(binwidth = 0.01, position = \"identity\"):\n\"Ignoring unknown parameters: `binwidth`\"\n\n\n\n\n\n\n\n\n\n\nsummary(white_wines$volatile.acidity)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0800  0.2100  0.2600  0.2782  0.3200  1.1000 \n\n\nДані все ще мають довгий хвіст з піком близько 0,3 г/дм³ оцтової кислоти.\nМаксимальне значення дійсно виходить за межі інших точок даних. Медіана становить 0,26 г/дм³ оцтової кислоти, а середнє значення дуже схоже - 0,2782 г/дм³ оцтової кислоти.\n\n### Загальний діоксид сірки (Total Sulfur Dioxide)\n\nЗагальний діоксид сірки - це сума вільних (для окислення вина) та зв'язаних форм сірки. При високій концентрації він може впливати на смак.\n\n\nggplot(aes(x = total.sulfur.dioxide), data = white_wines) +\n  geom_bar(binwidth = 0.01, position = 'identity', fill = \"grey25\", alpha = 0.7) +\n  coord_cartesian() +\n  scale_x_log10(breaks = seq(1,500,100))\n\nВісь x перетворена за допомогою log10. Ми маємо кілька горбів, але загалом нормальний розподіл. Деякі точки даних здаються ізольованими. Давайте розглянемо підсумок змінної.\n\nsummary(white_wines$total.sulfur.dioxide)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    9.0   108.0   134.0   138.4   167.0   440.0 \n\n\nМаксимальне значення дійсно занадто високе порівняно з медіаною та третім квартилем. Я думаю, що це помилка у звітуванні/записі даних.\nОсновна лінія досліджень буде стосуватися взаємозв’язку між усіма змінними та якістю.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>EDA на прикладі вина</span>"
    ]
  },
  {
    "objectID": "eda.html#двовимірний-аналіз",
    "href": "eda.html#двовимірний-аналіз",
    "title": "27  EDA на прикладі вина",
    "section": "27.4 Двовимірний аналіз",
    "text": "27.4 Двовимірний аналіз\n\n27.4.1 Алкоголь і якість\nЧи впливає рівень алкоголю на якість?\n\nggplot(aes(x = quality.lvl, y = alcohol), data = white_wines) +\n  geom_jitter(alpha = 0.5, size = 0.2) +\n  geom_boxplot(outlier.colour= 'red', alpha = 0.5, color = 'blue') +\n  ggtitle(\"Alcohol by quality\")\n\ncor(white_wines$alcohol, white_wines$quality)\n\n0.435574715461373\n\n\n\n\n\n\n\n\n\nІснує середня кореляція (близько 0,435) між якістю та алкоголем, графік показує, що чим вища якість, тим кращий алкоголь. Це особливо вірно для вин вищого класу.\n\n\n\n27.4.2 Змінні кислотності\nУ нас є 3 типи кислотності:\n\nФіксована кислотність: більшість кислот, що містяться у вині, є фіксованими або нелеткими (не випаровуються легко)\nЛетюча кислотність: кількість оцтової кислоти у вині, яка при занадто високих рівнях може призвести до неприємного смаку оцту\nЛимонна кислота: зустрічається в невеликих кількостях, лимонна кислота може додати винам “свіжості” та смаку\n\nВони всі повинні бути пов’язані з pH, я думаю.\n\ncor(white_wines$pH, white_wines$fixed.acidity)\n\ncor(white_wines$pH, white_wines$volatile.acidity)\n\ncor(white_wines$pH, white_wines$citric.acid)\n\n-0.425858290991382\n\n\n-0.0319153682734889\n\n\n-0.163748211400624\n\n\nОтже, фіксована кислотність і лимонна кислота досить сильно корелюють з pH (близько -0,42 і -0,16 відповідно), але не летюча кислотність (близько -0,03).\n\nggplot(aes(x = pH, y = volatile.acidity), data = white_wines) +\n  geom_smooth(color = 'red1') +\n  geom_jitter(alpha = 0.5, color = 'cyan4', size = 0.6) +\n  ggtitle(\"White Wine Volatile acidity by pH\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nЛетюча кислотність і pH не пов’язані. Навіть для вин високої якості pH, здається, сильно варіюється. Це підтверджує низьку кореляцію, виявлену раніше.\n\n\n27.4.3 Якість і pH\nДавайте тепер уважніше розглянемо взаємозв’язок між pH і якістю.\n\nggplot(aes(x = quality.lvl, y = pH), data = white_wines) +\n  geom_jitter(alpha = 0.5, color = 'orange', size = 0.7) +\n  geom_boxplot(outlier.color = 'red', alpha = 0.5) +\n  stat_summary(fun.y = \"mean\", geom = \"point\", color = \"blue\", shape = 8, size = 4)\n  ggtitle(\"White Wine pH by quality\")\n\n$title\n[1] \"White Wine pH by quality\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\n\n\n\n\n\n\n\nЧим вища якість, тим вищий pH, але це не дуже чітко видно.\n\ncor(white_wines$pH, white_wines$quality)\n\n0.0994272457366642\n\n\nКоефіцієнт кореляції між ними дуже низький і становить 0,09. Отже, перше враження від графіка насправді не підтверджується числом.\nА як щодо солі? (Хлорид)\n\n\n27.4.4 Якість і хлориди\nНагадаємо, підсумок хлоридів.\n\nsummary(white_wines$chlorides)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00900 0.03600 0.04300 0.04577 0.05000 0.34600 \n\n\nПідсумок показує, що максимальне значення становить 0,346, тоді як третій квартиль становить 0,05. Давайте збільшимо масштаб, щоб отримати кращий огляд.\n\n#coloring outlier and limiting the y scale to have a better look\n\nggplot(aes(x = quality.lvl, y = chlorides), data = white_wines) +\n  geom_jitter(alpha = 0.5, color = 'orange', size =0.6) +\n  geom_boxplot(outlier.colour = 'red', alpha = 0.3) +\n  stat_summary(fun.y = \"mean\", geom = \"point\", color = \"blue\", shape = 8, size = 4) +\n  coord_cartesian() +\n  ylim(quantile(white_wines$chlorides,0.05),quantile(white_wines$chlorides,0.95)) +\n  ggtitle(\"Chlorides by quality, red outliers\")\n\nWarning message:\n\"Removed 469 rows containing non-finite outside the scale range (`stat_boxplot()`).\"\nWarning message:\n\"Removed 469 rows containing non-finite outside the scale range (`stat_summary()`).\"\nWarning message:\n\"Removed 509 rows containing missing values or values outside the scale range (`geom_point()`).\"\n\n\n\n\n\n\n\n\n\nГрафік обмежений між 0,05 і 0,95 квантилями. Є деякі перекриття, але чим вища якість, тим менше хлоридів. (Це певною мірою підтверджує нашу попередню гіпотезу, що наявність солі погано впливає на смак вина)\n\ncor(white_wines$quality,white_wines$chlorides)\n\n-0.209934410946761\n\n\nКореляція дуже низька і становить -0,20. Отже, взаємозв’язок між цими двома змінними не дуже сильний.\nОстання змінна, яка очевидно впливає на смак (а отже, і на якість), на мою думку, це цукор.\n\n\n27.4.5 Якість і залишковий цукор\n\nggplot(aes(x = quality.lvl, y = residual.sugar), data = white_wines) +\n  geom_jitter(color = 'orange', alpha = 0.5, size = 0.5) +\n  geom_boxplot(outlier.color='red', alpha = 0.5) +\n  stat_summary(fun.y = \"mean\", geom = \"point\", color = \"blue\", shape = 8, size = 4) +\n  coord_cartesian() +\n  ylim(quantile(white_wines$residual.sugar,0.05),\n       quantile(white_wines$residual.sugar,0.95)) +\n  ggtitle(\"Residual sugar by quality (0.05 to 0.95 quantile)\")\n\nWarning message:\n\"Removed 411 rows containing non-finite outside the scale range (`stat_boxplot()`).\"\nWarning message:\n\"Removed 411 rows containing non-finite outside the scale range (`stat_summary()`).\"\nWarning message:\n\"Removed 481 rows containing missing values or values outside the scale range (`geom_point()`).\"\n\n\n\n\n\n\n\n\n\nНемає чіткого взаємозв’язку між цукром і якістю. Точковий графік показує точки по всьому діапазону. Що стосується діаграми розмаху, то медіана здається вищою для середнього діапазону якості, але нічого особливого тут не помітно.\n\ncor(white_wines$quality,white_wines$residual.sugar)\n\n-0.0975768288946932\n\n\nКореляція підтверджує, що взаємозв’язок дуже низький (-0,09), майже незначний.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>EDA на прикладі вина</span>"
    ]
  },
  {
    "objectID": "eda.html#багатовимірний-аналіз",
    "href": "eda.html#багатовимірний-аналіз",
    "title": "27  EDA на прикладі вина",
    "section": "27.5 Багатовимірний аналіз",
    "text": "27.5 Багатовимірний аналіз\nУважніше розглянемо хлориди.\n\n27.5.1 Хлориди та алкоголь за якістю\n\nggplot_base_alcohol.quality&lt;-ggplot(aes(x = alcohol), data = white_wines) +\n  facet_wrap(~quality.lvl, scales = 'free_y')\n  \nggplot_base_alcohol.quality +\n   geom_jitter(aes(y = chlorides), alpha =0.2, size = 0.4) +\n  geom_smooth(aes(y = chlorides)) +\n  ggtitle(\"Chlorides by alcohol facetted by quality\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\nWarning message:\n\"Failed to fit group -1.\nCaused by error in `smooth.construct.cr.smooth.spec()`:\n! x has insufficient unique values to support 10 knots: reduce k.\"\n\n\n\n\n\n\n\n\n\nЗагальна тенденція знижується: менше хлоридів при підвищенні рівня алкоголю. Але ми можемо побачити, що для найвищих 4 якостей (6, 7, 8) ми маємо основну концентрацію навколо 10-12, де рівень солі трохи підвищується. Це можуть бути деякі викиди, оскільки ми маємо лише кілька точок для цієї якості.\n\nwith(subset(white_wines, quality.lvl==5),cor(chlorides,alcohol))\nwith(subset(white_wines, quality.lvl==6),cor(chlorides,alcohol))\nwith(subset(white_wines, quality.lvl==7),cor(chlorides,alcohol))\nwith(subset(white_wines, quality.lvl==8),cor(chlorides,alcohol))\n\n-0.22310980966467\n\n\n-0.319942402779244\n\n\n-0.554550369137382\n\n\n-0.512482443523902\n\n\nКореляція між хлоридами та алкоголем становить:\n\n-0,223 для якості 5\n-0,319 для якості 6\n-0,554 для якості 7\n-0,512 для якості 8\n\nКореляції слабкі для 5-го та 6-го рівнів якості, але досить помітні між 7-м і 8-м рівнями.\n\n\n27.5.2 Залишковий цукор і алкоголь за якістю\nДавайте подивимося на те ж саме з цукром:\n\nggplot_base_alcohol.quality +\n  geom_jitter(aes(y = residual.sugar), alpha =0.2, size = 0.4) +\n  geom_smooth(aes(y = residual.sugar)) +\n  ggtitle(\"Residual sugar by alcohol facetted by quality\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\nWarning message:\n\"Failed to fit group -1.\nCaused by error in `smooth.construct.cr.smooth.spec()`:\n! x has insufficient unique values to support 10 knots: reduce k.\"\n\n\n\n\n\n\n\n\n\nЗагальна тенденція до зниження тут помітна. Давайте підтвердимо це за допомогою кореляції.\n\nwith(subset(white_wines, quality.lvl==5),cor(residual.sugar,alcohol))\nwith(subset(white_wines, quality.lvl==6),cor(residual.sugar,alcohol))\nwith(subset(white_wines, quality.lvl==7),cor(residual.sugar,alcohol))\nwith(subset(white_wines, quality.lvl==8),cor(residual.sugar,alcohol))\n\n-0.441482522581375\n\n\n-0.454996081839005\n\n\n-0.480936935895533\n\n\n-0.52201078865539\n\n\nКореляції не є сильними:\n\n-0,441 для якості 5\n-0,454 для якості 6\n-0,480 для якості 7\n-0,522 для якості 8\n\nКореляція для всіх якостей є досить послідовною, 8-ма якість має вищу кореляцію, але у нас є лише кілька точок даних для цієї якості, тому кореляція може бути зумовлена цим.\nТенденції на графіках для хлоридів і залишкового цукру виглядають схожими, обидва мають негативну кореляцію. Хлориди та залишковий цукор можуть бути пов’язані.\n\n\n27.5.3 Хлориди та залишковий цукор за якістю\n\nggplot(aes(x = chlorides, y = residual.sugar), data = white_wines) +\n  geom_jitter(alpha=0.4, size = 0.3) +\n  geom_smooth(color=I('orange')) +\n  xlim(quantile(white_wines$chlorides,0.05), \n       quantile(white_wines$chlorides,0.95)) +\n  ylim(quantile(white_wines$residual.sugar,0.05), \n       quantile(white_wines$residual.sugar,0.95)) +\n  facet_wrap(~quality.lvl, scales = 'free') +\n  ggtitle(\"Chlorides by residual sugar smoother, .95 quantile\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\nWarning message:\n\"Removed 841 rows containing non-finite outside the scale range (`stat_smooth()`).\"\nWarning message:\n\"Failed to fit group -1.\nCaused by error in `smooth.construct.cr.smooth.spec()`:\n! x has insufficient unique values to support 10 knots: reduce k.\"\nWarning message:\n\"Removed 939 rows containing missing values or values outside the scale range (`geom_point()`).\"\n\n\n\n\n\n\n\n\n\nВзаємозв’язок між хлоридами та залишковим цукром, здається, стає більш хвилястим (нелінійним) із покращенням якості. Це може зробити смак білого вина неприємним і непередбачуваним у кінцевому підсумку.\nДавайте подивимося, чи допоможе співвідношення цукру до хлоридів:\n\n#adding ratio of residual.sugar/chlorides to white_wines dataframe\nwhite_wines$residual.sugar_chlorides=with(white_wines,residual.sugar/chlorides)\n\nggplot(aes(x = quality.lvl, y = residual.sugar_chlorides, color = density), data = white_wines) +\n  geom_jitter(alpha = 0.3, size = 0.7) +\n  geom_boxplot(alpha = 0.2) +\n  ylim(quantile(white_wines$residual.sugar_chlorides,0.05), \n       quantile(white_wines$residual.sugar_chlorides,0.95)) +\n  ggtitle(\"Residual sugar/Chlorides by quality, .95 quantile\")\n\nWarning message:\n\"Removed 490 rows containing non-finite outside the scale range (`stat_boxplot()`).\"\nWarning message:\n\"The following aesthetics were dropped during statistical transformation: colour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical variable into a factor?\"\nWarning message:\n\"Removed 490 rows containing missing values or values outside the scale range (`geom_point()`).\"\n\n\n\n\n\n\n\n\n\nНемає шансів тут. Я сподівався на деякі кластери точок для кожного рівня якості, але вони можуть сильно відрізнятися.\n\nwith(subset(white_wines, quality.lvl==5),cor(residual.sugar,chlorides))\nwith(subset(white_wines, quality.lvl==6),cor(residual.sugar,chlorides))\nwith(subset(white_wines, quality.lvl==7),cor(residual.sugar,chlorides))\nwith(subset(white_wines, quality.lvl==8),cor(residual.sugar,chlorides))\n\n0.0225277904423787\n\n\n0.0351203237857836\n\n\n0.275530791594432\n\n\n0.314866893426447\n\n\nКореляція для якості 5, 6 і 7 є досить низькою (0,02, 0,03 і 0,27 відповідно), але для якості 8 вона значна. Це може бути через те, що у нас менше точок для якості 8, ніж для інших рівнів якості. Крім того, у нас є дуже віддалені значення як для залишкового цукру, так і для хлоридів, давайте обчислимо ті ж значення на підмножині, обмеженій між 0,05 і 0,95 квантилями.\n\nwhite_wines_quant&lt;-subset(white_wines,\n                        quantile(white_wines$residual.sugar,0.05)&lt;=residual.sugar\n                        & quantile(white_wines$residual.sugar,0.95)&gt;=residual.sugar\n                        & quantile(white_wines$chlorides,0.05)&lt;=chlorides \n                        & quantile(white_wines$chlorides,0.95)&gt;=chlorides)\n\nwith(subset(white_wines_quant, quality.lvl==5),\n     cor(residual.sugar, chlorides))\nwith(subset(white_wines_quant, quality.lvl==6),\n     cor(residual.sugar, chlorides))\nwith(subset(white_wines_quant, quality.lvl==7),\n     cor(residual.sugar, chlorides))\nwith(subset(white_wines_quant, quality.lvl==8),\n     cor(residual.sugar, chlorides))\n\n0.112410296722973\n\n\n0.197972817731615\n\n\n0.348869425502618\n\n\n0.417167001300737\n\n\nКореляції трохи покращилися після видалення крайніх квантилів:\n\n0,112 для якості 5\n0,197 для якості 6\n0,348 для якості 7\n0,417 для якості 8\n\nЯ думаю, що цей взаємозв’язок між залишковим цукром і хлоридами варто дослідити, щоб побачити, чи можуть інші змінні вплинути на нього. Отже, давайте зіставимо хлориди з цукром і додамо алкоголь як колір і використаємо це як основу для інших графіків.\n\n\n27.5.4 Хлориди, залишковий цукор і алкоголь за якістю\n\nwhite_wines_quant$alcohol.bucket = cut(white_wines_quant$alcohol,\n                            c(8, 9.5, 11, 12.5, 14))\nggplot(aes(x = factor(quality), y = volatile.acidity ), data = white_wines_quant) + \n   geom_boxplot( aes(fill= alcohol.bucket))  +\n  scale_fill_brewer(type='seq', guide=guide_legend(title='Quality'))\n\n\nggplot(aes(x =residual.sugar, y = chlorides, color = factor(quality)), \n       data = white_wines_quant) +\n      geom_point(alpha = 0.8, size = 1) +\n      geom_smooth(method = \"lm\", se = FALSE,size=1)  +\n  scale_color_brewer(type='seq', guide=guide_legend(title='Quality'))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nМи можемо побачити, як і раніше, що чим нижчий вміст хлоридів, тим кращий алкоголь. Крім того, низький рівень алкоголю, здається, має менше цукру і високий вміст хлоридів.\n\nwith(white_wines_quant, cor(residual.sugar_chlorides, alcohol))\nwith(subset(white_wines_quant, quality.lvl==5),\n     cor(residual.sugar_chlorides, alcohol))\nwith(subset(white_wines_quant, quality.lvl==6),\n     cor(residual.sugar_chlorides, alcohol))\nwith(subset(white_wines_quant, quality.lvl==7),\n     cor(residual.sugar_chlorides, alcohol))\nwith(subset(white_wines_quant, quality.lvl==8),\n     cor(residual.sugar_chlorides, alcohol))\n\n-0.299601532893232\n\n\n-0.350255657211274\n\n\n-0.330941786318977\n\n\n-0.295743178557349\n\n\n-0.287638527567753\n\n\nКореляція між залишковим цукром/хлоридами та алкоголем для підмножини становить -0,299, що є низьким показником. Розбиваючи за якістю, ми маємо:\n\n-0,350 для якості 5\n-0,330 для якості 6\n-0,295 для якості 7\n-0,287 для якості 8\n\nОтже, кореляція не сильно змінюється залежно від якості. У нас немає специфічного взаємозв’язку для деяких якостей.\n\n\n27.5.5 Змінні кислотності та якість\nДавайте повернемося до наших змінних кислотності:\n\nggplot_base_acidity&lt;-ggplot(aes(x = quality.lvl), data = white_wines)\n\n#fixed acidity\nggplot_base_acidity +\n  geom_boxplot(aes(y = fixed.acidity), outlier.colour = 'red', alpha = 0.2) +\n  ggtitle(\"Fixed acidity by quality\")\n\n#volatile acidity\nggplot_base_acidity +\n  geom_boxplot(aes(y = volatile.acidity), outlier.colour = 'red', alpha = 0.2) +\n  ggtitle(\"Volatile acidity by quality\")\n\n#citric acidity\nggplot_base_acidity +\n  geom_boxplot(aes(y = citric.acid), outlier.colour = 'red', alpha = 0.2) +\n  ggtitle(\"Citric acid by quality\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЦі діаграми розмаху не дуже допомагають у дослідженні. Давайте перевіримо кореляції.\n\nwith(white_wines, cor(quality, fixed.acidity))\nwith(white_wines, cor(quality, volatile.acidity))\nwith(white_wines, cor(quality, citric.acid))\n\n-0.113662830713018\n\n\n-0.194722968921134\n\n\n-0.00920909088397543\n\n\nКореляція показує, що лимонна кислота дуже слабо корелює з якістю (-0,009). З іншого боку, летка кислотність (-0,19) і фіксована кислотність (-0,11) дещо присутні, особливо летюча кислотність. Така кислотність негативно корелює з якістю, що означає, що зі збільшенням якості летюча кислотність зменшується. Оцтовий смак, який приносить летка кислотність, дійсно шкодить якості.\nОтже, тепер у нас є алкоголь, фіксована кислотність і летка кислотність, які досить сильно корелюють з якістю. Я повернуся до попередньої діаграми залишкового цукру та хлоридів і додам лимонну кислоту/летку кислотність, щоб побачити, чи зможу я отримати більше інформації.\n\n\n27.5.6 Хлориди, залишковий цукор і кислотність за якістю\n\nsugar.chlorides&lt;-ggplot(aes(x = residual.sugar, y = chlorides), \n                        data = white_wines_quant) +\n  facet_wrap(~quality.lvl, scales = 'free')\n\nsugar.chlorides +\n  geom_jitter(aes(color=volatile.acidity), size = 0.7) +\n  scale_colour_gradient2(low=\"yellow\", high=\"blue\", midpoint=0.3) +\n  ggtitle('Chlorides, residual sugar & volatile acidity facetted by quality')\n\nwith(white_wines_quant, cor(residual.sugar_chlorides, volatile.acidity))\n\n0.0818414498956549\n\n\n\n\n\n\n\n\n\nМи можемо побачити, що зі збільшенням рівня леткої кислотності якість, здається, трохи знижується (-0,19), що показує незначну негативну кореляцію.\n\nsugar.chlorides+\n  geom_jitter(aes(color=citric.acid), size = 0.7) +\n  scale_colour_gradient2(low=\"red\", high=\"blue\", midpoint=0.25) +\n  ggtitle('Chlorides, residual sugar & citric acid facetted by quality')\n\n\n\n\n\n\n\n\nЛимонна кислота і якість білого вина дужепов’язані. Здається, вирівнюються на всіх рівнях.\n\nwith(white_wines_quant, cor(residual.sugar_chlorides, citric.acid))\nwith(subset(white_wines_quant, quality.lvl==5),\n     cor(residual.sugar_chlorides, citric.acid))\nwith(subset(white_wines_quant, quality.lvl==6),\n     cor(residual.sugar_chlorides, citric.acid))\nwith(subset(white_wines_quant, quality.lvl==7),\n     cor(residual.sugar_chlorides, citric.acid))\nwith(subset(white_wines_quant, quality.lvl==8),\n     cor(residual.sugar_chlorides, citric.acid))\n\n0.0994882466327046\n\n\n0.205085454808991\n\n\n0.0291507936288976\n\n\n0.00739929991358686\n\n\n0.0828935988213208\n\n\nКореляції показують, що насправді нічого немає. Кореляція для нашої підмножини становить 0,09. Розбиваючи за якістю:\n\n0,20 для якості 5\n0,02 для якості 6\n0,007 для якості 7\n-0,08 для якості 8\n\n\n\n27.5.7 Хлориди, залишковий цукор і щільність за якістю\nЩільність пов’язана з алкоголем, тому давайте подивимося, чи можемо ми знайти щось тут.\n\nsugar.chlorides+\n  geom_jitter(aes(color=density), size = 0.7) +\n  scale_colour_gradient2(low=\"red\", high=\"blue\", midpoint=0.9975) +\n  ggtitle(\"Chlorides by residual sugar and density\")\n\n\n\n\n\n\n\n\nЄ певна тенденція: краща якість вина має вищий залишковий цукор і вищу щільність.\n\nwith(white_wines_quant, cor(residual.sugar_chlorides, density))\nwith(white_wines_quant, cor(residual.sugar_chlorides, alcohol))\n\n\nwith(subset(white_wines_quant, quality.lvl==5),\n     cor(residual.sugar_chlorides, density))\nwith(subset(white_wines_quant, quality.lvl==5),\n     cor(residual.sugar_chlorides, alcohol))\n\n0.672887037704576\n\n\n-0.299601532893232\n\n\n0.790192212823227\n\n\n-0.350255657211274\n\n\nЩільність більше пов’язана зі співвідношенням залишкового цукру до хлоридів, ніж з алкоголем: 0,67 проти -0,29. Ми маємо той самий феномен для якості 5: 0,79 для щільності проти -0,35 для алкоголю.\n\n\n27.5.8 Хлориди, залишковий цукор і сульфати за якістю\n\n#adding a limit on the sulphates as there seems to be outlier far off\nsugar.chlorides+\n  geom_jitter(aes(color=sulphates), size =0.7, \n              data = subset(white_wines, \n                            sulphates&lt;quantile(white_wines$sulphates,0.95))) +\n  scale_colour_gradient2(low=\"red\", high=\"blue\", midpoint=0.5) +\n  ggtitle(\"Chlorides by residual sugar and sulphates\")\n\n\n\n\n\n\n\n\nЗдається, що чим більше залишкового цукру та хлоридів у вині, тим більше сульфатів.\n\nwith(white_wines_quant, cor(residual.sugar_chlorides, citric.acid))\nwith(subset(white_wines_quant, quality.lvl==5),\n     cor(residual.sugar_chlorides, citric.acid))\nwith(subset(white_wines_quant, quality.lvl==6),\n     cor(residual.sugar_chlorides, citric.acid))\nwith(subset(white_wines_quant, quality.lvl==7),\n     cor(residual.sugar_chlorides, citric.acid))\nwith(subset(white_wines_quant, quality.lvl==8),\n     cor(residual.sugar_chlorides, citric.acid))\n\n0.0994882466327046\n\n\n0.205085454808991\n\n\n0.0291507936288976\n\n\n0.00739929991358686\n\n\n0.0828935988213208\n\n\nКореляції показують, що насправді нічого немає. Кореляція для нашої підмножини становить 0,09. Розбиваючи за якістю:\n\n0,20 для якості 5\n0,02 для якості 6\n0,00 для якості 7\n0,08 для якості 8\n\n\n\n\n27.5.9 Хлориди, залишковий цукор і щільність за якістю\nЩільність пов’язана з алкоголем, тому давайте подивимося, чи можемо ми знайти щось тут.\n\nsugar.chlorides+\n  geom_jitter(aes(color=density), size = 0.4) +\n  scale_colour_gradient2(low=\"red\", high=\"blue\", midpoint=0.995) +\n  ggtitle(\"Chlorides by residual sugar and density\")\n\n\n\n\n\n\n\n\nМи отримуємо графік, де менш щільні вина знаходяться з лівого боку графіка, тоді як більш щільні вина розташовані в правій області на всіх рівнях якості.\n\nwith(white_wines_quant, cor(residual.sugar_chlorides, density))\nwith(white_wines_quant, cor(residual.sugar_chlorides, alcohol))\n\nwith(subset(white_wines_quant, quality.lvl==6),\n     cor(residual.sugar_chlorides, density))\nwith(subset(white_wines_quant, quality.lvl==6),\n     cor(residual.sugar_chlorides, alcohol))\n\n0.672887037704576\n\n\n-0.299601532893232\n\n\n0.690810668905907\n\n\n-0.330941786318977\n\n\nЩільність більше пов’язана зі співвідношенням залишкового цукру до хлоридів, ніж з алкоголем: 0,67 проти -0,29. Ми маємо той самий феномен для якості 6 (0,69 для щільності проти -0,33 для алкоголю).\n\n\n27.5.10 Хлориди, залишковий цукор і сульфати за якістю\n\n#adding a limit on the sulphates as there seems to be outlier far off\nsugar.chlorides+\n  geom_jitter(aes(color=sulphates), size = 0.4,\n              data = subset(white_wines, \n                            sulphates&lt;quantile(white_wines$sulphates,0.95))) +\n  scale_colour_gradient2(low=\"red\", high=\"blue\", midpoint=0.4) +\n  ggtitle(\"Chlorides by residual sugar and sulphates\")\n\n\n\n\n\n\n\n\nЗдається, немає чіткої картини взаємозв’язку сульфатів з якістю, а також з хлоридами та залишковим цукром.\n\nwith(white_wines_quant, cor(residual.sugar_chlorides, sulphates))\n\nwith(subset(white_wines_quant, quality.lvl==5),\n     cor(residual.sugar_chlorides, sulphates))\nwith(subset(white_wines_quant, quality.lvl==6),\n     cor(residual.sugar_chlorides, sulphates))\nwith(subset(white_wines_quant, quality.lvl==7),\n     cor(residual.sugar_chlorides, sulphates))\nwith(subset(white_wines_quant, quality.lvl==8),\n     cor(residual.sugar_chlorides, sulphates))\n\n-0.0753957002724575\n\n\n0.0222775535302433\n\n\n-0.0783773189493531\n\n\n-0.140269643837553\n\n\n-0.271830882076931\n\n\nКореляція підтверджує це враження з кореляцією -0,07 для підмножини між залишковим цукром/хлоридами та сульфатами. Розбиваючи за якістю:\n\n0,02 для якості 5\n-0,07 для якості 6\n-0,14 для якості 7\n-0,27 для якості 8\n\nКореляції за якістю низькі і збільшуються для якості 8, але ця якість 8 має лише кілька точок даних.\n\nggplot(aes(x = quality.lvl, y = sulphates), \n       data = subset(white_wines, \n                     sulphates&lt;quantile(white_wines$sulphates,0.95))) +\n  geom_boxplot(outlier.colour = 'red', alpha =0.5) +\n  ggtitle(\"Sulphates by quality, with sulphates&lt;0.95)\")\n\n\n\n\n\n\n\n\nТут немає сильної кореляції.\n\nwith(white_wines_quant, cor(quality, sulphates))\n\n0.0521672041266398\n\n\nКореляція підтверджує це (0,05), що немає сильної залежності між якістю та сульфатами.\nДавайте тепер подивимося на сірку.\n\n\n27.5.11 Хлориди, залишковий цукор і сірка за якістю\n\n# subset to filter outlier\n      \nsugar.chlorides+\n  geom_jitter(aes(color=free.sulfur.dioxide), size = 0.7, \n              data = subset(white_wines, \n                     free.sulfur.dioxide&lt;quantile(white_wines$free.sulfur.dioxide,\n                                                  0.95))) +\n  scale_colour_gradient2(low=\"red\", high=\"blue\", midpoint=14) +\n  ggtitle('chlorides by residual sugar & \n          free sulfur dioxide facetted by quality')\n\n\n# subset to filter outlier\nsugar.chlorides+\n  geom_jitter(aes(color=total.sulfur.dioxide), size = 0.5, \n              data = subset(white_wines, \n                     total.sulfur.dioxide &lt; \n                       quantile(white_wines$total.sulfur.dioxide, 0.95))) +\n  scale_colour_gradient2(low=\"red\", high=\"blue\", midpoint=100) +\n  ggtitle('chlorides by residual sugar & total sulfur dioxide facetted by quality')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nОбидва графіки показують рівні сірки на всіх рівнях якості та на всіх рівнях залишкового цукру та хлоридів. Існує слабка позитивна залежність між цими змінними.\n\nwith(white_wines_quant, cor(residual.sugar_chlorides, free.sulfur.dioxide))\nwith(white_wines_quant, cor(residual.sugar_chlorides, total.sulfur.dioxide))\n\n0.272855682251137\n\n\n0.302895112462388\n\n\nКореляції дійсно слабко позитивні:\n\n0,27 між залишковим цукром/хлоридами та вільним діоксидом сірки\n0,30 між залишковим цукром/хлоридами та загальним діоксидом сірки\n\nПерш за все, у наборі даних є викиди. Я намагався видалити їх, коли їх значення здавалося дійсно поза межами. Але це також може підкреслити різноманітність вин. Оскільки я не впевнений, наступні графіки базуються на всьому наборі даних. Загалом, немає дійсно сильних кореляцій між якістю та іншими змінними, представленими тут. Але я міг зрозуміти деякі взаємозв’язки щодо смаків:\n\n\n27.5.12 Смак оцту не дуже шукають\n\nggplot(aes(x = volatile.acidity, fill=quality.lvl), data = white_wines) +\n  geom_density(aes(y=..scaled..), alpha = 0.5) +\n  scale_fill_brewer(type = 'seq', palette = 3) +\n  labs(x = 'Volatile acidity (g / dm^3)', \n       y = 'Scaled density',  \n       fill='Quality') +\n  ggtitle(\"Volatile acidity by quality\")\n\n\n\n\n\n\n\n\nvolatile.acidity = Смак оцту, насправді неприємний\nЩільність була масштабована так, щоб низькі кількості екстремальної якості не впливали на загальний розподіл. З підвищенням якості вин летка кислотність знижується. Кореляція не така сильна, -0,19. Негативний знак означає, що чим вища летка кислотність (а отже, і смак оцту), тим нижча якість. Це не завжди так. Це те, чого можна очікувати. Коли ви купуєте дешеве вино (яке не завжди, але найчастіше є нижчої якості), воно найчастіше має різкий запах і смак, як оцет.\n\n\n27.5.13 Лимонна кислота неефективна для якості білого вина\nлимонна кислота: у невеликих кількостях лимонна кислота може додати винам “свіжості” та смаку\n\nggplot(aes(x = quality.lvl, y = citric.acid), data = white_wines) +\n  geom_jitter(size = 0.1, alpha = 0.4) +\n  geom_boxplot(outlier.colour = 'red', alpha =0.3, outlier.size = 0.5) +  \n  labs(x = 'Quality level', \n       y = 'Citric acid (g / dm^3)') +\n  ggtitle(\"Citric Acid by quality (red outliers)\")\n\n\n\n\n\n\n\n\nлимонна кислота не відіграє такої значної ролі у підвищенні якості.\n\n\n27.5.14 Отримайте високий рівень алкоголю та низький рівень сульфатів для кращих білих вин\n\nggplot(aes(x = sulphates, y = alcohol, color = quality.lvl), \n       data = white_wines) +\n  geom_jitter(size = 0.2) +\n  stat_smooth(aes(fill = quality.lvl),\n              level = 0.75, size = 1.2, alpha = 0.1, method = 'loess') +\n  coord_cartesian() +\n  xlim(0.4,0.75) +\n  ylim(6,15) +\n  scale_fill_brewer(type = 'seq', palette = 2, guide = FALSE) +\n  scale_color_brewer(type = 'seq', palette = 4) +\n  labs(x = 'Sulphates (g / dm3)', \n       y = 'Alcohol (%/volume)', \n       color = 'Quality') +\n  ggtitle(\"Alcohol by sulphates and quality (.75 confidence interval)\")\n\n`geom_smooth()` using formula = 'y ~ x'\nWarning message:\n\"Removed 1157 rows containing non-finite outside the scale range (`stat_smooth()`).\"\nWarning message in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, :\n\"span too small.   fewer data values than degrees of freedom.\"\nWarning message in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, :\n\"pseudoinverse used at 0.41905\"\nWarning message in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, :\n\"neighborhood radius 0.06095\"\nWarning message in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, :\n\"reciprocal condition number  0\"\nWarning message in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, :\n\"There are other near singularities as well. 0.022786\"\nWarning message in sqrt(sum.squares/one.delta):\n\"NaNs produced\"\nWarning message in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)), :\n\"span too small.   fewer data values than degrees of freedom.\"\nWarning message in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)), :\n\"pseudoinverse used at 0.41905\"\nWarning message in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)), :\n\"neighborhood radius 0.06095\"\nWarning message in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)), :\n\"reciprocal condition number  0\"\nWarning message in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)), :\n\"There are other near singularities as well. 0.022786\"\nWarning message in stats::qt(level/2 + 0.5, pred$df):\n\"NaNs produced\"\nWarning message:\n\"Removed 1243 rows containing missing values or values outside the scale range (`geom_point()`).\"\nWarning message in max(ids, na.rm = TRUE):\n\"no non-missing arguments to max; returning -Inf\"\n\n\n\n\n\n\n\n\n\nЦей графік показує згладжування алкоголю за сульфатами для всіх якостей. Якість і алкоголь мають сильну кореляцію.\nВина низької якості (3) сильно варіюються, але загальна тенденція полягає в тому, що вони мають низький вміст алкоголю, і рівень алкоголю знижується зі збільшенням рівня сульфатів. Якості від 4 до 7 загалом рівні, і єдині відмінності походять від рівня алкоголю. Вина якості 7 і 8 особливі тим, що вони згруповані у верхньому лівому куті. Взаємозв’язок не є рівним, як у інших якостей (крім 3), але ці вина мають високий вміст алкоголю та менше сульфатів.\n\n\n27.5.15 Зверніть увагу на меншу присутність хлоридів для кращого вина\nлимонна кислота: кількість солі у вашому вині\n\nggplot(aes(x = quality.lvl, y = chlorides), data = white_wines) +\n  geom_jitter(size = 0.1, alpha = 0.4) +\n  scale_y_continuous(trans = 'log10') +\n  geom_boxplot(outlier.colour = 'red', alpha =0.5, outlier.size = 0.5) +  \n  labs(x = 'Quality level', \n       y = 'Citric acid (g / dm^3)') +\n  ggtitle(\"Citric Acid by quality (red outliers)\")",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>EDA на прикладі вина</span>"
    ]
  },
  {
    "objectID": "eda.html#підсумки",
    "href": "eda.html#підсумки",
    "title": "27  EDA на прикладі вина",
    "section": "28.1 Підсумки",
    "text": "28.1 Підсумки\nУ цьому проєкті моя основна увага була зосереджена на проведенні лише дослідницького аналізу.\nЯ розпочав аналіз з огляду на загальні дані. Деякі точки даних здавалися дійсно далекими, і під час аналізу я видалив деякі з них. Але в кінцевому підсумку я вирішив залишити їх, оскільки смаки вина можуть сильно відрізнятися. Розглядаючи різні змінні, я вирішив дослідити, що робить вино хорошим. Я почав з розгляду основних факторів, які можуть впливати на смак: алкоголь, хлориди, залишковий цукор і pH. Моїм першим висновком було те, що алкоголь і якість дійсно пов’язані. Потім я перейшов до багатозмінного аналізу, поєднуючи алкоголь з хлоридами та залишковим цукром. Загальні тенденції за якістю виглядали досить схожими, тому я спробував побудувати графік хлоридів із залишковим цукром. Але результат не був таким, як я очікував. Думаючи, що інший фактор може бути пов’язаний з цими двома, я пройшов через змінні кислотності та якості, щоб вибрати летючу кислотність і лимонну кислоту та побудувати їх з хлоридами та залишковим цукром. Я зробив те саме для решти змінних. Я шукав більш складні взаємозв’язки між змінними, але не зміг їх знайти. В цілому, я отримав кілька змінних, що корелюють з якістю:\n\nАлкоголь\nЛетюча кислотність\nЛимонна кислота\nСульфати\n\nОбмеження набору даних полягають у відсутності точок для вин нижчої та вищої якості. Крім того, джерело якості невідоме: це професіонал? Магазин? І ми повинні пам’ятати, що смак - це дуже культурна річ (див. аналіз індійської їжі), і хороше вино для однієї людини може не бути таким для іншої.\nОстаннє, але не менш важливе, ми не маємо віку вина. У Франції однією з перших речей, яку перевіряють у вина, є його вік, оскільки загальновідомо, що старіше вино краще. Ще одна річ, яку я хотів би мати, це назви вин, щоб я міг дізнатися їхні ціни та побачити, як вони співвідносяться з якістю.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>EDA на прикладі вина</span>"
    ]
  },
  {
    "objectID": "eda-charts.html",
    "href": "eda-charts.html",
    "title": "28  EDA, робота з ggplot2",
    "section": "",
    "text": "28.1 Дослідницький аналіз даних (EDA)\nДослідницький аналіз даних (EDA) є важливим етапом у процесі науки про дані, який допомагає зрозуміти основну структуру набору даних. Один з найефективніших способів виконання EDA - це використання графічних представлень даних. Графіки можуть виявити шаблони, викиди та взаємозв’язки в даних, які можуть бути неочевидними з сирих даних.\nR є популярною мовою програмування для аналізу та візуалізації даних, і однією з найпоширеніших бібліотек для створення високоякісних графіків, готових до публікації, є ggplot2.\nДеякі поширені приклади графіків EDA, які можна створити за допомогою ggplot2, включають:",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#дослідницький-аналіз-даних-eda",
    "href": "eda-charts.html#дослідницький-аналіз-даних-eda",
    "title": "28  EDA, робота з ggplot2",
    "section": "",
    "text": "Точкові діаграми (Scatter plots): Використовуються для візуалізації взаємозв’язку між двома змінними.\nГістограми (Histograms): Використовуються для візуалізації розподілу однієї змінної.\nКоробкові діаграми (Box plots): Використовуються для візуалізації розподілу змінної та ідентифікації викидів.\nТочкові діаграми (Scatter plot): Використовуються для ідентифікації взаємозв’язків між усіма парами змінних у наборі даних.\nТеплові карти (Heatmaps): Використовуються для візуалізації взаємозв’язку між двома змінними шляхом побудови щільності точок у 2D-просторі.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#як-обрати-правильний-тип-діаграми",
    "href": "eda-charts.html#як-обрати-правильний-тип-діаграми",
    "title": "28  EDA, робота з ggplot2",
    "section": "28.2 Як обрати правильний тип діаграми?",
    "text": "28.2 Як обрати правильний тип діаграми?\nhttps://rpubs.com/Qemilo/chart_types_choose\nhttps://www.reddit.com/r/Infographics/comments/avrrlk/how_to_choose_the_right_chart_type_infographic/\nhttps://mintea.blog/?p=2305",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#стовпчасті-діаграми-column-charts",
    "href": "eda-charts.html#стовпчасті-діаграми-column-charts",
    "title": "28  EDA, робота з ggplot2",
    "section": "29.1 Стовпчасті діаграми (Column charts)",
    "text": "29.1 Стовпчасті діаграми (Column charts)\n\nВикористовуються для порівняння значень між кількома категоріями.\nКатегорії розташовані горизонтально (вісь X), а значення - вертикально (вісь Y).\nУ стовпчастих діаграмах можна також показати інформацію про частини цілого в різних категоріях, як в абсолютних значеннях, так і у відносних термінах. Тут з’являється концепція складених стовпчастих діаграм і 100% складених стовпчастих діаграм.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#гістограми-bar-charts",
    "href": "eda-charts.html#гістограми-bar-charts",
    "title": "28  EDA, робота з ggplot2",
    "section": "29.2 Гістограми (Bar charts)",
    "text": "29.2 Гістограми (Bar charts)\n\nДуже схожі на стовпчасті діаграми, але значення представлені на осі X, а категорії - на осі Y.\nВикористовуються для показу значень по категоріях, коли текст категорії або тривалість є довгими.\nСкладені гістограми використовуються для порівняння частин цілого (відносних і абсолютних) і порівняння змін по категоріях або часу.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#лінійні-діаграми-line-charts",
    "href": "eda-charts.html#лінійні-діаграми-line-charts",
    "title": "28  EDA, робота з ggplot2",
    "section": "29.3 Лінійні діаграми (Line charts)",
    "text": "29.3 Лінійні діаграми (Line charts)\n\nОдин з найпопулярніших графіків, широко використовуваний у багатьох галузях.\nВикористовуються для показу трендів за часом або категоріями.\nКатегорії розташовані горизонтально (вісь X), а значення - вертикально (вісь Y).",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#точкові-діаграми-scatter-plots",
    "href": "eda-charts.html#точкові-діаграми-scatter-plots",
    "title": "28  EDA, робота з ggplot2",
    "section": "29.4 Точкові діаграми (Scatter plots)",
    "text": "29.4 Точкові діаграми (Scatter plots)\n\nВикористовують числові значення вздовж обох осей.\nКорисні для показу кореляції між точками даних, яку може бути важко побачити з самих даних.\nВикористовуються для відображення та порівняння числових значень, таких як наукові або статистичні дані.\n\n\nlibrary(ggplot2)\n\n# Створення прикладного набору даних\ndata &lt;- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"D\"),\n  value = c(3, 7, 2, 5),\n  value2 = c(4, 6, 1, 8)\n)\n\n\nggplot(data, aes(x = category, y = value)) +\n  geom_col() +\n  labs(title = \"Стовпчаста діаграма\", x = \"Категорія\", y = \"Значення\")\n\n\n\n\n\n\n\n\n\nggplot(data, aes(x = category, y = value)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Гістограма\", x = \"Категорія\", y = \"Значення\")\n\n\n\n\n\n\n\n\n\nggplot(data, aes(x = category, y = value, group = 1)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Лінійна діаграма\", x = \"Категорія\", y = \"Значення\")\n\n\n\n\n\n\n\n\n\nggplot(data, aes(x = value, y = value2)) +\n  geom_point() +\n  labs(title = \"Точкова діаграма\", x = \"Значення 1\", y = \"Значення 2\")",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#гістограма-histogram",
    "href": "eda-charts.html#гістограма-histogram",
    "title": "28  EDA, робота з ggplot2",
    "section": "30.1 Гістограма (Histogram)",
    "text": "30.1 Гістограма (Histogram)\n\nВикористовується для графічного представлення частоти розподілу.\nВсі стовпчики торкаються один одного без проміжків між ними.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#коробковий-графік-box-plot",
    "href": "eda-charts.html#коробковий-графік-box-plot",
    "title": "28  EDA, робота з ggplot2",
    "section": "30.2 Коробковий графік (Box plot)",
    "text": "30.2 Коробковий графік (Box plot)\n\nТакож відомий як графік коробки з вусами.\nЛінія в середині коробки - це медіанне значення. Це означає, що 50% даних вище медіанного значення і 50% даних нижче медіанного значення.\nМедіани корисні, оскільки вони не піддаються впливу викидів, як середнє значення.\nУ самій коробці є 25% даних вище медіани і 25% даних нижче медіани, тобто 50% даних знаходяться в коробці.\nЗа допомогою цього графіка ми можемо легко виявити викиди і розподіл даних.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#kde-графік-kde-plot",
    "href": "eda-charts.html#kde-графік-kde-plot",
    "title": "28  EDA, робота з ggplot2",
    "section": "30.3 KDE графік (KDE Plot)",
    "text": "30.3 KDE графік (KDE Plot)\n\nKDE - це абревіатура від Kernel Density Estimation plot.\nЦе гладка форма гістограми.\nKDE графік є методом візуалізації розподілу спостережень у наборі даних, аналогічним гістограмі.\nВідносно гістограми, KDE може створити графік, який є менш захаращеним і більш інтерпретованим, особливо при побудові кількох розподілів.\n\n\nlibrary(ggplot2)\n\n# Створення прикладного набору даних\nset.seed(123)\ndata &lt;- data.frame(\n  value = rnorm(100),\n  category = sample(letters[1:4], 100, replace = TRUE)\n)\n\n\nggplot(data, aes(x = value)) +\n  geom_histogram(binwidth = 0.5, fill = \"blue\", color = \"black\") +\n  labs(title = \"Гістограма\", x = \"Значення\", y = \"Частота\")\n\n\n\n\n\n\n\n\n\nggplot(data, aes(x = category, y = value)) +\n  geom_boxplot() +\n  labs(title = \"Коробковий графік\", x = \"Категорія\", y = \"Значення\")\n\n\n\n\n\n\n\n\n\nggplot(data, aes(x = value)) +\n  geom_density(fill = \"blue\", alpha = 0.5) +\n  labs(title = \"KDE графік\", x = \"Значення\", y = \"Щільність\")",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#кругова-діаграма-pie-chart",
    "href": "eda-charts.html#кругова-діаграма-pie-chart",
    "title": "28  EDA, робота з ggplot2",
    "section": "31.1 Кругова діаграма (Pie Chart)",
    "text": "31.1 Кругова діаграма (Pie Chart)\n\nВикористовується для представлення категоричних даних як частини цілого.\nКожен сегмент представляє відсоток, який займає дана категорія від цілого.\nКраще використовувати кругову діаграму, якщо у вас менше 5 категорій.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#кільцева-діаграма-donut-chart",
    "href": "eda-charts.html#кільцева-діаграма-donut-chart",
    "title": "28  EDA, робота з ggplot2",
    "section": "31.2 Кільцева діаграма (Donut Chart)",
    "text": "31.2 Кільцева діаграма (Donut Chart)\n\nВаріант кругової діаграми з отвором у центрі.\nВідображає категорії у вигляді дуг, а не сегментів.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#складена-стовпчаста-діаграма-stacked-column-chart",
    "href": "eda-charts.html#складена-стовпчаста-діаграма-stacked-column-chart",
    "title": "28  EDA, робота з ggplot2",
    "section": "31.3 Складена стовпчаста діаграма (Stacked Column Chart)",
    "text": "31.3 Складена стовпчаста діаграма (Stacked Column Chart)\n\nВикористовується, коли потрібно показати відносний відсоток кількох рядів даних у складених стовпчиках, загальна сума яких завжди дорівнює 100%.\n100% складена стовпчаста діаграма може показати пропорції частин до цілого з часом, наприклад, пропорцію квартальних продажів за регіонами або пропорцію щомісячного платежу за іпотекою, що йде на відсотки проти основної суми.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#складена-гістограма-stacked-bar-chart",
    "href": "eda-charts.html#складена-гістограма-stacked-bar-chart",
    "title": "28  EDA, робота з ggplot2",
    "section": "31.4 Складена гістограма (Stacked Bar Chart)",
    "text": "31.4 Складена гістограма (Stacked Bar Chart)\n\nВикористовується для показу відносного відсотка кількох рядів даних у складеній гістограмі.\n\n\nlibrary(ggplot2)\n\n# Створення прикладного набору даних\ndata &lt;- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"D\"),\n  value = c(10, 20, 30, 40),\n  value2 = c(5, 15, 25, 35)\n)\n\n\nggplot(data, aes(x = \"\", y = value, fill = category)) +\n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start = 0) +\n  labs(title = \"Кругова діаграма\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nggplot(data, aes(x = 2, y = value, fill = category)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") +\n  xlim(0.5, 2.5) +\n  labs(title = \"Кільцева діаграма\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nggplot(data, aes(x = category, y = value, fill = category)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Складена стовпчаста діаграма\", x = \"Категорія\", y = \"Значення\")\n\n\n\n\n\n\n\n\n\nggplot(data, aes(x = category, y = value, fill = category)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Складена гістограма\", x = \"Категорія\", y = \"Значення\")\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\n# Створення складнішого набору даних\ndata &lt;- data.frame(\n  category = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 3),\n  subcategory = rep(c(\"X\", \"Y\", \"Z\"), times = 4),\n  value = c(10, 20, 30, 15, 25, 35, 20, 30, 40, 25, 35, 45)\n)\n\n\nggplot(data, aes(x = category, y = value, fill = subcategory)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Складена стовпчаста діаграма з підкатегоріями\", x = \"Категорія\", y = \"Значення\") +\n  theme_minimal()",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#точкова-діаграма-scatter-plot",
    "href": "eda-charts.html#точкова-діаграма-scatter-plot",
    "title": "28  EDA, робота з ggplot2",
    "section": "32.1 Точкова діаграма (Scatter Plot)",
    "text": "32.1 Точкова діаграма (Scatter Plot)\n\nТочкова діаграма використовує числові значення вздовж обох осей.\nВикористовує точки для представлення значень для двох різних числових значень.\nПоложення кожної точки на горизонтальній осі та вертикальній осі вказує на значення певної точки даних.\nКорисна для показу кореляції між точками даних, яку може бути важко побачити з самих даних.\nВикористовується для відображення та порівняння числових значень, таких як наукові або статистичні дані.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#лінійна-діаграма-line-chart",
    "href": "eda-charts.html#лінійна-діаграма-line-chart",
    "title": "28  EDA, робота з ggplot2",
    "section": "32.2 Лінійна діаграма (Line Chart)",
    "text": "32.2 Лінійна діаграма (Line Chart)\n\nЯк обговорювалося вище, лінійна діаграма також використовується для знаходження взаємозв’язку між двома змінними.\n\n\nlibrary(ggplot2)\n\n# Створення прикладного набору даних\nset.seed(123)\ndata &lt;- data.frame(\n  x_value = rnorm(100),\n  y_value = rnorm(100)\n)\n\n\nggplot(data, aes(x = x_value, y = y_value)) +\n  geom_point() +\n  labs(title = \"Точкова діаграма\", x = \"Значення X\", y = \"Значення Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata_line &lt;- data.frame(\n  time = 1:100,\n  value = cumsum(rnorm(100))\n)\n\nggplot(data_line, aes(x = time, y = value)) +\n  geom_line() +\n  labs(title = \"Лінійна діаграма\", x = \"Час\", y = \"Значення\") +\n  theme_minimal()",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#лінійна-діаграма-line-chart-1",
    "href": "eda-charts.html#лінійна-діаграма-line-chart-1",
    "title": "28  EDA, робота з ggplot2",
    "section": "33.1 Лінійна діаграма (Line Chart)",
    "text": "33.1 Лінійна діаграма (Line Chart)\n\nНайкращий спосіб візуалізувати дані трендів - це лінійна діаграма.\nЛінійні діаграми також використовуються для перегляду трендів у різних доменах.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#площева-діаграма-area-chart",
    "href": "eda-charts.html#площева-діаграма-area-chart",
    "title": "28  EDA, робота з ggplot2",
    "section": "33.2 Площева діаграма (Area Chart)",
    "text": "33.2 Площева діаграма (Area Chart)\n\nВикористовується для перегляду величини значень.\nПоказує відносну важливість значень за часом.\nСхожа на лінійну діаграму, але оскільки площа між лініями заповнена, площова діаграма підкреслює величину значень більше, ніж лінійна діаграма.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-charts.html#стовпчаста-діаграма-column-chart",
    "href": "eda-charts.html#стовпчаста-діаграма-column-chart",
    "title": "28  EDA, робота з ggplot2",
    "section": "33.3 Стовпчаста діаграма (Column Chart)",
    "text": "33.3 Стовпчаста діаграма (Column Chart)\n\nСтовпчаста діаграма, як обговорювалося вище, також використовується для показу трендів значень за часом і категоріями.\n\n\nlibrary(ggplot2)\n\n# Створення прикладного набору даних\nset.seed(123)\ndata &lt;- data.frame(\n  time = 1:100,\n  value = cumsum(rnorm(100))\n)\n\n\nggplot(data, aes(x = time, y = value)) +\n  geom_line() +\n  labs(title = \"Лінійна діаграма\", x = \"Час\", y = \"Значення\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(data, aes(x = time, y = value)) +\n  geom_area(fill = \"blue\", alpha = 0.5) +\n  labs(title = \"Площева діаграма\", x = \"Час\", y = \"Значення\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(data, aes(x = time, y = value)) +\n  geom_col() +\n  labs(title = \"Стовпчаста діаграма\", x = \"Час\", y = \"Значення\") +\n  theme_minimal()",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>EDA, робота з ggplot2</span>"
    ]
  },
  {
    "objectID": "eda-dlookr.html",
    "href": "eda-dlookr.html",
    "title": "29  EDA з dlookR",
    "section": "",
    "text": "29.1 00. Introduction\nExploratory Data Analysis (EDA) is the first step in data analysis process developed by “John Tukey” in the 1970s. In statistics, exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>EDA з dlookR</span>"
    ]
  },
  {
    "objectID": "eda-dlookr.html#introduction",
    "href": "eda-dlookr.html#introduction",
    "title": "29  EDA з dlookR",
    "section": "",
    "text": "note: some processed data is skipped or ignored like cbind or selecting variable because it’s not the focus of this tutorial",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>EDA з dlookR</span>"
    ]
  },
  {
    "objectID": "eda-dlookr.html#load-package",
    "href": "eda-dlookr.html#load-package",
    "title": "29  EDA з dlookR",
    "section": "29.2 01. Load Package",
    "text": "29.2 01. Load Package\n\n#install.packages(\"dlookr\")\n\n\nlibrary(dplyr) #A Grammar of Data Manipulation\nlibrary(tibble) #modern take on data frames.\nlibrary(dlookr) #Tools for Data Diagnosis, Exploration, Transformation (main library)",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>EDA з dlookR</span>"
    ]
  },
  {
    "objectID": "eda-dlookr.html#load-dataset",
    "href": "eda-dlookr.html#load-dataset",
    "title": "29  EDA з dlookR",
    "section": "29.3 02. Load Dataset",
    "text": "29.3 02. Load Dataset\nhe dataste used is airquality which is already available in R, the airquality dataset is daily air quality measurements in New York, May to September 1973.\n\n# View Data\nhead(airquality)\n\n\nA data.frame: 6 × 6\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n41\n190\n7.4\n67\n5\n1\n\n\n2\n36\n118\n8.0\n72\n5\n2\n\n\n3\n12\n149\n12.6\n74\n5\n3\n\n\n4\n18\n313\n11.5\n62\n5\n4\n\n\n5\nNA\nNA\n14.3\n56\n5\n5\n\n\n6\n28\nNA\n14.9\n66\n5\n6",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>EDA з dlookR</span>"
    ]
  },
  {
    "objectID": "eda-dlookr.html#data-diagnosis",
    "href": "eda-dlookr.html#data-diagnosis",
    "title": "29  EDA з dlookR",
    "section": "29.4 03. Data Diagnosis",
    "text": "29.4 03. Data Diagnosis\nThe first step is diagnosis from simple data\n\n29.4.1 3.1 General Data Diagnosis\n\ndiagnose(airquality)\n\n\nA tibble: 6 × 6\n\n\nvariables\ntypes\nmissing_count\nmissing_percent\nunique_count\nunique_rate\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nOzone\ninteger\n37\n24.183007\n68\n0.44444444\n\n\nSolar.R\ninteger\n7\n4.575163\n118\n0.77124183\n\n\nWind\nnumeric\n0\n0.000000\n31\n0.20261438\n\n\nTemp\ninteger\n0\n0.000000\n40\n0.26143791\n\n\nMonth\ninteger\n0\n0.000000\n5\n0.03267974\n\n\nDay\ninteger\n0\n0.000000\n31\n0.20261438\n\n\n\n\n\n\nvariables : variable names\ntypes : the data type of the variables\nmissing_count : number of missing values\nmissing_percent : percentage of missing values\nunique_count : number of unique values\nunique_rate : rate of unique value. unique_count / number of observation\n\n\n\n29.4.2 3.2 Diagnose Numeric Variable\nOnly Numeric Variable\n\ndiagnose_numeric(airquality)\n\n\nA data.frame: 6 × 10\n\n\nvariables\nmin\nQ1\nmean\nmedian\nQ3\nmax\nzero\nminus\noutlier\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nOzone\n1.0\n18.00\n42.129310\n31.5\n63.25\n168.0\n0\n0\n2\n\n\nSolar.R\n7.0\n115.75\n185.931507\n205.0\n258.75\n334.0\n0\n0\n0\n\n\nWind\n1.7\n7.40\n9.957516\n9.7\n11.50\n20.7\n0\n0\n3\n\n\nTemp\n56.0\n72.00\n77.882353\n79.0\n85.00\n97.0\n0\n0\n0\n\n\nMonth\n5.0\n6.00\n6.993464\n7.0\n8.00\n9.0\n0\n0\n0\n\n\nDay\n1.0\n8.00\n15.803922\n16.0\n23.00\n31.0\n0\n0\n0\n\n\n\n\n\n\n#diagnose_web_report(airquality)",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>EDA з dlookR</span>"
    ]
  },
  {
    "objectID": "edainspectdf.html",
    "href": "edainspectdf.html",
    "title": "30  # EDA з використанням inspectdf",
    "section": "",
    "text": "31 Встановлення та завантаження пакету inspectdf\nМені подобається цей пакет, тому що він має багато функціональних можливостей і є надзвичайно простим у використанні. Коротко кажучи, він дозволяє зрозуміти та візуалізувати типи стовпців, розміри, значення, дисбаланс значень і розподіли, а також кореляції. Крім того, він дозволяє дуже легко виконувати будь-яку з вищезазначених функцій для окремого датафрейму або порівнювати відмінності між двома датафреймами.\nlibrary(inspectdf)\nlibrary(tidyverse)\ndf= read_csv('https://raw.githubusercontent.com/lgellis/STEM/master/DATA-ART-1/Data/FinalData.csv', col_names = TRUE)\n\nRows: 185 Columns: 17\n── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): Gender, Horoscope, Subject, IntExt, OptPest, SpendTime1, SpendTime...\ndbl  (6): ID, Grade, ScreenTime, Sleep, PhysActive, HrsHomework\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(df, 10)\n\n\nA tibble: 10 × 17\n\n\nID\nGender\nGrade\nHoroscope\nSubject\nIntExt\nOptPest\nScreenTime\nSleep\nPhysActive\nHrsHomework\nSpendTime1\nSpendTime2\nSelf1\nSelf2\nCareer\nSuperpower\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\nmale\n4\nScorpio\nMath\nExtravert\nOptimist\n1\n7\n10\n10\nbaseball\nrelaxing\nactive\ncompetitive\nprofessional baseball player\nsonic speed\n\n\n2\nfemale\n4\nCapricorn\nGym\nExtravert\nOptimist\n1\n8\n5\n0\nplaying outside\nswimming\nkind\nactive\nTeacher\npower to grant wishes\n\n\n3\nmale\n4\nTaurus\nMath\nIntrovert\nOptimist\n4\n9\n22\n1\nvideo games\nsoccer\nactive\ncreative\nprofessional soccer player\npowerful kick\n\n\n4\nmale\n4\nAquarius\nMath\nDon't Know\nDon't Know\n3\n9\n9\n1\nvideo games\nsports\nactive\nresponsible\nprofessional hockey player\nteleportaion\n\n\n5\nmale\n4\nScorpio\nGym\nDon't Know\nDon't Know\n1\n9\n10\n1\nreading\nhanging out\nintellegent\nstrong\nengineer\npower to answer any question\n\n\n6\nmale\n4\nPisces\nGym\nExtravert\nOptimist\n2\n9\n20\n2\nsports\nplaying with friends\nfunny\nactive\nprofessional hockey player\nfly\n\n\n7\nmale\n3\nScorpio\nArt\nIntrovert\nOptimist\n1\n11\n4\n14\nwatching TV\nreading\njoyful\nlazy\nartist\ntelekinesis\n\n\n8\nmale\n6\nTaurus\nMath\nExtravert\nOptimist\n4\n9\n12\n21\nvideo games\nplaying outside\nfun\nconfident\nbanker\nteleportation\n\n\n9\nmale\n6\nAries\nGym\nIntrovert\nPessimist\n6\n8\n4\n6\nvideo games\nbiking\nsad\ncalm\ncomputer scientist\nmaterialize anything\n\n\n10\nmale\n6\nPisces\nMath\nIntrovert\nDon't Know\n3\n9\n12\n3\nvideo games\ndungeons and dragons\nsmart\ntired\nTeacher\nmanipulating physics\ndim(df)\n\n\n18517",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>EDA з використанням inspectdf</span>"
    ]
  },
  {
    "objectID": "edainspectdf.html#оцінка-типів-стовпців-датафрейму-за-допомогою-функції-inspect_types",
    "href": "edainspectdf.html#оцінка-типів-стовпців-датафрейму-за-допомогою-функції-inspect_types",
    "title": "30  # EDA з використанням inspectdf",
    "section": "33.1 1 Оцінка типів стовпців датафрейму за допомогою функції inspect_types()",
    "text": "33.1 1 Оцінка типів стовпців датафрейму за допомогою функції inspect_types()\n\n33.1.1 a) Оцінка повного датафрейму: allGrades\n\ninspect_types(allGrades) %&gt;% show_plot()\n\n\n\n\n\n\n\n\n\n\n33.1.2 b) Порівняння між youngGrades та oldGrades\n\ninspect_types(youngGrades, oldGrades) %&gt;% show_plot()",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>EDA з використанням inspectdf</span>"
    ]
  },
  {
    "objectID": "edainspectdf.html#оцінка-використання-памяті-стовпцями-датафрейму-за-допомогою-функції-inspect_mem",
    "href": "edainspectdf.html#оцінка-використання-памяті-стовпцями-датафрейму-за-допомогою-функції-inspect_mem",
    "title": "30  # EDA з використанням inspectdf",
    "section": "33.2 2 Оцінка використання пам’яті стовпцями датафрейму за допомогою функції inspect_mem()",
    "text": "33.2 2 Оцінка використання пам’яті стовпцями датафрейму за допомогою функції inspect_mem()\n\n33.2.1 a) Оцінка повного датафрейму: allGrades\n\ninspect_mem(allGrades) %&gt;% show_plot()\n\n\n\n\n\n\n\n\n\n\n33.2.2 b) Порівняння між youngGrades та oldGrades\n\ninspect_mem(youngGrades, oldGrades) %&gt;% show_plot()",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>EDA з використанням inspectdf</span>"
    ]
  },
  {
    "objectID": "edainspectdf.html#оцінка-поширеності-na-в-датафреймі-за-допомогою-функції-inspect_na",
    "href": "edainspectdf.html#оцінка-поширеності-na-в-датафреймі-за-допомогою-функції-inspect_na",
    "title": "30  # EDA з використанням inspectdf",
    "section": "33.3 3 Оцінка поширеності NA в датафреймі за допомогою функції inspect_na()",
    "text": "33.3 3 Оцінка поширеності NA в датафреймі за допомогою функції inspect_na()\n\n33.3.1 a) Оцінка повного датафрейму: allGrades\n\ninspect_na(allGrades) %&gt;% show_plot()\n\n\n\n\n\n\n\n\n\n\n33.3.2 b) Порівняння між youngGrades та oldGrades\n\ninspect_na(youngGrades, oldGrades) %&gt;% show_plot()",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>EDA з використанням inspectdf</span>"
    ]
  },
  {
    "objectID": "edainspectdf.html#оцінка-розподілу-числових-стовпців-за-допомогою-функції-inspect_num",
    "href": "edainspectdf.html#оцінка-розподілу-числових-стовпців-за-допомогою-функції-inspect_num",
    "title": "30  # EDA з використанням inspectdf",
    "section": "33.4 4 Оцінка розподілу числових стовпців за допомогою функції inspect_num()",
    "text": "33.4 4 Оцінка розподілу числових стовпців за допомогою функції inspect_num()\n\n33.4.1 a) Оцінка повного датафрейму: allGrades\n\ninspect_num(allGrades) %&gt;% show_plot()\n\n\n\n\n\n\n\n\n\n\n33.4.2 b) Порівняння між youngGrades та oldGrades\n\ninspect_num(youngGrades) %&gt;% show_plot()\n\n\n\n\n\n\n\n\n\ninspect_num(oldGrades) %&gt;% show_plot()",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>EDA з використанням inspectdf</span>"
    ]
  },
  {
    "objectID": "edainspectdf.html#оцінка-можливого-дисбалансу-категоріальних-стовпців-за-допомогою-функції-inspect_imb-для-виявлення-факторів-які-можуть-бути-надмірно-поширеними.",
    "href": "edainspectdf.html#оцінка-можливого-дисбалансу-категоріальних-стовпців-за-допомогою-функції-inspect_imb-для-виявлення-факторів-які-можуть-бути-надмірно-поширеними.",
    "title": "30  # EDA з використанням inspectdf",
    "section": "33.5 5 Оцінка можливого дисбалансу категоріальних стовпців за допомогою функції inspect_imb() для виявлення факторів, які можуть бути надмірно поширеними.",
    "text": "33.5 5 Оцінка можливого дисбалансу категоріальних стовпців за допомогою функції inspect_imb() для виявлення факторів, які можуть бути надмірно поширеними.\n\n33.5.1 a) Оцінка повного датафрейму: allGrades\n\ninspect_imb(allGrades) %&gt;% show_plot()\n\n\n\n\n\n\n\n\n\n\n33.5.2 b) Порівняння між youngGrades та oldGrades\n\n\n33.5.3 b) Порівняння між youngGrades та oldGrades\n\ninspect_imb(youngGrades, oldGrades) %&gt;% show_plot()",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>EDA з використанням inspectdf</span>"
    ]
  },
  {
    "objectID": "edainspectdf.html#оцінка-розподілу-категоріальних-стовпців-за-допомогою-функції-inspect_cat",
    "href": "edainspectdf.html#оцінка-розподілу-категоріальних-стовпців-за-допомогою-функції-inspect_cat",
    "title": "30  # EDA з використанням inspectdf",
    "section": "33.6 6 Оцінка розподілу категоріальних стовпців за допомогою функції inspect_cat()",
    "text": "33.6 6 Оцінка розподілу категоріальних стовпців за допомогою функції inspect_cat()\n\n33.6.1 a) Оцінка повного датафрейму: allGrades\n\ninspect_cat(allGrades) %&gt;% show_plot()\n\n\n\n\n\n\n\n\n\n\n33.6.2 b) Порівняння між youngGrades та oldGrades\n\ninspect_cat(youngGrades, oldGrades) %&gt;% show_plot()",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>EDA з використанням inspectdf</span>"
    ]
  },
  {
    "objectID": "edainspectdf.html#evaluate-the-7-оцінка-кореляцій-між-стовпцями-за-допомогою-функції-inspect_cor",
    "href": "edainspectdf.html#evaluate-the-7-оцінка-кореляцій-між-стовпцями-за-допомогою-функції-inspect_cor",
    "title": "30  # EDA з використанням inspectdf",
    "section": "33.7 7 Evaluate the## 7 Оцінка кореляцій між стовпцями за допомогою функції inspect_cor()",
    "text": "33.7 7 Evaluate the## 7 Оцінка кореляцій між стовпцями за допомогою функції inspect_cor()\n\n33.7.1 a) Оцінка повного датафрейму: allGrade\n\ninspect_cor(allGrades) %&gt;% show_plot()\n\n\n\n\n\n\n\n\n\n\n33.7.2 b) Порівняння між youngGrades та oldGrades\n\ninspect_cor(youngGrades, oldGrades) %&gt;% show_plot()",
    "crumbs": [
      "ТЕМА 7. ОГЛЯДОВИЙ АНАЛІЗ ДАНИХ",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>EDA з використанням inspectdf</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Основи роботи з даними в R",
    "section": "",
    "text": "Про посібник\nЗамінити інформацію про курс Аналіз даних\nМатеріали навчального посібника підготовлені для читання курсу “Вступ до прикладного програмування в R” [05.250] студентам 1-го року навчання спеціальності економічна кібернетика Національного університету “Острозька академія”.",
    "crumbs": [
      "Про посібник"
    ]
  },
  {
    "objectID": "index.html#опис-навчальної-дисципліни",
    "href": "index.html#опис-навчальної-дисципліни",
    "title": "Основи роботи з даними в R",
    "section": "Опис навчальної дисципліни",
    "text": "Опис навчальної дисципліни\nНавчальна дисципліна спрямована на вивчення основ практичного застосування популярної мови R для проведення статистичних досліджень в економіці.\nУ процесі вивчення курсу розглядаються теми, що стосуються теоретичних основ та практичної реалізації алгоритмів, завантаження, підготовки та обробки економічних даних.\nМісце навчальної дисципліни у підготовці здобувачів: програмні результати дисципліни використовуються під час вивчення таких навчальних дисциплін: “Алгоритми та структури даних”, “Аналіз даних в R”, “Прикладне математичне моделювання в R”, “Підготовка аналітичних звітів”. Закріплення на практиці здобутих програмних результатів відбувається під час проходження навчальної практики з курсу “Економіко-математичне моделювання”.",
    "crumbs": [
      "Про посібник"
    ]
  },
  {
    "objectID": "index.html#мета-дисципліни",
    "href": "index.html#мета-дисципліни",
    "title": "Основи роботи з даними в R",
    "section": "Мета дисципліни",
    "text": "Мета дисципліни\nМета навчальної дисципліни – формування у студентів теоретичних знань та практичних навичок використання мови програмування R для роботи з даними та базовими структурами мови (типи даних, розгалуження, цикли, функції).",
    "crumbs": [
      "Про посібник"
    ]
  },
  {
    "objectID": "index.html#підтримка-проєкту",
    "href": "index.html#підтримка-проєкту",
    "title": "Основи роботи з даними в R",
    "section": "Підтримка проєкту",
    "text": "Підтримка проєкту\nМатеріали навчального посібника створено у межах проєкту “Підготовка, обробка та ефективне використання даних для наукових досліджень (на основі R)”, що підтримується Європейським союзою за програмою House of Europe.",
    "crumbs": [
      "Про посібник"
    ]
  },
  {
    "objectID": "index.html#дотримання-принципів-доброчесності",
    "href": "index.html#дотримання-принципів-доброчесності",
    "title": "Основи роботи з даними в R",
    "section": "Дотримання принципів доброчесності",
    "text": "Дотримання принципів доброчесності\nВикладач та слухач цього курсу, як очікується, повинні дотримуватися Кодексу академічної доброчесності університету:\n\nбудь-яка робота, подана здобувачем протягом курсу, має бути його власною роботою здобувача; не вдаватися до кроків, що можуть нечесно покращити Ваші результати чи погіршити/покращити результати інших здобувачів;\nякщо буде виявлено ознаки плагіату або іншої недобросовісної академічної поведінки, то студент буде позбавлений можливості отримати передбачені бали за завдання;\nне публікувати у відкритому доступі відповіді на запитання, що використовуються в рамках курсу для оцінювання знань здобувачів;\nпід час фінальних видів контролю необхідно працювати самостійно; не дозволяється говорити або обговорювати, а також не можна копіювати документи, використовувати електронні засоби отримання інформації.\n\nПорушення академічної доброчесності під час виконання контрольних завдань призведе до втрати балів або вживання заходів, які передбачені Кодексу академічної доброчесності НаУОА.\n\n\n\n\n\n\n\nМатеріали курсу створені з використанням ряду технологій та середовищ розробки:\n\nМова R - безкоштована мова програмування для виконання досліджень у сфері статистики, машинного навчання та візуалізацї результатів.\nQuarto Book - система для публікації наукових та технічних текстів з відкритим кодом (R/Python/Julia/Observable).\nJupyterLab - середовище розробки на основі Jupyter Notebook. JupyterLab є розширеним веб-інтерфейсом для роботи з ноутбуками.\nGit/Github - система контролю версій та, відповідно, сервіс для організації зберігання коду, а також публікації статичних сторінок.\nRStudio Desktop - інтегроване середовище розробки (IDE) для мови R з відкритим кодом, що містить в собі редактор коду, консоль, планер, засоби візуалізації та можливості.\nVisual Studio Code - інтегроване середовище розробки (IDE) з відкритим кодом практично для усіх відомих технологій та мов програмування.\n\n\n\n\n\nБібілографічний опис bibtex:\n@book{yk-r-intro,\n  author       = {Юрій Клебан},\n  title        = {Вступ до програмування в R},\n  publisher    = {Zenodo},\n  year         = 2022,\n  doi          = {10.5281/zenodo.7251419},\n  url          = {https://doi.org/10.5281/zenodo.7251419}\n}",
    "crumbs": [
      "Про посібник"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Вступ",
    "section": "",
    "text": "Замінити вступ! —-\nФахівці спеціальності економічна кібернетика, а також фінанси та кредит у майбутньому працюватимуть з великими масивами даних, що накопичуються у даний момент і збиралися у попередні дисятиліття. Підготовка, обробка і трансформація даних у зручний формат прийняття рішень забирає все більше часу, а звичні рашіне інструменти аналізу даних, як наприклад, Microsoft Excel не мають достатньо вбудованих можливостей для виконнання задач бізнесу.\nНа даний час існує велика кількість мов програмування, що інтегруються у суспільні сфери діяльності людини та роботи технічних систем: біоінформатика, а також економіка та бізнес.\nОднією з мов програмування, що отримали широке поширення серед економістів-науковців, аналітиків та практиків математичного моделювання (machine learning) є мова програмування R(R Core Team 2020). Свою популярність ця мова програмування здобула завдяки простоті у використанні, доступності (безкоштовні як базові компоненти для написання коду, так і середовища розробки), розширюваності (кожен розробник має можливість створювати власні пакети та публікувати їх у відкритому доступі).\nОсновними задачами курсу “Вступ до прикладного програмування в R” є ознайомлення студентів з базовми конструкціями мови програмування R, вивчення способів роботи з найпоширенішими типами даних, читання інформації з різноманітних джерел. Також студенти отримують знання про можливості використання R для виконання задач аналізу даних та візуалізації.\n\n\n\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "Вступ"
    ]
  },
  {
    "objectID": "10-r-data-read-intro.html",
    "href": "10-r-data-read-intro.html",
    "title": "1  Загальна інформація + презентація",
    "section": "",
    "text": "1.1 Мета заняття\nРозглянути осноновні типи джерел даних, їх структуру, сервіси, бібліотеки та способи завантаження/вивантаження у R.",
    "crumbs": [
      "ТЕМА 1. ВСТУП ДО РОБОТИ З ДАНИМИ",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Загальна інформація + презентація</span>"
    ]
  },
  {
    "objectID": "10-r-data-read-intro.html#короткий-опис",
    "href": "10-r-data-read-intro.html#короткий-опис",
    "title": "1  Загальна інформація + презентація",
    "section": "1.2 Короткий опис",
    "text": "1.2 Короткий опис\nМатеріали розділу містять інформацію про структуру файлів у форматах CSV, XML, XLSX, JSON, а також способи читання інформації з API, chatGPT 3.5. Окрім того розглянуто також можливості читання запису SQL (на прикладі SQLite) та веб-сторінок (HTML).\n\n\nЦе вбудований документ &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; на платформі &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;.",
    "crumbs": [
      "ТЕМА 1. ВСТУП ДО РОБОТИ З ДАНИМИ",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Загальна інформація + презентація</span>"
    ]
  },
  {
    "objectID": "20-r-data-read-intro.html",
    "href": "20-r-data-read-intro.html",
    "title": "2  Загальна інформація + презентація",
    "section": "",
    "text": "2.1 Мета заняття\nРозглянути осноновні типи джерел даних, їх структуру, сервіси, бібліотеки та способи завантаження/вивантаження у R.",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Загальна інформація + презентація</span>"
    ]
  },
  {
    "objectID": "20-r-data-read-intro.html#короткий-опис",
    "href": "20-r-data-read-intro.html#короткий-опис",
    "title": "2  Загальна інформація + презентація",
    "section": "2.2 Короткий опис",
    "text": "2.2 Короткий опис\nМатеріали розділу містять інформацію про структуру файлів у форматах CSV, XML, XLSX, JSON, а також способи читання інформації з API, chatGPT 3.5. Окрім того розглянуто також можливості читання запису SQL (на прикладі SQLite) та веб-сторінок (HTML).",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Загальна інформація + презентація</span>"
    ]
  },
  {
    "objectID": "20-r-data-read-intro.html#презентація",
    "href": "20-r-data-read-intro.html#презентація",
    "title": "2  Загальна інформація + презентація",
    "section": "2.3 Презентація",
    "text": "2.3 Презентація\n\nЦе вбудований документ &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; на платформі &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;.",
    "crumbs": [
      "ТЕМА 2. ЧИТАННЯ ДАНИХ",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Загальна інформація + презентація</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Список використаних джерел",
    "section": "",
    "text": "R Core Team. 2020. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.",
    "crumbs": [
      "Список використаних джерел"
    ]
  }
]